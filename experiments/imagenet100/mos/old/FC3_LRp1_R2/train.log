2022-01-11 14:06:57,017 - INFO - ==> Preparing data..
2022-01-11 14:06:57,393 - INFO - checkpoint filename: experiments/coarse/mos/FC3_LRp1_R2/checkpoint.pt
2022-01-11 14:06:57,393 - INFO - log filename: experiments/coarse/mos/FC3_LRp1_R2/train.log
2022-01-11 14:06:57,393 - INFO - ********************************************************
2022-01-11 14:06:57,393 - INFO - Starting Iter: 0 / 1
2022-01-11 14:06:57,393 - INFO - ********************************************************
2022-01-11 14:07:00,782 - INFO - 
Epoch: 0
2022-01-11 14:07:00,783 - INFO - 
Learning Rate: 0.0100
2022-01-11 14:09:12,238 - INFO - [Step=250]	Loss=6.9427	243.4 examples/second
2022-01-11 14:11:22,383 - INFO - [Step=500]	Loss=5.4499	245.9 examples/second
2022-01-11 14:13:33,953 - INFO - [Step=750]	Loss=5.3101	243.2 examples/second
2022-01-11 14:14:26,528 - INFO - Test Loss=5.3365, Test top-1 acc=0.0330
2022-01-11 14:14:26,528 - INFO - Group Accuracy:

2022-01-11 14:14:26,528 - INFO - [0.93903613 0.939759   0.9373494  0.93783134 0.939759   0.9387952
 0.93710846 0.939759   0.939759   0.9395181  0.9518072  0.939759
 0.939759   0.939759   0.939759   0.939759   0.9518072 ]
2022-01-11 14:14:26,529 - INFO - Saving...
2022-01-11 14:14:26,693 - INFO - Epoch time: 445.91016387939453
2022-01-11 14:14:26,693 - INFO - 
Epoch: 1
2022-01-11 14:14:26,693 - INFO - 
Learning Rate: 0.0280
2022-01-11 14:15:54,739 - INFO - [Step=1000]	Loss=5.1679	227.3 examples/second
2022-01-11 14:18:04,714 - INFO - [Step=1250]	Loss=4.9561	246.2 examples/second
2022-01-11 14:20:16,645 - INFO - [Step=1500]	Loss=4.7692	242.6 examples/second
2022-01-11 14:21:53,169 - INFO - Test Loss=4.7747, Test top-1 acc=0.0771
2022-01-11 14:21:53,169 - INFO - Group Accuracy:

2022-01-11 14:21:53,169 - INFO - [0.939759   0.9387952  0.94       0.94144577 0.939759   0.94289154
 0.939759   0.939759   0.939759   0.9366265  0.9518072  0.939759
 0.9395181  0.9395181  0.939759   0.93903613 0.9518072 ]
2022-01-11 14:21:53,170 - INFO - Saving...
2022-01-11 14:21:53,410 - INFO - Epoch time: 446.7169213294983
2022-01-11 14:21:53,410 - INFO - 
Epoch: 2
2022-01-11 14:21:53,410 - INFO - 
Learning Rate: 0.0460
2022-01-11 14:22:37,244 - INFO - [Step=1750]	Loss=4.6402	227.6 examples/second
2022-01-11 14:24:47,464 - INFO - [Step=2000]	Loss=4.5198	245.7 examples/second
2022-01-11 14:26:58,245 - INFO - [Step=2250]	Loss=4.3285	244.7 examples/second
2022-01-11 14:29:07,507 - INFO - [Step=2500]	Loss=4.1574	247.6 examples/second
2022-01-11 14:29:18,531 - INFO - Test Loss=4.3079, Test top-1 acc=0.1229
2022-01-11 14:29:18,532 - INFO - Group Accuracy:

2022-01-11 14:29:18,532 - INFO - [0.94289154 0.94       0.939759   0.9501205  0.940241   0.94554216
 0.94192773 0.9392771  0.946506   0.94096386 0.9518072  0.94192773
 0.939759   0.9385542  0.9392771  0.9404819  0.95421684]
2022-01-11 14:29:18,533 - INFO - Saving...
2022-01-11 14:29:18,793 - INFO - Epoch time: 445.38261318206787
2022-01-11 14:29:18,793 - INFO - 
Epoch: 3
2022-01-11 14:29:18,793 - INFO - 
Learning Rate: 0.0640
2022-01-11 14:31:28,875 - INFO - [Step=2750]	Loss=4.0619	226.4 examples/second
2022-01-11 14:33:39,145 - INFO - [Step=3000]	Loss=3.8919	245.6 examples/second
2022-01-11 14:35:49,457 - INFO - [Step=3250]	Loss=3.7486	245.6 examples/second
2022-01-11 14:36:44,670 - INFO - Test Loss=3.5176, Test top-1 acc=0.2094
2022-01-11 14:36:44,670 - INFO - Group Accuracy:

2022-01-11 14:36:44,670 - INFO - [0.9426506  0.9479518  0.94216865 0.95686746 0.94289154 0.95566267
 0.9431325  0.94578314 0.9508434  0.94216865 0.9554217  0.94240963
 0.9407229  0.93903613 0.94120485 0.9477109  0.96048194]
2022-01-11 14:36:44,671 - INFO - Saving...
2022-01-11 14:36:44,899 - INFO - Epoch time: 446.10560846328735
2022-01-11 14:36:44,899 - INFO - 
Epoch: 4
2022-01-11 14:36:44,899 - INFO - 
Learning Rate: 0.1000
2022-01-11 14:38:09,713 - INFO - [Step=3500]	Loss=3.6953	228.2 examples/second
2022-01-11 14:40:19,846 - INFO - [Step=3750]	Loss=3.5665	245.9 examples/second
2022-01-11 14:42:30,090 - INFO - [Step=4000]	Loss=3.4452	245.7 examples/second
2022-01-11 14:44:08,704 - INFO - Test Loss=3.2705, Test top-1 acc=0.2431
2022-01-11 14:44:08,704 - INFO - Group Accuracy:

2022-01-11 14:44:08,705 - INFO - [0.94433737 0.9363856  0.94192773 0.96072286 0.94168675 0.9438554
 0.9366265  0.95228916 0.9549398  0.9460241  0.9559036  0.9433735
 0.94168675 0.94216865 0.94240963 0.94554216 0.9662651 ]
2022-01-11 14:44:08,706 - INFO - Saving...
2022-01-11 14:44:08,934 - INFO - Epoch time: 444.0346643924713
2022-01-11 14:44:08,934 - INFO - 
Epoch: 5
2022-01-11 14:44:08,934 - INFO - 
Learning Rate: 0.1000
2022-01-11 14:44:49,922 - INFO - [Step=4250]	Loss=3.3284	228.8 examples/second
2022-01-11 14:47:01,622 - INFO - [Step=4500]	Loss=3.1874	243.0 examples/second
2022-01-11 14:49:11,820 - INFO - [Step=4750]	Loss=3.0700	245.8 examples/second
2022-01-11 14:51:21,078 - INFO - [Step=5000]	Loss=2.9871	247.6 examples/second
2022-01-11 14:51:34,603 - INFO - Test Loss=2.9370, Test top-1 acc=0.3111
2022-01-11 14:51:34,603 - INFO - Group Accuracy:

2022-01-11 14:51:34,612 - INFO - [0.9460241  0.9506024  0.94506025 0.9703615  0.9479518  0.9672289
 0.94915664 0.946747   0.9662651  0.9477109  0.9595181  0.9460241
 0.94168675 0.94289154 0.9501205  0.9508434  0.973253  ]
2022-01-11 14:51:34,613 - INFO - Saving...
2022-01-11 14:51:34,891 - INFO - Epoch time: 445.9568238258362
2022-01-11 14:51:34,891 - INFO - 
Epoch: 6
2022-01-11 14:51:34,891 - INFO - 
Learning Rate: 0.1000
2022-01-11 14:53:43,025 - INFO - [Step=5250]	Loss=2.8792	225.4 examples/second
2022-01-11 14:55:53,392 - INFO - [Step=5500]	Loss=2.8130	245.5 examples/second
2022-01-11 14:58:04,007 - INFO - [Step=5750]	Loss=2.7286	245.0 examples/second
2022-01-11 14:59:01,960 - INFO - Test Loss=2.5463, Test top-1 acc=0.3733
2022-01-11 14:59:01,960 - INFO - Group Accuracy:

2022-01-11 14:59:01,961 - INFO - [0.9510843  0.95421684 0.9515663  0.97156626 0.9513253  0.97638553
 0.9472289  0.9592771  0.96819276 0.94963855 0.9626506  0.9501205
 0.9433735  0.9472289  0.9486747  0.9544578  0.9775904 ]
2022-01-11 14:59:01,962 - INFO - Saving...
2022-01-11 14:59:02,219 - INFO - Epoch time: 447.3276834487915
2022-01-11 14:59:02,219 - INFO - 
Epoch: 7
2022-01-11 14:59:02,219 - INFO - 
Learning Rate: 0.1000
2022-01-11 15:00:25,949 - INFO - [Step=6000]	Loss=2.6400	225.4 examples/second
2022-01-11 15:02:36,194 - INFO - [Step=6250]	Loss=2.5817	245.7 examples/second
2022-01-11 15:04:46,952 - INFO - [Step=6500]	Loss=2.5614	244.7 examples/second
2022-01-11 15:06:29,078 - INFO - Test Loss=2.3989, Test top-1 acc=0.4145
2022-01-11 15:06:29,079 - INFO - Group Accuracy:

2022-01-11 15:06:29,079 - INFO - [0.95373493 0.9614458  0.95349395 0.973494   0.9392771  0.97951806
 0.9546988  0.9546988  0.96891564 0.95662653 0.9660241  0.95686746
 0.94578314 0.9438554  0.94915664 0.9619277  0.9821687 ]
2022-01-11 15:06:29,080 - INFO - Saving...
2022-01-11 15:06:29,312 - INFO - Epoch time: 447.09335684776306
2022-01-11 15:06:29,312 - INFO - 
Epoch: 8
2022-01-11 15:06:29,313 - INFO - 
Learning Rate: 0.1000
2022-01-11 15:07:08,762 - INFO - [Step=6750]	Loss=2.4540	225.7 examples/second
2022-01-11 15:09:18,822 - INFO - [Step=7000]	Loss=2.4111	246.0 examples/second
2022-01-11 15:11:29,141 - INFO - [Step=7250]	Loss=2.3562	245.6 examples/second
2022-01-11 15:13:39,614 - INFO - [Step=7500]	Loss=2.3247	245.3 examples/second
2022-01-11 15:13:55,537 - INFO - Test Loss=2.7660, Test top-1 acc=0.3911
2022-01-11 15:13:55,538 - INFO - Group Accuracy:

2022-01-11 15:13:55,538 - INFO - [0.94939756 0.96048194 0.95349395 0.96795183 0.9549398  0.97542167
 0.94216865 0.95710844 0.9672289  0.9339759  0.9626506  0.9551807
 0.9445783  0.946747   0.94915664 0.9595181  0.97614455]
2022-01-11 15:13:55,539 - INFO - Epoch time: 446.22654008865356
2022-01-11 15:13:55,539 - INFO - 
Epoch: 9
2022-01-11 15:13:55,539 - INFO - 
Learning Rate: 0.1000
2022-01-11 15:16:00,544 - INFO - [Step=7750]	Loss=2.2354	227.1 examples/second
2022-01-11 15:18:09,686 - INFO - [Step=8000]	Loss=2.2182	247.8 examples/second
2022-01-11 15:20:20,773 - INFO - [Step=8250]	Loss=2.1888	244.1 examples/second
2022-01-11 15:21:21,298 - INFO - Test Loss=2.2049, Test top-1 acc=0.4513
2022-01-11 15:21:21,298 - INFO - Group Accuracy:

2022-01-11 15:21:21,298 - INFO - [0.9590362  0.95325303 0.96072286 0.97566265 0.95710844 0.9807229
 0.9518072  0.9621687  0.97228914 0.9551807  0.9655422  0.96096385
 0.9506024  0.94843376 0.9551807  0.9657831  0.9826506 ]
2022-01-11 15:21:21,298 - INFO - Saving...
2022-01-11 15:21:21,549 - INFO - Epoch time: 446.0098304748535
2022-01-11 15:21:21,549 - INFO - 
Epoch: 10
2022-01-11 15:21:21,549 - INFO - 
Learning Rate: 0.1000
2022-01-11 15:22:41,523 - INFO - [Step=8500]	Loss=2.1343	227.4 examples/second
2022-01-11 15:24:51,187 - INFO - [Step=8750]	Loss=2.1324	246.8 examples/second
2022-01-11 15:27:03,436 - INFO - [Step=9000]	Loss=2.0939	242.0 examples/second
2022-01-11 15:28:47,787 - INFO - Test Loss=1.9637, Test top-1 acc=0.5246
2022-01-11 15:28:47,787 - INFO - Group Accuracy:

2022-01-11 15:28:47,787 - INFO - [0.95710844 0.96843374 0.9619277  0.9778313  0.9653012  0.98289156
 0.9510843  0.9653012  0.9737349  0.9662651  0.97301203 0.9631325
 0.9518072  0.9551807  0.95710844 0.9693976  0.98289156]
2022-01-11 15:28:47,788 - INFO - Saving...
2022-01-11 15:28:48,038 - INFO - Epoch time: 446.48839807510376
2022-01-11 15:28:48,038 - INFO - 
Epoch: 11
2022-01-11 15:28:48,038 - INFO - 
Learning Rate: 0.1000
2022-01-11 15:29:23,718 - INFO - [Step=9250]	Loss=2.0499	228.1 examples/second
2022-01-11 15:31:34,006 - INFO - [Step=9500]	Loss=2.0058	245.6 examples/second
2022-01-11 15:33:45,672 - INFO - [Step=9750]	Loss=1.9961	243.0 examples/second
2022-01-11 15:35:55,456 - INFO - [Step=10000]	Loss=1.9792	246.6 examples/second
2022-01-11 15:36:14,158 - INFO - Test Loss=1.8863, Test top-1 acc=0.5145
2022-01-11 15:36:14,159 - INFO - Group Accuracy:

2022-01-11 15:36:14,159 - INFO - [0.9585542  0.97156626 0.9590362  0.9787952  0.9633735  0.98313254
 0.95662653 0.96795183 0.97542167 0.96506023 0.9778313  0.96096385
 0.95325303 0.9544578  0.9580723  0.96385545 0.9848193 ]
2022-01-11 15:36:14,160 - INFO - Epoch time: 446.122141122818
2022-01-11 15:36:14,160 - INFO - 
Epoch: 12
2022-01-11 15:36:14,160 - INFO - 
Learning Rate: 0.1000
2022-01-11 15:38:16,524 - INFO - [Step=10250]	Loss=1.9425	226.8 examples/second
2022-01-11 15:40:27,402 - INFO - [Step=10500]	Loss=1.9086	244.5 examples/second
2022-01-11 15:42:37,255 - INFO - [Step=10750]	Loss=1.8885	246.4 examples/second
2022-01-11 15:43:41,237 - INFO - Test Loss=2.0172, Test top-1 acc=0.5031
2022-01-11 15:43:41,237 - INFO - Group Accuracy:

2022-01-11 15:43:41,237 - INFO - [0.9619277  0.97518075 0.9674699  0.98313254 0.9621687  0.98289156
 0.9595181  0.96072286 0.96096385 0.96072286 0.96457833 0.96385545
 0.9518072  0.9472289  0.9554217  0.9706024  0.98240966]
2022-01-11 15:43:41,238 - INFO - Epoch time: 447.0778567790985
2022-01-11 15:43:41,238 - INFO - 
Epoch: 13
2022-01-11 15:43:41,238 - INFO - 
Learning Rate: 0.1000
2022-01-11 15:44:59,059 - INFO - [Step=11000]	Loss=1.8767	225.7 examples/second
2022-01-11 15:47:10,098 - INFO - [Step=11250]	Loss=1.8350	244.2 examples/second
2022-01-11 15:49:20,541 - INFO - [Step=11500]	Loss=1.8517	245.3 examples/second
2022-01-11 15:51:07,927 - INFO - Test Loss=1.9950, Test top-1 acc=0.5133
2022-01-11 15:51:07,927 - INFO - Group Accuracy:

2022-01-11 15:51:07,928 - INFO - [0.9631325  0.97228914 0.96433735 0.98       0.966506   0.9860241
 0.9590362  0.95566267 0.97445786 0.96771085 0.97566265 0.96409637
 0.9387952  0.95228916 0.95373493 0.96795183 0.98313254]
2022-01-11 15:51:07,929 - INFO - Epoch time: 446.6908628940582
2022-01-11 15:51:07,929 - INFO - 
Epoch: 14
2022-01-11 15:51:07,929 - INFO - 
Learning Rate: 0.1000
2022-01-11 15:51:41,364 - INFO - [Step=11750]	Loss=1.8356	227.2 examples/second
2022-01-11 15:53:52,512 - INFO - [Step=12000]	Loss=1.7922	244.0 examples/second
2022-01-11 15:56:02,905 - INFO - [Step=12250]	Loss=1.7712	245.4 examples/second
2022-01-11 15:58:12,686 - INFO - [Step=12500]	Loss=1.7654	246.6 examples/second
2022-01-11 15:58:33,778 - INFO - Test Loss=1.6233, Test top-1 acc=0.5742
2022-01-11 15:58:33,779 - INFO - Group Accuracy:

2022-01-11 15:58:33,779 - INFO - [0.96433735 0.9780723  0.9693976  0.9853012  0.96481925 0.9853012
 0.9619277  0.97108436 0.9771084  0.96698797 0.9775904  0.966506
 0.9583132  0.9587952  0.9624096  0.9737349  0.9881928 ]
2022-01-11 15:58:33,780 - INFO - Saving...
2022-01-11 15:58:34,008 - INFO - Epoch time: 446.07946038246155
2022-01-11 15:58:34,009 - INFO - 
Epoch: 15
2022-01-11 15:58:34,009 - INFO - 
Learning Rate: 0.1000
2022-01-11 16:00:35,159 - INFO - [Step=12750]	Loss=1.7438	224.6 examples/second
2022-01-11 16:02:44,822 - INFO - [Step=13000]	Loss=1.7266	246.8 examples/second
2022-01-11 16:04:54,559 - INFO - [Step=13250]	Loss=1.7237	246.7 examples/second
2022-01-11 16:06:00,208 - INFO - Test Loss=1.8024, Test top-1 acc=0.5284
2022-01-11 16:06:00,208 - INFO - Group Accuracy:

2022-01-11 16:06:00,208 - INFO - [0.96457833 0.966747   0.96385545 0.98313254 0.96457833 0.9826506
 0.96457833 0.9614458  0.97301203 0.96481925 0.9773494  0.9660241
 0.95325303 0.96168673 0.95686746 0.9686747  0.98746985]
2022-01-11 16:06:00,209 - INFO - Epoch time: 446.20027709007263
2022-01-11 16:06:00,209 - INFO - 
Epoch: 16
2022-01-11 16:06:00,209 - INFO - 
Learning Rate: 0.1000
2022-01-11 16:07:15,784 - INFO - [Step=13500]	Loss=1.6999	226.6 examples/second
2022-01-11 16:09:25,066 - INFO - [Step=13750]	Loss=1.6816	247.5 examples/second
2022-01-11 16:11:35,244 - INFO - [Step=14000]	Loss=1.6706	245.8 examples/second
2022-01-11 16:13:25,337 - INFO - Test Loss=2.1475, Test top-1 acc=0.5504
2022-01-11 16:13:25,337 - INFO - Group Accuracy:

2022-01-11 16:13:25,338 - INFO - [0.9633735  0.97493976 0.9674699  0.9778313  0.95686746 0.9848193
 0.96096385 0.96072286 0.9701205  0.9626506  0.9746988  0.9653012
 0.9448193  0.9583132  0.95975906 0.97204816 0.9812048 ]
2022-01-11 16:13:25,338 - INFO - Epoch time: 445.129225730896
2022-01-11 16:13:25,338 - INFO - 
Epoch: 17
2022-01-11 16:13:25,338 - INFO - 
Learning Rate: 0.1000
2022-01-11 16:13:55,968 - INFO - [Step=14250]	Loss=1.6682	227.4 examples/second
2022-01-11 16:16:05,577 - INFO - [Step=14500]	Loss=1.6226	246.9 examples/second
2022-01-11 16:18:15,581 - INFO - [Step=14750]	Loss=1.6392	246.1 examples/second
2022-01-11 16:20:25,756 - INFO - [Step=15000]	Loss=1.6229	245.8 examples/second
2022-01-11 16:20:49,395 - INFO - Test Loss=1.9056, Test top-1 acc=0.5583
2022-01-11 16:20:49,395 - INFO - Group Accuracy:

2022-01-11 16:20:49,395 - INFO - [0.95975906 0.97228914 0.9585542  0.9826506  0.96409637 0.9826506
 0.96168673 0.9701205  0.97903615 0.9655422  0.9696385  0.9660241
 0.96048194 0.9592771  0.9628916  0.9708434  0.9838554 ]
2022-01-11 16:20:49,396 - INFO - Epoch time: 444.0572440624237
2022-01-11 16:20:49,396 - INFO - 
Epoch: 18
2022-01-11 16:20:49,396 - INFO - 
Learning Rate: 0.1000
2022-01-11 16:22:46,316 - INFO - [Step=15250]	Loss=1.5653	227.7 examples/second
2022-01-11 16:24:56,128 - INFO - [Step=15500]	Loss=1.6075	246.5 examples/second
2022-01-11 16:27:07,443 - INFO - [Step=15750]	Loss=1.5907	243.7 examples/second
2022-01-11 16:28:17,094 - INFO - Test Loss=1.8405, Test top-1 acc=0.5573
2022-01-11 16:28:17,095 - INFO - Group Accuracy:

2022-01-11 16:28:17,095 - INFO - [0.9696385  0.9686747  0.966506   0.97975904 0.96819276 0.9884337
 0.9653012  0.9513253  0.9814458  0.9626506  0.97614455 0.96819276
 0.9549398  0.96       0.9628916  0.9737349  0.9893976 ]
2022-01-11 16:28:17,096 - INFO - Epoch time: 447.70027351379395
2022-01-11 16:28:17,096 - INFO - 
Epoch: 19
2022-01-11 16:28:17,096 - INFO - 
Learning Rate: 0.1000
2022-01-11 16:29:29,986 - INFO - [Step=16000]	Loss=1.5814	224.5 examples/second
2022-01-11 16:31:39,620 - INFO - [Step=16250]	Loss=1.5608	246.9 examples/second
2022-01-11 16:33:51,590 - INFO - [Step=16500]	Loss=1.5684	242.5 examples/second
2022-01-11 16:35:44,517 - INFO - Test Loss=1.8477, Test top-1 acc=0.5552
2022-01-11 16:35:44,517 - INFO - Group Accuracy:

2022-01-11 16:35:44,517 - INFO - [0.966506   0.97614455 0.96361446 0.9785542  0.9631325  0.98650604
 0.95566267 0.9706024  0.9785542  0.9674699  0.97301203 0.97228914
 0.9318072  0.95975906 0.9621687  0.9737349  0.98433733]
2022-01-11 16:35:44,518 - INFO - Epoch time: 447.4221541881561
2022-01-11 16:35:44,518 - INFO - 
Epoch: 20
2022-01-11 16:35:44,518 - INFO - 
Learning Rate: 0.1000
2022-01-11 16:36:12,686 - INFO - [Step=16750]	Loss=1.5558	226.8 examples/second
2022-01-11 16:38:24,113 - INFO - [Step=17000]	Loss=1.5396	243.5 examples/second
2022-01-11 16:40:33,809 - INFO - [Step=17250]	Loss=1.5270	246.7 examples/second
2022-01-11 16:42:42,985 - INFO - [Step=17500]	Loss=1.5378	247.7 examples/second
2022-01-11 16:43:09,089 - INFO - Test Loss=2.9424, Test top-1 acc=0.4516
2022-01-11 16:43:09,089 - INFO - Group Accuracy:

2022-01-11 16:43:09,089 - INFO - [0.9592771  0.97204816 0.8819277  0.96457833 0.9621687  0.9838554
 0.9549398  0.9583132  0.97831327 0.96048194 0.9626506  0.94
 0.94963855 0.95349395 0.9544578  0.96795183 0.9816868 ]
2022-01-11 16:43:09,090 - INFO - Epoch time: 444.57194232940674
2022-01-11 16:43:09,090 - INFO - 
Epoch: 21
2022-01-11 16:43:09,090 - INFO - 
Learning Rate: 0.1000
2022-01-11 16:45:01,966 - INFO - [Step=17750]	Loss=1.4907	230.2 examples/second
2022-01-11 16:47:11,565 - INFO - [Step=18000]	Loss=1.5084	246.9 examples/second
2022-01-11 16:49:19,959 - INFO - [Step=18250]	Loss=1.5038	249.2 examples/second
2022-01-11 16:50:30,148 - INFO - Test Loss=1.6064, Test top-1 acc=0.6092
2022-01-11 16:50:30,149 - INFO - Group Accuracy:

2022-01-11 16:50:30,149 - INFO - [0.9628916  0.9628916  0.97228914 0.98433733 0.9706024  0.9881928
 0.96457833 0.97542167 0.9771084  0.96506023 0.97951806 0.96819276
 0.9633735  0.96457833 0.9653012  0.97542167 0.9879518 ]
2022-01-11 16:50:30,149 - INFO - Saving...
2022-01-11 16:50:30,370 - INFO - Epoch time: 441.2794156074524
2022-01-11 16:50:30,370 - INFO - 
Epoch: 22
2022-01-11 16:50:30,370 - INFO - 
Learning Rate: 0.1000
2022-01-11 16:51:39,445 - INFO - [Step=18500]	Loss=1.4731	229.4 examples/second
2022-01-11 16:53:48,855 - INFO - [Step=18750]	Loss=1.4814	247.3 examples/second
2022-01-11 16:55:57,556 - INFO - [Step=19000]	Loss=1.4946	248.6 examples/second
2022-01-11 16:57:51,063 - INFO - Test Loss=1.5270, Test top-1 acc=0.6251
2022-01-11 16:57:51,064 - INFO - Group Accuracy:

2022-01-11 16:57:51,064 - INFO - [0.96481925 0.97951806 0.96891564 0.97951806 0.96891564 0.98963857
 0.9653012  0.97301203 0.9816868  0.96795183 0.9807229  0.9725301
 0.9614458  0.9653012  0.9626506  0.9780723  0.98963857]
2022-01-11 16:57:51,065 - INFO - Saving...
2022-01-11 16:57:51,325 - INFO - Epoch time: 440.95523619651794
2022-01-11 16:57:51,325 - INFO - 
Epoch: 23
2022-01-11 16:57:51,325 - INFO - 
Learning Rate: 0.1000
2022-01-11 16:58:16,562 - INFO - [Step=19250]	Loss=1.4683	230.2 examples/second
2022-01-11 17:00:26,323 - INFO - [Step=19500]	Loss=1.4504	246.6 examples/second
2022-01-11 17:02:35,255 - INFO - [Step=19750]	Loss=1.4786	248.2 examples/second
2022-01-11 17:04:44,600 - INFO - [Step=20000]	Loss=1.4662	247.4 examples/second
2022-01-11 17:05:13,348 - INFO - Test Loss=1.4488, Test top-1 acc=0.6284
2022-01-11 17:05:13,348 - INFO - Group Accuracy:

2022-01-11 17:05:13,348 - INFO - [0.96361446 0.97228914 0.9686747  0.9891566  0.96891564 0.98963857
 0.9693976  0.97493976 0.9804819  0.97108436 0.9809638  0.97204816
 0.9628916  0.966506   0.9662651  0.9766265  0.9893976 ]
2022-01-11 17:05:13,349 - INFO - Saving...
2022-01-11 17:05:13,618 - INFO - Epoch time: 442.2920591831207
2022-01-11 17:05:13,618 - INFO - 
Epoch: 24
2022-01-11 17:05:13,618 - INFO - 
Learning Rate: 0.1000
2022-01-11 17:07:05,302 - INFO - [Step=20250]	Loss=1.4129	227.4 examples/second
2022-01-11 17:09:14,339 - INFO - [Step=20500]	Loss=1.4421	248.0 examples/second
2022-01-11 17:11:22,897 - INFO - [Step=20750]	Loss=1.4484	248.9 examples/second
2022-01-11 17:12:35,106 - INFO - Test Loss=1.4038, Test top-1 acc=0.6323
2022-01-11 17:12:35,107 - INFO - Group Accuracy:

2022-01-11 17:12:35,107 - INFO - [0.9698795  0.9701205  0.97590363 0.98313254 0.9698795  0.99084336
 0.9698795  0.9768675  0.9819277  0.97204816 0.97975904 0.97445786
 0.9614458  0.9578313  0.96771085 0.97445786 0.9918072 ]
2022-01-11 17:12:35,108 - INFO - Saving...
2022-01-11 17:12:35,353 - INFO - Epoch time: 441.73567032814026
2022-01-11 17:12:35,354 - INFO - 
Epoch: 25
2022-01-11 17:12:35,354 - INFO - 
Learning Rate: 0.1000
2022-01-11 17:13:42,202 - INFO - [Step=21000]	Loss=1.4053	229.7 examples/second
2022-01-11 17:15:50,748 - INFO - [Step=21250]	Loss=1.4252	248.9 examples/second
2022-01-11 17:17:59,434 - INFO - [Step=21500]	Loss=1.4350	248.7 examples/second
2022-01-11 17:19:56,818 - INFO - Test Loss=1.4218, Test top-1 acc=0.6320
2022-01-11 17:19:56,818 - INFO - Group Accuracy:

2022-01-11 17:19:56,818 - INFO - [0.9696385  0.9771084  0.9739759  0.9855422  0.9746988  0.99108434
 0.9614458  0.9739759  0.9821687  0.9706024  0.97831327 0.9742169
 0.95566267 0.9706024  0.9614458  0.9768675  0.9879518 ]
2022-01-11 17:19:56,819 - INFO - Epoch time: 441.4655258655548
2022-01-11 17:19:56,819 - INFO - 
Epoch: 26
2022-01-11 17:19:56,819 - INFO - 
Learning Rate: 0.1000
2022-01-11 17:20:19,456 - INFO - [Step=21750]	Loss=1.4268	228.5 examples/second
2022-01-11 17:22:28,840 - INFO - [Step=22000]	Loss=1.3662	247.3 examples/second
2022-01-11 17:24:37,822 - INFO - [Step=22250]	Loss=1.4008	248.1 examples/second
2022-01-11 17:26:47,927 - INFO - [Step=22500]	Loss=1.4040	246.0 examples/second
2022-01-11 17:27:19,420 - INFO - Test Loss=1.7768, Test top-1 acc=0.5819
2022-01-11 17:27:19,421 - INFO - Group Accuracy:

2022-01-11 17:27:19,421 - INFO - [0.9660241  0.9691566  0.9674699  0.9853012  0.96771085 0.9881928
 0.95759034 0.9561446  0.98024094 0.9653012  0.97638553 0.96819276
 0.9479518  0.9633735  0.9626506  0.97301203 0.98771083]
2022-01-11 17:27:19,422 - INFO - Epoch time: 442.60233306884766
2022-01-11 17:27:19,422 - INFO - 
Epoch: 27
2022-01-11 17:27:19,422 - INFO - 
Learning Rate: 0.1000
2022-01-11 17:29:06,676 - INFO - [Step=22750]	Loss=1.3638	230.6 examples/second
2022-01-11 17:31:15,522 - INFO - [Step=23000]	Loss=1.3804	248.4 examples/second
2022-01-11 17:33:25,218 - INFO - [Step=23250]	Loss=1.4029	246.7 examples/second
2022-01-11 17:34:40,340 - INFO - Test Loss=1.6308, Test top-1 acc=0.5957
2022-01-11 17:34:40,340 - INFO - Group Accuracy:

2022-01-11 17:34:40,341 - INFO - [0.9674699  0.966747   0.9653012  0.9848193  0.9706024  0.9881928
 0.96506023 0.9703615  0.9787952  0.9590362  0.9698795  0.9701205
 0.9585542  0.96506023 0.9657831  0.9768675  0.9886747 ]
2022-01-11 17:34:40,342 - INFO - Epoch time: 440.92018580436707
2022-01-11 17:34:40,342 - INFO - 
Epoch: 28
2022-01-11 17:34:40,342 - INFO - 
Learning Rate: 0.1000
2022-01-11 17:35:44,157 - INFO - [Step=23500]	Loss=1.3802	230.3 examples/second
2022-01-11 17:37:53,298 - INFO - [Step=23750]	Loss=1.3746	247.8 examples/second
2022-01-11 17:40:03,162 - INFO - [Step=24000]	Loss=1.3679	246.4 examples/second
2022-01-11 17:42:01,822 - INFO - Test Loss=1.7033, Test top-1 acc=0.6116
2022-01-11 17:42:01,822 - INFO - Group Accuracy:

2022-01-11 17:42:01,822 - INFO - [0.96168673 0.973253   0.97108436 0.97927713 0.9696385  0.9853012
 0.9693976  0.9701205  0.98289156 0.9660241  0.973253   0.9708434
 0.9433735  0.9628916  0.9631325  0.97228914 0.99036145]
2022-01-11 17:42:01,823 - INFO - Epoch time: 441.4811911582947
2022-01-11 17:42:01,823 - INFO - 
Epoch: 29
2022-01-11 17:42:01,823 - INFO - 
Learning Rate: 0.0100
2022-01-11 17:42:21,900 - INFO - [Step=24250]	Loss=1.3629	230.7 examples/second
2022-01-11 17:44:30,982 - INFO - [Step=24500]	Loss=1.0700	247.9 examples/second
2022-01-11 17:46:40,822 - INFO - [Step=24750]	Loss=0.9982	246.5 examples/second
2022-01-11 17:48:49,814 - INFO - [Step=25000]	Loss=0.9772	248.1 examples/second
2022-01-11 17:49:23,947 - INFO - Test Loss=0.8944, Test top-1 acc=0.7492
2022-01-11 17:49:23,947 - INFO - Group Accuracy:

2022-01-11 17:49:23,947 - INFO - [0.97542167 0.9891566  0.9845783  0.99156624 0.98361444 0.99445784
 0.97927713 0.9816868  0.98963857 0.97493976 0.9901205  0.9812048
 0.97445786 0.9775904  0.97445786 0.98650604 0.99493974]
2022-01-11 17:49:23,949 - INFO - Saving...
2022-01-11 17:49:24,180 - INFO - Epoch time: 442.35696244239807
2022-01-11 17:49:24,180 - INFO - 
Epoch: 30
2022-01-11 17:49:24,181 - INFO - 
Learning Rate: 0.0100
2022-01-11 17:51:09,368 - INFO - [Step=25250]	Loss=0.9319	229.3 examples/second
2022-01-11 17:53:19,163 - INFO - [Step=25500]	Loss=0.9207	246.5 examples/second
2022-01-11 17:55:27,843 - INFO - [Step=25750]	Loss=0.9139	248.7 examples/second
2022-01-11 17:56:45,733 - INFO - Test Loss=0.8572, Test top-1 acc=0.7564
2022-01-11 17:56:45,733 - INFO - Group Accuracy:

2022-01-11 17:56:45,733 - INFO - [0.9771084  0.98963857 0.98626506 0.9922892  0.9833735  0.9946988
 0.9775904  0.9821687  0.99036145 0.9746988  0.9898795  0.98313254
 0.97542167 0.9771084  0.9768675  0.98722893 0.99421686]
2022-01-11 17:56:45,734 - INFO - Saving...
2022-01-11 17:56:45,992 - INFO - Epoch time: 441.81140995025635
2022-01-11 17:56:45,992 - INFO - 
Epoch: 31
2022-01-11 17:56:45,992 - INFO - 
Learning Rate: 0.0100
2022-01-11 17:57:47,432 - INFO - [Step=26000]	Loss=0.9170	229.2 examples/second
2022-01-11 17:59:57,839 - INFO - [Step=26250]	Loss=0.8884	245.4 examples/second
2022-01-11 18:02:06,426 - INFO - [Step=26500]	Loss=0.8777	248.9 examples/second
2022-01-11 18:04:07,985 - INFO - Test Loss=0.8431, Test top-1 acc=0.7631
2022-01-11 18:04:07,985 - INFO - Group Accuracy:

2022-01-11 18:04:07,986 - INFO - [0.9787952  0.99036145 0.9838554  0.9922892  0.9819277  0.9956626
 0.97975904 0.9840964  0.9901205  0.9766265  0.9898795  0.98289156
 0.9737349  0.97831327 0.97614455 0.98746985 0.9954217 ]
2022-01-11 18:04:07,987 - INFO - Saving...
2022-01-11 18:04:08,220 - INFO - Epoch time: 442.227422952652
2022-01-11 18:04:08,220 - INFO - 
Epoch: 32
2022-01-11 18:04:08,220 - INFO - 
Learning Rate: 0.0100
2022-01-11 18:04:25,729 - INFO - [Step=26750]	Loss=0.8843	229.7 examples/second
2022-01-11 18:06:35,802 - INFO - [Step=27000]	Loss=0.8607	246.0 examples/second
2022-01-11 18:08:45,273 - INFO - [Step=27250]	Loss=0.8634	247.2 examples/second
2022-01-11 18:10:54,644 - INFO - [Step=27500]	Loss=0.8652	247.4 examples/second
2022-01-11 18:11:31,145 - INFO - Test Loss=0.8401, Test top-1 acc=0.7576
2022-01-11 18:11:31,145 - INFO - Group Accuracy:

2022-01-11 18:11:31,145 - INFO - [0.9775904  0.9898795  0.9845783  0.99108434 0.98361444 0.9956626
 0.98       0.9826506  0.9901205  0.97566265 0.9889157  0.9840964
 0.97493976 0.98       0.9766265  0.98626506 0.9951807 ]
2022-01-11 18:11:31,146 - INFO - Epoch time: 442.92628049850464
2022-01-11 18:11:31,146 - INFO - 
Epoch: 33
2022-01-11 18:11:31,146 - INFO - 
Learning Rate: 0.0100
2022-01-11 18:13:14,395 - INFO - [Step=27750]	Loss=0.8432	229.0 examples/second
2022-01-11 18:15:23,440 - INFO - [Step=28000]	Loss=0.8438	248.0 examples/second
2022-01-11 18:17:32,324 - INFO - [Step=28250]	Loss=0.8251	248.3 examples/second
2022-01-11 18:18:52,814 - INFO - Test Loss=0.8341, Test top-1 acc=0.7629
2022-01-11 18:18:52,814 - INFO - Group Accuracy:

2022-01-11 18:18:52,841 - INFO - [0.9787952  0.9901205  0.9850602  0.9913253  0.98240966 0.9951807
 0.9819277  0.9814458  0.9901205  0.97445786 0.99060243 0.98313254
 0.97493976 0.9804819  0.9773494  0.98626506 0.9954217 ]
2022-01-11 18:18:52,843 - INFO - Epoch time: 441.6967475414276
2022-01-11 18:18:52,843 - INFO - 
Epoch: 34
2022-01-11 18:18:52,843 - INFO - 
Learning Rate: 0.0100
2022-01-11 18:19:52,502 - INFO - [Step=28500]	Loss=0.8427	228.3 examples/second
2022-01-11 18:22:02,599 - INFO - [Step=28750]	Loss=0.8422	246.0 examples/second
2022-01-11 18:24:11,681 - INFO - [Step=29000]	Loss=0.8132	247.9 examples/second
2022-01-11 18:26:16,827 - INFO - Test Loss=0.8259, Test top-1 acc=0.7670
2022-01-11 18:26:16,827 - INFO - Group Accuracy:

2022-01-11 18:26:16,827 - INFO - [0.98       0.99060243 0.9850602  0.9927711  0.98361444 0.9959036
 0.97951806 0.9819277  0.9898795  0.97445786 0.9901205  0.9833735
 0.97638553 0.98       0.9768675  0.9884337  0.9956626 ]
2022-01-11 18:26:16,828 - INFO - Saving...
2022-01-11 18:26:17,062 - INFO - Epoch time: 444.21901679039
2022-01-11 18:26:17,062 - INFO - 
Epoch: 35
2022-01-11 18:26:17,062 - INFO - 
Learning Rate: 0.0100
2022-01-11 18:26:32,087 - INFO - [Step=29250]	Loss=0.8237	227.9 examples/second
2022-01-11 18:28:40,928 - INFO - [Step=29500]	Loss=0.8186	248.4 examples/second
2022-01-11 18:30:49,892 - INFO - [Step=29750]	Loss=0.8231	248.1 examples/second
2022-01-11 18:32:59,706 - INFO - [Step=30000]	Loss=0.8125	246.5 examples/second
2022-01-11 18:33:39,621 - INFO - Test Loss=0.8275, Test top-1 acc=0.7641
2022-01-11 18:33:39,621 - INFO - Group Accuracy:

2022-01-11 18:33:39,621 - INFO - [0.9778313  0.9901205  0.9855422  0.99108434 0.9821687  0.9961446
 0.98       0.9814458  0.99036145 0.97566265 0.99060243 0.9838554
 0.97831327 0.9816868  0.9778313  0.98722893 0.9954217 ]
2022-01-11 18:33:39,622 - INFO - Epoch time: 442.55989146232605
2022-01-11 18:33:39,622 - INFO - 
Epoch: 36
2022-01-11 18:33:39,622 - INFO - 
Learning Rate: 0.0100
2022-01-11 18:35:21,373 - INFO - [Step=30250]	Loss=0.7945	225.9 examples/second
2022-01-11 18:37:30,689 - INFO - [Step=30500]	Loss=0.8033	247.5 examples/second
2022-01-11 18:39:41,570 - INFO - [Step=30750]	Loss=0.7963	244.5 examples/second
2022-01-11 18:41:05,428 - INFO - Test Loss=0.8258, Test top-1 acc=0.7672
2022-01-11 18:41:05,429 - INFO - Group Accuracy:

2022-01-11 18:41:05,429 - INFO - [0.9778313  0.99084336 0.9850602  0.99108434 0.9838554  0.9956626
 0.98       0.98240966 0.9884337  0.9746988  0.99084336 0.9850602
 0.97638553 0.9809638  0.97831327 0.98674697 0.9961446 ]
2022-01-11 18:41:05,430 - INFO - Saving...
2022-01-11 18:41:05,679 - INFO - Epoch time: 446.0567190647125
2022-01-11 18:41:05,679 - INFO - 
Epoch: 37
2022-01-11 18:41:05,679 - INFO - 
Learning Rate: 0.0100
2022-01-11 18:42:02,572 - INFO - [Step=31000]	Loss=0.7908	226.9 examples/second
2022-01-11 18:44:12,069 - INFO - [Step=31250]	Loss=0.7914	247.1 examples/second
2022-01-11 18:46:22,997 - INFO - [Step=31500]	Loss=0.8007	244.4 examples/second
2022-01-11 18:48:29,801 - INFO - Test Loss=0.8232, Test top-1 acc=0.7708
2022-01-11 18:48:29,801 - INFO - Group Accuracy:

2022-01-11 18:48:29,801 - INFO - [0.9785542  0.99036145 0.98626506 0.9920482  0.9838554  0.9959036
 0.9807229  0.98       0.9901205  0.9746988  0.99084336 0.9850602
 0.97614455 0.97975904 0.9773494  0.98698795 0.9954217 ]
2022-01-11 18:48:29,802 - INFO - Saving...
2022-01-11 18:48:30,105 - INFO - Epoch time: 444.4258542060852
2022-01-11 18:48:30,105 - INFO - 
Epoch: 38
2022-01-11 18:48:30,105 - INFO - 
Learning Rate: 0.0100
2022-01-11 18:48:42,528 - INFO - [Step=31750]	Loss=0.7680	229.3 examples/second
2022-01-11 18:50:53,068 - INFO - [Step=32000]	Loss=0.7624	245.1 examples/second
2022-01-11 18:53:04,366 - INFO - [Step=32250]	Loss=0.7816	243.7 examples/second
2022-01-11 18:55:13,804 - INFO - [Step=32500]	Loss=0.7788	247.2 examples/second
2022-01-11 18:55:55,809 - INFO - Test Loss=0.8162, Test top-1 acc=0.7716
2022-01-11 18:55:55,810 - INFO - Group Accuracy:

2022-01-11 18:55:55,810 - INFO - [0.9785542  0.99084336 0.98626506 0.99060243 0.98313254 0.9951807
 0.9814458  0.9833735  0.9893976  0.97493976 0.9886747  0.9848193
 0.97614455 0.9807229  0.9778313  0.9879518  0.9966265 ]
2022-01-11 18:55:55,811 - INFO - Saving...
2022-01-11 18:55:56,056 - INFO - Epoch time: 445.95028471946716
2022-01-11 18:55:56,056 - INFO - 
Epoch: 39
2022-01-11 18:55:56,056 - INFO - 
Learning Rate: 0.0100
2022-01-11 18:57:34,241 - INFO - [Step=32750]	Loss=0.7685	227.9 examples/second
2022-01-11 18:59:45,275 - INFO - [Step=33000]	Loss=0.7619	244.2 examples/second
2022-01-11 19:01:55,399 - INFO - [Step=33250]	Loss=0.7608	245.9 examples/second
2022-01-11 19:03:20,670 - INFO - Test Loss=0.8123, Test top-1 acc=0.7706
2022-01-11 19:03:20,671 - INFO - Group Accuracy:

2022-01-11 19:03:20,671 - INFO - [0.9787952  0.9913253  0.98650604 0.9920482  0.98313254 0.9954217
 0.98       0.9821687  0.9889157  0.97518075 0.993253   0.9838554
 0.97518075 0.9812048  0.9773494  0.98771083 0.9961446 ]
2022-01-11 19:03:20,672 - INFO - Epoch time: 444.61608600616455
2022-01-11 19:03:20,672 - INFO - 
Epoch: 40
2022-01-11 19:03:20,672 - INFO - 
Learning Rate: 0.0100
2022-01-11 19:04:14,471 - INFO - [Step=33500]	Loss=0.7694	230.1 examples/second
2022-01-11 19:06:24,142 - INFO - [Step=33750]	Loss=0.7368	246.8 examples/second
2022-01-11 19:08:33,204 - INFO - [Step=34000]	Loss=0.7692	247.9 examples/second
2022-01-11 19:10:42,990 - INFO - Test Loss=0.8025, Test top-1 acc=0.7737
2022-01-11 19:10:42,990 - INFO - Group Accuracy:

2022-01-11 19:10:42,990 - INFO - [0.97903615 0.9918072  0.98626506 0.993253   0.98433733 0.9956626
 0.97927713 0.9821687  0.99060243 0.97638553 0.99060243 0.9819277
 0.97493976 0.98240966 0.97903615 0.9886747  0.9959036 ]
2022-01-11 19:10:42,992 - INFO - Saving...
2022-01-11 19:10:43,269 - INFO - Epoch time: 442.59658765792847
2022-01-11 19:10:43,269 - INFO - 
Epoch: 41
2022-01-11 19:10:43,269 - INFO - 
Learning Rate: 0.0100
2022-01-11 19:10:52,975 - INFO - [Step=34250]	Loss=0.7630	228.9 examples/second
2022-01-11 19:13:02,958 - INFO - [Step=34500]	Loss=0.7491	246.2 examples/second
2022-01-11 19:15:12,240 - INFO - [Step=34750]	Loss=0.7433	247.5 examples/second
2022-01-11 19:17:21,182 - INFO - [Step=35000]	Loss=0.7501	248.2 examples/second
2022-01-11 19:18:04,867 - INFO - Test Loss=0.7995, Test top-1 acc=0.7752
2022-01-11 19:18:04,867 - INFO - Group Accuracy:

2022-01-11 19:18:04,872 - INFO - [0.98       0.99084336 0.98674697 0.99373496 0.9845783  0.9956626
 0.9807229  0.98313254 0.9893976  0.97638553 0.9920482  0.98361444
 0.97614455 0.9816868  0.9780723  0.98722893 0.9951807 ]
2022-01-11 19:18:04,873 - INFO - Saving...
2022-01-11 19:18:05,149 - INFO - Epoch time: 441.87991547584534
2022-01-11 19:18:05,149 - INFO - 
Epoch: 42
2022-01-11 19:18:05,149 - INFO - 
Learning Rate: 0.0100
2022-01-11 19:19:41,149 - INFO - [Step=35250]	Loss=0.7377	228.6 examples/second
2022-01-11 19:21:49,883 - INFO - [Step=35500]	Loss=0.7420	248.6 examples/second
2022-01-11 19:23:59,094 - INFO - [Step=35750]	Loss=0.7413	247.7 examples/second
2022-01-11 19:25:27,047 - INFO - Test Loss=0.8112, Test top-1 acc=0.7735
2022-01-11 19:25:27,047 - INFO - Group Accuracy:

2022-01-11 19:25:27,047 - INFO - [0.97951806 0.99084336 0.9860241  0.9925301  0.9853012  0.9956626
 0.97951806 0.9819277  0.99108434 0.97518075 0.99084336 0.98289156
 0.97590363 0.9812048  0.9773494  0.9879518  0.9961446 ]
2022-01-11 19:25:27,048 - INFO - Epoch time: 441.898806810379
2022-01-11 19:25:27,048 - INFO - 
Epoch: 43
2022-01-11 19:25:27,048 - INFO - 
Learning Rate: 0.0100
2022-01-11 19:26:18,951 - INFO - [Step=36000]	Loss=0.7367	228.8 examples/second
2022-01-11 19:28:28,376 - INFO - [Step=36250]	Loss=0.7281	247.2 examples/second
2022-01-11 19:30:38,306 - INFO - [Step=36500]	Loss=0.7347	246.3 examples/second
2022-01-11 19:32:51,489 - INFO - Test Loss=0.8125, Test top-1 acc=0.7745
2022-01-11 19:32:51,489 - INFO - Group Accuracy:

2022-01-11 19:32:51,489 - INFO - [0.98024094 0.99084336 0.98650604 0.9927711  0.98313254 0.9956626
 0.98024094 0.9816868  0.98963857 0.97566265 0.99156624 0.9840964
 0.97638553 0.9816868  0.9775904  0.9884337  0.9959036 ]
2022-01-11 19:32:51,491 - INFO - Epoch time: 444.44235157966614
2022-01-11 19:32:51,491 - INFO - 
Epoch: 44
2022-01-11 19:32:51,491 - INFO - 
Learning Rate: 0.0100
2022-01-11 19:32:58,657 - INFO - [Step=36750]	Loss=0.7293	228.0 examples/second
2022-01-11 19:35:07,544 - INFO - [Step=37000]	Loss=0.7017	248.3 examples/second
2022-01-11 19:37:16,919 - INFO - [Step=37250]	Loss=0.7276	247.3 examples/second
2022-01-11 19:39:27,629 - INFO - [Step=37500]	Loss=0.7281	244.8 examples/second
2022-01-11 19:40:14,546 - INFO - Test Loss=0.8018, Test top-1 acc=0.7766
2022-01-11 19:40:14,547 - INFO - Group Accuracy:

2022-01-11 19:40:14,547 - INFO - [0.9807229  0.99060243 0.98674697 0.9925301  0.9833735  0.99493974
 0.9807229  0.9807229  0.9913253  0.9742169  0.9930121  0.9840964
 0.9785542  0.9819277  0.97614455 0.9889157  0.9956626 ]
2022-01-11 19:40:14,548 - INFO - Saving...
2022-01-11 19:40:14,783 - INFO - Epoch time: 443.29253697395325
2022-01-11 19:40:14,783 - INFO - 
Epoch: 45
2022-01-11 19:40:14,784 - INFO - 
Learning Rate: 0.0100
2022-01-11 19:41:47,796 - INFO - [Step=37750]	Loss=0.7106	228.3 examples/second
2022-01-11 19:43:57,166 - INFO - [Step=38000]	Loss=0.7174	247.4 examples/second
2022-01-11 19:46:06,885 - INFO - [Step=38250]	Loss=0.7194	246.7 examples/second
2022-01-11 19:47:37,887 - INFO - Test Loss=0.7996, Test top-1 acc=0.7766
2022-01-11 19:47:37,888 - INFO - Group Accuracy:

2022-01-11 19:47:37,888 - INFO - [0.97951806 0.9898795  0.98722893 0.993494   0.9853012  0.9954217
 0.9807229  0.9833735  0.9913253  0.97542167 0.99084336 0.9833735
 0.97638553 0.9816868  0.97903615 0.9891566  0.9966265 ]
2022-01-11 19:47:37,889 - INFO - Epoch time: 443.1056754589081
2022-01-11 19:47:37,889 - INFO - 
Epoch: 46
2022-01-11 19:47:37,889 - INFO - 
Learning Rate: 0.0100
2022-01-11 19:48:26,776 - INFO - [Step=38500]	Loss=0.7247	228.8 examples/second
2022-01-11 19:50:35,105 - INFO - [Step=38750]	Loss=0.7174	249.4 examples/second
2022-01-11 19:52:44,953 - INFO - [Step=39000]	Loss=0.7199	246.4 examples/second
2022-01-11 19:54:59,683 - INFO - Test Loss=0.8158, Test top-1 acc=0.7701
2022-01-11 19:54:59,683 - INFO - Group Accuracy:

2022-01-11 19:54:59,683 - INFO - [0.97903615 0.9913253  0.98578316 0.9920482  0.9840964  0.99421686
 0.9785542  0.97951806 0.99156624 0.97542167 0.9920482  0.98313254
 0.97638553 0.9821687  0.97831327 0.98963857 0.9961446 ]
2022-01-11 19:54:59,685 - INFO - Epoch time: 441.79519510269165
2022-01-11 19:54:59,685 - INFO - 
Epoch: 47
2022-01-11 19:54:59,685 - INFO - 
Learning Rate: 0.0100
2022-01-11 19:55:04,564 - INFO - [Step=39250]	Loss=0.7108	229.2 examples/second
2022-01-11 19:57:13,608 - INFO - [Step=39500]	Loss=0.6970	248.0 examples/second
2022-01-11 19:59:23,140 - INFO - [Step=39750]	Loss=0.7068	247.0 examples/second
2022-01-11 20:01:32,645 - INFO - [Step=40000]	Loss=0.7023	247.1 examples/second
2022-01-11 20:02:22,129 - INFO - Test Loss=0.8124, Test top-1 acc=0.7766
2022-01-11 20:02:22,130 - INFO - Group Accuracy:

2022-01-11 20:02:22,130 - INFO - [0.9812048  0.99084336 0.98578316 0.993494   0.9840964  0.9951807
 0.98024094 0.9819277  0.99060243 0.97445786 0.99036145 0.98289156
 0.9766265  0.98313254 0.9785542  0.9881928  0.9959036 ]
2022-01-11 20:02:22,131 - INFO - Epoch time: 442.4461290836334
2022-01-11 20:02:22,131 - INFO - 
Epoch: 48
2022-01-11 20:02:22,131 - INFO - 
Learning Rate: 0.0100
2022-01-11 20:03:52,030 - INFO - [Step=40250]	Loss=0.6955	229.6 examples/second
2022-01-11 20:06:02,031 - INFO - [Step=40500]	Loss=0.6863	246.2 examples/second
2022-01-11 20:08:11,239 - INFO - [Step=40750]	Loss=0.6966	247.7 examples/second
2022-01-11 20:09:45,019 - INFO - Test Loss=0.8109, Test top-1 acc=0.7766
2022-01-11 20:09:45,019 - INFO - Group Accuracy:

2022-01-11 20:09:45,019 - INFO - [0.97975904 0.9918072  0.98650604 0.993253   0.9840964  0.99373496
 0.9812048  0.9809638  0.99036145 0.9780723  0.9913253  0.9821687
 0.9787952  0.98240966 0.9768675  0.9891566  0.9963855 ]
2022-01-11 20:09:45,020 - INFO - Epoch time: 442.8891222476959
2022-01-11 20:09:45,020 - INFO - 
Epoch: 49
2022-01-11 20:09:45,020 - INFO - 
Learning Rate: 0.0100
2022-01-11 20:10:31,140 - INFO - [Step=41000]	Loss=0.7055	228.7 examples/second
2022-01-11 20:12:40,450 - INFO - [Step=41250]	Loss=0.6824	247.5 examples/second
2022-01-11 20:14:49,842 - INFO - [Step=41500]	Loss=0.6844	247.3 examples/second
2022-01-11 20:16:59,093 - INFO - [Step=41750]	Loss=0.7064	247.6 examples/second
2022-01-11 20:17:07,324 - INFO - Test Loss=0.8222, Test top-1 acc=0.7790
2022-01-11 20:17:07,325 - INFO - Group Accuracy:

2022-01-11 20:17:07,325 - INFO - [0.9787952  0.99084336 0.98698795 0.9925301  0.98313254 0.9954217
 0.9809638  0.9826506  0.9898795  0.97638553 0.99036145 0.98313254
 0.9768675  0.98024094 0.9773494  0.9884337  0.99493974]
2022-01-11 20:17:07,326 - INFO - Saving...
2022-01-11 20:17:07,563 - INFO - Epoch time: 442.5431339740753
2022-01-11 20:17:07,563 - INFO - 
Epoch: 50
2022-01-11 20:17:07,563 - INFO - 
Learning Rate: 0.0100
2022-01-11 20:19:18,987 - INFO - [Step=42000]	Loss=0.6757	228.8 examples/second
2022-01-11 20:21:27,608 - INFO - [Step=42250]	Loss=0.6806	248.8 examples/second
2022-01-11 20:23:36,257 - INFO - [Step=42500]	Loss=0.6948	248.7 examples/second
2022-01-11 20:24:28,130 - INFO - Test Loss=0.8427, Test top-1 acc=0.7728
2022-01-11 20:24:28,131 - INFO - Group Accuracy:

2022-01-11 20:24:28,131 - INFO - [0.9780723  0.99060243 0.9853012  0.9927711  0.98433733 0.99493974
 0.97951806 0.98240966 0.98963857 0.97638553 0.9901205  0.9833735
 0.9775904  0.9821687  0.97614455 0.9898795  0.99710846]
2022-01-11 20:24:28,132 - INFO - Epoch time: 440.5687344074249
2022-01-11 20:24:28,132 - INFO - 
Epoch: 51
2022-01-11 20:24:28,132 - INFO - 
Learning Rate: 0.0100
2022-01-11 20:25:56,218 - INFO - [Step=42750]	Loss=0.7002	228.6 examples/second
2022-01-11 20:28:05,945 - INFO - [Step=43000]	Loss=0.6713	246.7 examples/second
2022-01-11 20:30:15,596 - INFO - [Step=43250]	Loss=0.6891	246.8 examples/second
2022-01-11 20:31:51,472 - INFO - Test Loss=0.8233, Test top-1 acc=0.7793
2022-01-11 20:31:51,473 - INFO - Group Accuracy:

2022-01-11 20:31:51,473 - INFO - [0.9804819  0.99060243 0.9853012  0.993494   0.9840964  0.9954217
 0.98024094 0.9826506  0.9898795  0.97638553 0.9898795  0.98361444
 0.9773494  0.9809638  0.97951806 0.98746985 0.9963855 ]
2022-01-11 20:31:51,474 - INFO - Saving...
2022-01-11 20:31:51,733 - INFO - Epoch time: 443.6010699272156
2022-01-11 20:31:51,734 - INFO - 
Epoch: 52
2022-01-11 20:31:51,734 - INFO - 
Learning Rate: 0.0100
2022-01-11 20:32:35,591 - INFO - [Step=43500]	Loss=0.6758	228.6 examples/second
2022-01-11 20:34:44,794 - INFO - [Step=43750]	Loss=0.6758	247.7 examples/second
2022-01-11 20:36:53,607 - INFO - [Step=44000]	Loss=0.6654	248.4 examples/second
2022-01-11 20:39:04,587 - INFO - [Step=44250]	Loss=0.6816	244.3 examples/second
2022-01-11 20:39:15,675 - INFO - Test Loss=0.8491, Test top-1 acc=0.7800
2022-01-11 20:39:15,675 - INFO - Group Accuracy:

2022-01-11 20:39:15,675 - INFO - [0.97975904 0.99060243 0.98626506 0.9939759  0.9848193  0.9961446
 0.9807229  0.97975904 0.9886747  0.97614455 0.9922892  0.9819277
 0.97590363 0.9819277  0.97927713 0.9893976  0.9963855 ]
2022-01-11 20:39:15,676 - INFO - Saving...
2022-01-11 20:39:15,940 - INFO - Epoch time: 444.2064428329468
2022-01-11 20:39:15,940 - INFO - 
Epoch: 53
2022-01-11 20:39:15,940 - INFO - 
Learning Rate: 0.0100
2022-01-11 20:41:28,202 - INFO - [Step=44500]	Loss=0.6599	222.8 examples/second
2022-01-11 20:43:41,027 - INFO - [Step=44750]	Loss=0.6686	240.9 examples/second
2022-01-11 20:45:54,705 - INFO - [Step=45000]	Loss=0.6891	239.4 examples/second
2022-01-11 20:46:50,852 - INFO - Test Loss=0.8166, Test top-1 acc=0.7812
2022-01-11 20:46:50,852 - INFO - Group Accuracy:

2022-01-11 20:46:50,852 - INFO - [0.97927713 0.9922892  0.98771083 0.9913253  0.9840964  0.9951807
 0.98024094 0.9821687  0.98963857 0.97518075 0.9913253  0.98433733
 0.9778313  0.9814458  0.97831327 0.9893976  0.9966265 ]
2022-01-11 20:46:50,853 - INFO - Saving...
2022-01-11 20:46:51,171 - INFO - Epoch time: 455.23083567619324
2022-01-11 20:46:51,171 - INFO - 
Epoch: 54
2022-01-11 20:46:51,171 - INFO - 
Learning Rate: 0.0100
2022-01-11 20:48:18,558 - INFO - [Step=45250]	Loss=0.6534	222.5 examples/second
2022-01-11 20:50:31,013 - INFO - [Step=45500]	Loss=0.6649	241.6 examples/second
2022-01-11 20:52:44,374 - INFO - [Step=45750]	Loss=0.6646	240.0 examples/second
2022-01-11 20:54:25,736 - INFO - Test Loss=0.8298, Test top-1 acc=0.7805
2022-01-11 20:54:25,736 - INFO - Group Accuracy:

2022-01-11 20:54:25,737 - INFO - [0.97951806 0.9893976  0.98578316 0.9925301  0.9840964  0.9956626
 0.9814458  0.9816868  0.9886747  0.97566265 0.9913253  0.9819277
 0.9778313  0.9816868  0.9771084  0.9893976  0.9963855 ]
2022-01-11 20:54:25,737 - INFO - Epoch time: 454.565949678421
2022-01-11 20:54:25,737 - INFO - 
Epoch: 55
2022-01-11 20:54:25,737 - INFO - 
Learning Rate: 0.0100
2022-01-11 20:55:07,348 - INFO - [Step=46000]	Loss=0.6570	223.8 examples/second
2022-01-11 20:57:19,807 - INFO - [Step=46250]	Loss=0.6581	241.6 examples/second
2022-01-11 20:59:33,065 - INFO - [Step=46500]	Loss=0.6633	240.1 examples/second
2022-01-11 21:01:45,347 - INFO - [Step=46750]	Loss=0.6619	241.9 examples/second
2022-01-11 21:01:59,229 - INFO - Test Loss=0.8276, Test top-1 acc=0.7786
2022-01-11 21:01:59,230 - INFO - Group Accuracy:

2022-01-11 21:01:59,230 - INFO - [0.98       0.9913253  0.98650604 0.9927711  0.9845783  0.9966265
 0.9785542  0.98289156 0.9884337  0.9768675  0.9891566  0.98578316
 0.97831327 0.9816868  0.97590363 0.98963857 0.9963855 ]
2022-01-11 21:01:59,230 - INFO - Epoch time: 453.49318170547485
2022-01-11 21:01:59,231 - INFO - 
Epoch: 56
2022-01-11 21:01:59,231 - INFO - 
Learning Rate: 0.0100
2022-01-11 21:04:08,326 - INFO - [Step=47000]	Loss=0.6469	223.8 examples/second
2022-01-11 21:06:22,170 - INFO - [Step=47250]	Loss=0.6639	239.1 examples/second
2022-01-11 21:08:34,579 - INFO - [Step=47500]	Loss=0.6651	241.7 examples/second
2022-01-11 21:09:33,543 - INFO - Test Loss=0.8391, Test top-1 acc=0.7839
2022-01-11 21:09:33,543 - INFO - Group Accuracy:

2022-01-11 21:09:33,543 - INFO - [0.9812048  0.99156624 0.9848193  0.993253   0.9840964  0.99493974
 0.9807229  0.9840964  0.9884337  0.9771084  0.98963857 0.9845783
 0.9775904  0.98024094 0.9785542  0.9884337  0.9966265 ]
2022-01-11 21:09:33,544 - INFO - Saving...
2022-01-11 21:09:33,837 - INFO - Epoch time: 454.6065185070038
2022-01-11 21:09:33,837 - INFO - 
Epoch: 57
2022-01-11 21:09:33,837 - INFO - 
Learning Rate: 0.0100
2022-01-11 21:10:58,352 - INFO - [Step=47750]	Loss=0.6495	222.6 examples/second
2022-01-11 21:13:12,355 - INFO - [Step=48000]	Loss=0.6409	238.8 examples/second
2022-01-11 21:15:24,486 - INFO - [Step=48250]	Loss=0.6605	242.2 examples/second
2022-01-11 21:17:08,215 - INFO - Test Loss=0.8667, Test top-1 acc=0.7766
2022-01-11 21:17:08,215 - INFO - Group Accuracy:

2022-01-11 21:17:08,215 - INFO - [0.97831327 0.9901205  0.9838554  0.9930121  0.9833735  0.9951807
 0.97927713 0.9821687  0.99084336 0.97542167 0.9913253  0.98674697
 0.9766265  0.9807229  0.97638553 0.98771083 0.9956626 ]
2022-01-11 21:17:08,216 - INFO - Epoch time: 454.37861227989197
2022-01-11 21:17:08,216 - INFO - 
Epoch: 58
2022-01-11 21:17:08,216 - INFO - 
Learning Rate: 0.0100
2022-01-11 21:17:47,504 - INFO - [Step=48500]	Loss=0.6574	223.7 examples/second
2022-01-11 21:20:01,929 - INFO - [Step=48750]	Loss=0.6365	238.1 examples/second
2022-01-11 21:22:14,645 - INFO - [Step=49000]	Loss=0.6358	241.1 examples/second
2022-01-11 21:24:27,365 - INFO - [Step=49250]	Loss=0.6442	241.1 examples/second
2022-01-11 21:24:43,647 - INFO - Test Loss=0.8657, Test top-1 acc=0.7761
2022-01-11 21:24:43,647 - INFO - Group Accuracy:

2022-01-11 21:24:43,647 - INFO - [0.98       0.9884337  0.98674697 0.9939759  0.98674697 0.99493974
 0.9807229  0.97975904 0.99036145 0.97566265 0.9893976  0.9812048
 0.97493976 0.9816868  0.97927713 0.9886747  0.9956626 ]
2022-01-11 21:24:43,648 - INFO - Epoch time: 455.4319484233856
2022-01-11 21:24:43,648 - INFO - 
Epoch: 59
2022-01-11 21:24:43,648 - INFO - 
Learning Rate: 0.0010
2022-01-11 21:26:52,846 - INFO - [Step=49500]	Loss=0.6122	220.0 examples/second
2022-01-11 21:29:06,162 - INFO - [Step=49750]	Loss=0.5822	240.0 examples/second
2022-01-11 21:31:19,187 - INFO - [Step=50000]	Loss=0.5609	240.6 examples/second
2022-01-11 21:32:21,428 - INFO - Test Loss=0.7707, Test top-1 acc=0.7947
2022-01-11 21:32:21,428 - INFO - Group Accuracy:

2022-01-11 21:32:21,428 - INFO - [0.9819277  0.9925301  0.9881928  0.993253   0.98626506 0.9959036
 0.9821687  0.98313254 0.9901205  0.97638553 0.99156624 0.9850602
 0.9775904  0.98240966 0.97831327 0.9884337  0.9966265 ]
2022-01-11 21:32:21,429 - INFO - Saving...
2022-01-11 21:32:21,678 - INFO - Epoch time: 458.0300006866455
2022-01-11 21:32:21,678 - INFO - 
Epoch: 60
2022-01-11 21:32:21,678 - INFO - 
Learning Rate: 0.0010
2022-01-11 21:33:45,044 - INFO - [Step=50250]	Loss=0.5553	219.4 examples/second
2022-01-11 21:35:57,897 - INFO - [Step=50500]	Loss=0.5731	240.9 examples/second
2022-01-11 21:38:10,916 - INFO - [Step=50750]	Loss=0.5541	240.6 examples/second
2022-01-11 21:39:58,328 - INFO - Test Loss=0.7685, Test top-1 acc=0.7966
2022-01-11 21:39:58,328 - INFO - Group Accuracy:

2022-01-11 21:39:58,329 - INFO - [0.9814458  0.9930121  0.9881928  0.993494   0.98746985 0.9959036
 0.9821687  0.98289156 0.9901205  0.97614455 0.9920482  0.9848193
 0.97831327 0.9826506  0.97975904 0.9889157  0.9963855 ]
2022-01-11 21:39:58,330 - INFO - Saving...
2022-01-11 21:39:58,587 - INFO - Epoch time: 456.908406496048
2022-01-11 21:39:58,587 - INFO - 
Epoch: 61
2022-01-11 21:39:58,587 - INFO - 
Learning Rate: 0.0010
2022-01-11 21:40:35,495 - INFO - [Step=51000]	Loss=0.5584	221.3 examples/second
2022-01-11 21:42:48,328 - INFO - [Step=51250]	Loss=0.5401	240.9 examples/second
2022-01-11 21:45:00,555 - INFO - [Step=51500]	Loss=0.5463	242.0 examples/second
2022-01-11 21:47:13,010 - INFO - [Step=51750]	Loss=0.5401	241.6 examples/second
2022-01-11 21:47:31,901 - INFO - Test Loss=0.7677, Test top-1 acc=0.7961
2022-01-11 21:47:31,901 - INFO - Group Accuracy:

2022-01-11 21:47:31,901 - INFO - [0.9812048  0.9922892  0.98722893 0.993494   0.98698795 0.9961446
 0.9819277  0.98289156 0.9901205  0.97614455 0.9918072  0.9845783
 0.97927713 0.98313254 0.97951806 0.9891566  0.99710846]
2022-01-11 21:47:31,902 - INFO - Epoch time: 453.31486654281616
2022-01-11 21:47:31,902 - INFO - 
Epoch: 62
2022-01-11 21:47:31,902 - INFO - 
Learning Rate: 0.0010
2022-01-11 21:49:36,276 - INFO - [Step=52000]	Loss=0.5374	223.4 examples/second
2022-01-11 21:51:48,318 - INFO - [Step=52250]	Loss=0.5357	242.3 examples/second
2022-01-11 21:54:01,322 - INFO - [Step=52500]	Loss=0.5473	240.6 examples/second
2022-01-11 21:55:05,762 - INFO - Test Loss=0.7739, Test top-1 acc=0.7973
2022-01-11 21:55:05,763 - INFO - Group Accuracy:

2022-01-11 21:55:05,763 - INFO - [0.9814458  0.9920482  0.98746985 0.9930121  0.98674697 0.9963855
 0.9826506  0.98313254 0.98963857 0.97590363 0.9927711  0.9850602
 0.9775904  0.98361444 0.97951806 0.9893976  0.9966265 ]
2022-01-11 21:55:05,764 - INFO - Saving...
2022-01-11 21:55:06,001 - INFO - Epoch time: 454.0991837978363
2022-01-11 21:55:06,001 - INFO - 
Epoch: 63
2022-01-11 21:55:06,001 - INFO - 
Learning Rate: 0.0010
2022-01-11 21:56:24,833 - INFO - [Step=52750]	Loss=0.5391	223.0 examples/second
2022-01-11 21:58:37,162 - INFO - [Step=53000]	Loss=0.5403	241.8 examples/second
2022-01-11 22:00:50,035 - INFO - [Step=53250]	Loss=0.5373	240.8 examples/second
2022-01-11 22:02:39,521 - INFO - Test Loss=0.7717, Test top-1 acc=0.7952
2022-01-11 22:02:39,522 - INFO - Group Accuracy:

2022-01-11 22:02:39,522 - INFO - [0.9819277  0.9930121  0.98771083 0.99373496 0.98722893 0.9963855
 0.98240966 0.9821687  0.99060243 0.97590363 0.9925301  0.9840964
 0.97831327 0.9833735  0.97903615 0.98963857 0.9966265 ]
2022-01-11 22:02:39,523 - INFO - Epoch time: 453.5211811065674
2022-01-11 22:02:39,523 - INFO - 
Epoch: 64
2022-01-11 22:02:39,523 - INFO - 
Learning Rate: 0.0010
2022-01-11 22:03:13,524 - INFO - [Step=53500]	Loss=0.5321	223.0 examples/second
2022-01-11 22:05:25,968 - INFO - [Step=53750]	Loss=0.5239	241.6 examples/second
2022-01-11 22:07:39,418 - INFO - [Step=54000]	Loss=0.5359	239.8 examples/second
2022-01-11 22:09:52,053 - INFO - [Step=54250]	Loss=0.5219	241.3 examples/second
2022-01-11 22:10:13,404 - INFO - Test Loss=0.7743, Test top-1 acc=0.7937
2022-01-11 22:10:13,404 - INFO - Group Accuracy:

2022-01-11 22:10:13,404 - INFO - [0.9819277  0.9920482  0.9879518  0.993253   0.98746985 0.9963855
 0.9814458  0.98289156 0.98963857 0.97590363 0.9925301  0.98433733
 0.9775904  0.9821687  0.97975904 0.9889157  0.9966265 ]
2022-01-11 22:10:13,405 - INFO - Epoch time: 453.8820526599884
2022-01-11 22:10:13,405 - INFO - 
Epoch: 65
2022-01-11 22:10:13,405 - INFO - 
Learning Rate: 0.0010
2022-01-11 22:12:14,597 - INFO - [Step=54500]	Loss=0.5061	224.5 examples/second
2022-01-11 22:14:28,358 - INFO - [Step=54750]	Loss=0.5315	239.2 examples/second
2022-01-11 22:16:41,270 - INFO - [Step=55000]	Loss=0.5451	240.8 examples/second
2022-01-11 22:17:48,271 - INFO - Test Loss=0.7741, Test top-1 acc=0.7916
2022-01-11 22:17:48,272 - INFO - Group Accuracy:

2022-01-11 22:17:48,272 - INFO - [0.9816868  0.9918072  0.9881928  0.993253   0.98698795 0.9963855
 0.9819277  0.9826506  0.9901205  0.97638553 0.9922892  0.9845783
 0.97831327 0.98289156 0.97975904 0.9891566  0.9966265 ]
2022-01-11 22:17:48,273 - INFO - Epoch time: 454.86821961402893
2022-01-11 22:17:48,273 - INFO - 
Epoch: 66
2022-01-11 22:17:48,273 - INFO - 
Learning Rate: 0.0010
2022-01-11 22:19:05,084 - INFO - [Step=55250]	Loss=0.5204	222.5 examples/second
2022-01-11 22:21:17,947 - INFO - [Step=55500]	Loss=0.5153	240.9 examples/second
2022-01-11 22:23:30,938 - INFO - [Step=55750]	Loss=0.5047	240.6 examples/second
2022-01-11 22:25:22,627 - INFO - Test Loss=0.7710, Test top-1 acc=0.7973
2022-01-11 22:25:22,627 - INFO - Group Accuracy:

2022-01-11 22:25:22,627 - INFO - [0.9819277  0.9918072  0.9881928  0.9925301  0.98674697 0.9963855
 0.9833735  0.98313254 0.9901205  0.9768675  0.9927711  0.9853012
 0.97831327 0.98240966 0.98       0.98963857 0.9966265 ]
2022-01-11 22:25:22,628 - INFO - Epoch time: 454.35517024993896
2022-01-11 22:25:22,628 - INFO - 
Epoch: 67
2022-01-11 22:25:22,628 - INFO - 
Learning Rate: 0.0010
2022-01-11 22:25:54,792 - INFO - [Step=56000]	Loss=0.5259	222.4 examples/second
2022-01-11 22:28:07,784 - INFO - [Step=56250]	Loss=0.5191	240.6 examples/second
2022-01-11 22:30:21,117 - INFO - [Step=56500]	Loss=0.5246	240.0 examples/second
2022-01-11 22:32:35,068 - INFO - [Step=56750]	Loss=0.5213	238.9 examples/second
2022-01-11 22:32:59,494 - INFO - Test Loss=0.7795, Test top-1 acc=0.7945
2022-01-11 22:32:59,494 - INFO - Group Accuracy:

2022-01-11 22:32:59,494 - INFO - [0.9804819  0.9922892  0.98746985 0.9925301  0.98722893 0.9966265
 0.9826506  0.9819277  0.9898795  0.97590363 0.9925301  0.9855422
 0.97927713 0.9826506  0.97927713 0.98963857 0.9961446 ]
2022-01-11 22:32:59,495 - INFO - Epoch time: 456.86657190322876
2022-01-11 22:32:59,495 - INFO - 
Epoch: 68
2022-01-11 22:32:59,495 - INFO - 
Learning Rate: 0.0010
2022-01-11 22:34:58,612 - INFO - [Step=57000]	Loss=0.5174	222.9 examples/second
2022-01-11 22:37:11,271 - INFO - [Step=57250]	Loss=0.5135	241.2 examples/second
2022-01-11 22:39:25,235 - INFO - [Step=57500]	Loss=0.5140	238.9 examples/second
2022-01-11 22:40:34,271 - INFO - Test Loss=0.7739, Test top-1 acc=0.7983
2022-01-11 22:40:34,271 - INFO - Group Accuracy:

2022-01-11 22:40:34,271 - INFO - [0.9807229  0.9920482  0.98722893 0.9930121  0.98746985 0.9961446
 0.98240966 0.98313254 0.9898795  0.97542167 0.9930121  0.98433733
 0.97927713 0.9838554  0.97951806 0.9898795  0.99710846]
2022-01-11 22:40:34,272 - INFO - Saving...
2022-01-11 22:40:34,507 - INFO - Epoch time: 455.01159167289734
2022-01-11 22:40:34,507 - INFO - 
Epoch: 69
2022-01-11 22:40:34,507 - INFO - 
Learning Rate: 0.0010
2022-01-11 22:41:48,344 - INFO - [Step=57750]	Loss=0.5292	223.6 examples/second
2022-01-11 22:44:00,896 - INFO - [Step=58000]	Loss=0.5059	241.4 examples/second
2022-01-11 22:46:15,423 - INFO - [Step=58250]	Loss=0.5148	237.9 examples/second
2022-01-11 22:48:09,331 - INFO - Test Loss=0.7766, Test top-1 acc=0.7925
2022-01-11 22:48:09,331 - INFO - Group Accuracy:

2022-01-11 22:48:09,331 - INFO - [0.9809638  0.9925301  0.9879518  0.9930121  0.98578316 0.9961446
 0.9821687  0.9826506  0.9898795  0.97566265 0.9930121  0.9848193
 0.9785542  0.98313254 0.97975904 0.9891566  0.9966265 ]
2022-01-11 22:48:09,332 - INFO - Epoch time: 454.82519483566284
2022-01-11 22:48:09,332 - INFO - 
Epoch: 70
2022-01-11 22:48:09,332 - INFO - 
Learning Rate: 0.0010
2022-01-11 22:48:37,993 - INFO - [Step=58500]	Loss=0.5114	224.5 examples/second
2022-01-11 22:50:49,983 - INFO - [Step=58750]	Loss=0.5157	242.4 examples/second
2022-01-11 22:53:03,404 - INFO - [Step=59000]	Loss=0.5089	239.8 examples/second
2022-01-11 22:55:15,082 - INFO - [Step=59250]	Loss=0.5178	243.0 examples/second
2022-01-11 22:55:41,478 - INFO - Test Loss=0.7771, Test top-1 acc=0.7945
2022-01-11 22:55:41,479 - INFO - Group Accuracy:

2022-01-11 22:55:41,479 - INFO - [0.9812048  0.9927711  0.98771083 0.993494   0.98698795 0.9963855
 0.9816868  0.98313254 0.9901205  0.97566265 0.9930121  0.9855422
 0.97927713 0.98433733 0.97927713 0.9884337  0.99710846]
2022-01-11 22:55:41,479 - INFO - Epoch time: 452.1472804546356
2022-01-11 22:55:41,479 - INFO - 
Epoch: 71
2022-01-11 22:55:41,480 - INFO - 
Learning Rate: 0.0010
2022-01-11 22:57:36,294 - INFO - [Step=59500]	Loss=0.4998	226.6 examples/second
2022-01-11 22:59:47,960 - INFO - [Step=59750]	Loss=0.5081	243.0 examples/second
2022-01-11 23:01:58,258 - INFO - [Step=60000]	Loss=0.5159	245.6 examples/second
2022-01-11 23:03:09,314 - INFO - Test Loss=0.7780, Test top-1 acc=0.7993
2022-01-11 23:03:09,314 - INFO - Group Accuracy:

2022-01-11 23:03:09,314 - INFO - [0.9819277  0.9922892  0.98746985 0.993253   0.98626506 0.9961446
 0.98289156 0.98240966 0.9898795  0.97614455 0.9922892  0.9853012
 0.97903615 0.98289156 0.98024094 0.9884337  0.99710846]
2022-01-11 23:03:09,315 - INFO - Saving...
2022-01-11 23:03:09,577 - INFO - Epoch time: 448.09701585769653
2022-01-11 23:03:09,577 - INFO - 
Epoch: 72
2022-01-11 23:03:09,577 - INFO - 
Learning Rate: 0.0010
2022-01-11 23:04:19,676 - INFO - [Step=60250]	Loss=0.5072	226.3 examples/second
2022-01-11 23:06:31,213 - INFO - [Step=60500]	Loss=0.4989	243.3 examples/second
2022-01-11 23:08:41,566 - INFO - [Step=60750]	Loss=0.4937	245.5 examples/second
2022-01-11 23:10:37,220 - INFO - Test Loss=0.7866, Test top-1 acc=0.7983
2022-01-11 23:10:37,220 - INFO - Group Accuracy:

2022-01-11 23:10:37,220 - INFO - [0.9812048  0.9920482  0.9879518  0.9930121  0.98771083 0.9959036
 0.9819277  0.9833735  0.9901205  0.9746988  0.9913253  0.98626506
 0.9775904  0.9821687  0.9807229  0.9884337  0.9968675 ]
2022-01-11 23:10:37,221 - INFO - Epoch time: 447.6440086364746
2022-01-11 23:10:37,221 - INFO - 
Epoch: 73
2022-01-11 23:10:37,221 - INFO - 
Learning Rate: 0.0010
2022-01-11 23:11:03,055 - INFO - [Step=61000]	Loss=0.5159	226.2 examples/second
2022-01-11 23:13:15,511 - INFO - [Step=61250]	Loss=0.5076	241.6 examples/second
2022-01-11 23:15:26,044 - INFO - [Step=61500]	Loss=0.5021	245.1 examples/second
2022-01-11 23:17:36,942 - INFO - [Step=61750]	Loss=0.5071	244.5 examples/second
2022-01-11 23:18:06,145 - INFO - Test Loss=0.7775, Test top-1 acc=0.7978
2022-01-11 23:18:06,145 - INFO - Group Accuracy:

2022-01-11 23:18:06,145 - INFO - [0.9821687  0.9918072  0.98722893 0.993253   0.98698795 0.9961446
 0.9833735  0.98240966 0.98963857 0.97614455 0.9920482  0.9853012
 0.9785542  0.98289156 0.97951806 0.9884337  0.9968675 ]
2022-01-11 23:18:06,147 - INFO - Epoch time: 448.9257996082306
2022-01-11 23:18:06,147 - INFO - 
Epoch: 74
2022-01-11 23:18:06,147 - INFO - 
Learning Rate: 0.0010
2022-01-11 23:19:58,679 - INFO - [Step=62000]	Loss=0.5146	225.8 examples/second
2022-01-11 23:22:08,436 - INFO - [Step=62250]	Loss=0.4944	246.6 examples/second
2022-01-11 23:24:19,499 - INFO - [Step=62500]	Loss=0.4890	244.2 examples/second
2022-01-11 23:25:33,110 - INFO - Test Loss=0.7790, Test top-1 acc=0.7952
2022-01-11 23:25:33,110 - INFO - Group Accuracy:

2022-01-11 23:25:33,110 - INFO - [0.9819277  0.9922892  0.98722893 0.9927711  0.9879518  0.9963855
 0.9819277  0.9826506  0.98963857 0.97614455 0.9927711  0.98626506
 0.9780723  0.98313254 0.97951806 0.9886747  0.9966265 ]
2022-01-11 23:25:33,111 - INFO - Epoch time: 446.96409368515015
2022-01-11 23:25:33,111 - INFO - 
Epoch: 75
2022-01-11 23:25:33,111 - INFO - 
Learning Rate: 0.0010
2022-01-11 23:26:41,085 - INFO - [Step=62750]	Loss=0.4948	226.0 examples/second
2022-01-11 23:28:51,598 - INFO - [Step=63000]	Loss=0.4912	245.2 examples/second
2022-01-11 23:31:01,813 - INFO - [Step=63250]	Loss=0.4942	245.7 examples/second
2022-01-11 23:32:59,580 - INFO - Test Loss=0.7838, Test top-1 acc=0.7993
2022-01-11 23:32:59,580 - INFO - Group Accuracy:

2022-01-11 23:32:59,580 - INFO - [0.9816868  0.9920482  0.98746985 0.993253   0.98722893 0.9961446
 0.9826506  0.98313254 0.9898795  0.97638553 0.9925301  0.9860241
 0.97903615 0.98289156 0.97951806 0.9898795  0.99710846]
2022-01-11 23:32:59,581 - INFO - Epoch time: 446.47052359580994
2022-01-11 23:32:59,581 - INFO - 
Epoch: 76
2022-01-11 23:32:59,582 - INFO - 
Learning Rate: 0.0010
2022-01-11 23:33:22,532 - INFO - [Step=63500]	Loss=0.5029	227.4 examples/second
2022-01-11 23:35:32,368 - INFO - [Step=63750]	Loss=0.4998	246.5 examples/second
2022-01-11 23:37:42,668 - INFO - [Step=64000]	Loss=0.4859	245.6 examples/second
2022-01-11 23:39:53,679 - INFO - [Step=64250]	Loss=0.4763	244.3 examples/second
2022-01-11 23:40:25,409 - INFO - Test Loss=0.7779, Test top-1 acc=0.7971
2022-01-11 23:40:25,409 - INFO - Group Accuracy:

2022-01-11 23:40:25,419 - INFO - [0.9814458  0.9920482  0.98746985 0.993494   0.98722893 0.9961446
 0.9816868  0.98240966 0.98963857 0.97590363 0.9930121  0.98578316
 0.97903615 0.9840964  0.97951806 0.9901205  0.9966265 ]
2022-01-11 23:40:25,420 - INFO - Epoch time: 445.83809638023376
2022-01-11 23:40:25,420 - INFO - 
Epoch: 77
2022-01-11 23:40:25,420 - INFO - 
Learning Rate: 0.0010
2022-01-11 23:42:15,290 - INFO - [Step=64500]	Loss=0.4955	226.0 examples/second
2022-01-11 23:44:26,174 - INFO - [Step=64750]	Loss=0.4937	244.5 examples/second
2022-01-11 23:46:38,040 - INFO - [Step=65000]	Loss=0.4920	242.7 examples/second
2022-01-11 23:47:56,352 - INFO - Test Loss=0.7875, Test top-1 acc=0.7949
2022-01-11 23:47:56,352 - INFO - Group Accuracy:

2022-01-11 23:47:56,352 - INFO - [0.9826506  0.9922892  0.9879518  0.9930121  0.98746985 0.9961446
 0.98240966 0.98240966 0.9901205  0.97566265 0.9925301  0.98626506
 0.9787952  0.9821687  0.98024094 0.9898795  0.99710846]
2022-01-11 23:47:56,353 - INFO - Epoch time: 450.9336223602295
2022-01-11 23:47:56,353 - INFO - 
Epoch: 78
2022-01-11 23:47:56,353 - INFO - 
Learning Rate: 0.0010
2022-01-11 23:49:02,913 - INFO - [Step=65250]	Loss=0.4905	220.9 examples/second
2022-01-11 23:51:14,138 - INFO - [Step=65500]	Loss=0.4913	243.9 examples/second
2022-01-11 23:53:25,620 - INFO - [Step=65750]	Loss=0.4875	243.4 examples/second
2022-01-11 23:55:26,209 - INFO - Test Loss=0.7882, Test top-1 acc=0.7959
2022-01-11 23:55:26,210 - INFO - Group Accuracy:

2022-01-11 23:55:26,210 - INFO - [0.9819277  0.9920482  0.98698795 0.993494   0.98650604 0.9961446
 0.9816868  0.98361444 0.9898795  0.97542167 0.9927711  0.98578316
 0.9773494  0.9840964  0.97927713 0.9889157  0.9966265 ]
2022-01-11 23:55:26,211 - INFO - Epoch time: 449.85722374916077
2022-01-11 23:55:26,211 - INFO - 
Epoch: 79
2022-01-11 23:55:26,211 - INFO - 
Learning Rate: 0.0010
2022-01-11 23:55:46,755 - INFO - [Step=66000]	Loss=0.4972	226.7 examples/second
2022-01-11 23:57:57,117 - INFO - [Step=66250]	Loss=0.4966	245.5 examples/second
2022-01-12 00:00:08,874 - INFO - [Step=66500]	Loss=0.4886	242.9 examples/second
2022-01-12 00:02:19,834 - INFO - [Step=66750]	Loss=0.4963	244.3 examples/second
2022-01-12 00:02:54,450 - INFO - Test Loss=0.7851, Test top-1 acc=0.7976
2022-01-12 00:02:54,450 - INFO - Group Accuracy:

2022-01-12 00:02:54,450 - INFO - [0.9819277  0.99156624 0.9884337  0.9927711  0.98698795 0.9963855
 0.98240966 0.9826506  0.9901205  0.97614455 0.9927711  0.9855422
 0.9766265  0.9826506  0.98024094 0.98963857 0.99710846]
2022-01-12 00:02:54,451 - INFO - Epoch time: 448.2398569583893
2022-01-12 00:02:54,451 - INFO - 
Epoch: 80
2022-01-12 00:02:54,451 - INFO - 
Learning Rate: 0.0010
2022-01-12 00:04:41,082 - INFO - [Step=67000]	Loss=0.4790	226.6 examples/second
2022-01-12 00:06:52,075 - INFO - [Step=67250]	Loss=0.4880	244.3 examples/second
2022-01-12 00:09:01,492 - INFO - [Step=67500]	Loss=0.4873	247.3 examples/second
2022-01-12 00:10:19,433 - INFO - Test Loss=0.7902, Test top-1 acc=0.7969
2022-01-12 00:10:19,433 - INFO - Group Accuracy:

2022-01-12 00:10:19,433 - INFO - [0.9816868  0.9922892  0.98746985 0.993253   0.98650604 0.9959036
 0.9819277  0.98289156 0.9898795  0.9771084  0.993253   0.9853012
 0.97831327 0.98289156 0.97951806 0.9898795  0.9968675 ]
2022-01-12 00:10:19,434 - INFO - Epoch time: 444.9833436012268
2022-01-12 00:10:19,434 - INFO - 
Epoch: 81
2022-01-12 00:10:19,434 - INFO - 
Learning Rate: 0.0010
2022-01-12 00:11:20,687 - INFO - [Step=67750]	Loss=0.4859	229.9 examples/second
2022-01-12 00:13:29,870 - INFO - [Step=68000]	Loss=0.4873	247.7 examples/second
2022-01-12 00:15:38,718 - INFO - [Step=68250]	Loss=0.4890	248.4 examples/second
2022-01-12 00:17:39,428 - INFO - Test Loss=0.7954, Test top-1 acc=0.8005
2022-01-12 00:17:39,429 - INFO - Group Accuracy:

2022-01-12 00:17:39,429 - INFO - [0.98240966 0.9920482  0.9879518  0.9925301  0.98722893 0.9963855
 0.9821687  0.9821687  0.98963857 0.9771084  0.9927711  0.98674697
 0.97831327 0.98361444 0.97951806 0.9891566  0.9968675 ]
2022-01-12 00:17:39,430 - INFO - Saving...
2022-01-12 00:17:39,690 - INFO - Epoch time: 440.25550651550293
2022-01-12 00:17:39,690 - INFO - 
Epoch: 82
2022-01-12 00:17:39,690 - INFO - 
Learning Rate: 0.0010
2022-01-12 00:17:56,915 - INFO - [Step=68500]	Loss=0.4820	231.6 examples/second
2022-01-12 00:20:02,776 - INFO - [Step=68750]	Loss=0.4855	254.3 examples/second
2022-01-12 00:22:08,562 - INFO - [Step=69000]	Loss=0.4858	254.4 examples/second
2022-01-12 00:24:14,436 - INFO - [Step=69250]	Loss=0.4865	254.2 examples/second
2022-01-12 00:24:49,738 - INFO - Test Loss=0.7913, Test top-1 acc=0.7993
2022-01-12 00:24:49,738 - INFO - Group Accuracy:

2022-01-12 00:24:49,738 - INFO - [0.9826506  0.9925301  0.98674697 0.9925301  0.98674697 0.9963855
 0.98289156 0.98289156 0.9901205  0.9773494  0.9920482  0.9860241
 0.9775904  0.98361444 0.97975904 0.9898795  0.9968675 ]
2022-01-12 00:24:49,740 - INFO - Epoch time: 430.049617767334
2022-01-12 00:24:49,740 - INFO - 
Epoch: 83
2022-01-12 00:24:49,740 - INFO - 
Learning Rate: 0.0010
2022-01-12 00:26:31,704 - INFO - [Step=69500]	Loss=0.4700	233.1 examples/second
2022-01-12 00:28:39,812 - INFO - [Step=69750]	Loss=0.4870	249.8 examples/second
2022-01-12 00:30:48,679 - INFO - [Step=70000]	Loss=0.5001	248.3 examples/second
2022-01-12 00:32:08,012 - INFO - Test Loss=0.7877, Test top-1 acc=0.7978
2022-01-12 00:32:08,012 - INFO - Group Accuracy:

2022-01-12 00:32:08,012 - INFO - [0.9812048  0.9922892  0.9879518  0.993253   0.9879518  0.9966265
 0.9816868  0.98240966 0.9893976  0.9775904  0.9927711  0.9860241
 0.9775904  0.9826506  0.97975904 0.98963857 0.9968675 ]
2022-01-12 00:32:08,014 - INFO - Epoch time: 438.2740411758423
2022-01-12 00:32:08,014 - INFO - 
Epoch: 84
2022-01-12 00:32:08,014 - INFO - 
Learning Rate: 0.0010
2022-01-12 00:33:06,397 - INFO - [Step=70250]	Loss=0.4726	232.4 examples/second
2022-01-12 00:35:14,539 - INFO - [Step=70500]	Loss=0.4759	249.7 examples/second
2022-01-12 00:37:22,511 - INFO - [Step=70750]	Loss=0.4809	250.1 examples/second
2022-01-12 00:39:25,730 - INFO - Test Loss=0.7911, Test top-1 acc=0.7949
2022-01-12 00:39:25,730 - INFO - Group Accuracy:

2022-01-12 00:39:25,730 - INFO - [0.9814458  0.9925301  0.98650604 0.993253   0.98698795 0.9959036
 0.9816868  0.9821687  0.9898795  0.97638553 0.9922892  0.98626506
 0.9773494  0.98289156 0.98       0.9893976  0.9968675 ]
2022-01-12 00:39:25,731 - INFO - Epoch time: 437.7175261974335
2022-01-12 00:39:25,731 - INFO - 
Epoch: 85
2022-01-12 00:39:25,731 - INFO - 
Learning Rate: 0.0010
2022-01-12 00:39:40,625 - INFO - [Step=71000]	Loss=0.4732	231.7 examples/second
2022-01-12 00:41:45,900 - INFO - [Step=71250]	Loss=0.4950	255.4 examples/second
2022-01-12 00:43:51,543 - INFO - [Step=71500]	Loss=0.4898	254.7 examples/second
2022-01-12 00:45:57,155 - INFO - [Step=71750]	Loss=0.4787	254.8 examples/second
2022-01-12 00:46:34,730 - INFO - Test Loss=0.7970, Test top-1 acc=0.7966
2022-01-12 00:46:34,730 - INFO - Group Accuracy:

2022-01-12 00:46:34,730 - INFO - [0.9816868  0.9918072  0.98722893 0.9925301  0.98722893 0.9963855
 0.9826506  0.9826506  0.98963857 0.9768675  0.9927711  0.9853012
 0.9771084  0.98313254 0.98024094 0.98963857 0.99710846]
2022-01-12 00:46:34,732 - INFO - Epoch time: 429.0004880428314
2022-01-12 00:46:34,732 - INFO - 
Epoch: 86
2022-01-12 00:46:34,732 - INFO - 
Learning Rate: 0.0010
2022-01-12 00:48:13,837 - INFO - [Step=72000]	Loss=0.4746	234.1 examples/second
2022-01-12 00:50:21,910 - INFO - [Step=72250]	Loss=0.4900	249.9 examples/second
2022-01-12 00:52:30,096 - INFO - [Step=72500]	Loss=0.4735	249.6 examples/second
2022-01-12 00:53:52,572 - INFO - Test Loss=0.7912, Test top-1 acc=0.7952
2022-01-12 00:53:52,573 - INFO - Group Accuracy:

2022-01-12 00:53:52,573 - INFO - [0.9807229  0.99156624 0.98674697 0.9927711  0.9860241  0.9956626
 0.98240966 0.98313254 0.9898795  0.9768675  0.9927711  0.9855422
 0.9780723  0.98361444 0.9785542  0.9893976  0.9968675 ]
2022-01-12 00:53:52,574 - INFO - Epoch time: 437.8420193195343
2022-01-12 00:53:52,574 - INFO - 
Epoch: 87
2022-01-12 00:53:52,574 - INFO - 
Learning Rate: 0.0010
2022-01-12 00:54:48,323 - INFO - [Step=72750]	Loss=0.4790	231.5 examples/second
2022-01-12 00:56:57,813 - INFO - [Step=73000]	Loss=0.4749	247.1 examples/second
2022-01-12 00:59:07,087 - INFO - [Step=73250]	Loss=0.4680	247.5 examples/second
2022-01-12 01:01:12,872 - INFO - Test Loss=0.7979, Test top-1 acc=0.7966
2022-01-12 01:01:12,872 - INFO - Group Accuracy:

2022-01-12 01:01:12,872 - INFO - [0.9812048  0.99108434 0.9855422  0.9925301  0.98626506 0.9959036
 0.9814458  0.9826506  0.98963857 0.9771084  0.9925301  0.9855422
 0.9778313  0.98361444 0.97927713 0.9893976  0.9968675 ]
2022-01-12 01:01:12,873 - INFO - Epoch time: 440.29878211021423
2022-01-12 01:01:12,873 - INFO - 
Epoch: 88
2022-01-12 01:01:12,873 - INFO - 
Learning Rate: 0.0010
2022-01-12 01:01:24,981 - INFO - [Step=73500]	Loss=0.4751	232.1 examples/second
2022-01-12 01:03:32,536 - INFO - [Step=73750]	Loss=0.4723	250.9 examples/second
2022-01-12 01:05:39,773 - INFO - [Step=74000]	Loss=0.4640	251.5 examples/second
2022-01-12 01:07:47,191 - INFO - [Step=74250]	Loss=0.4742	251.1 examples/second
2022-01-12 01:08:28,140 - INFO - Test Loss=0.7937, Test top-1 acc=0.7986
2022-01-12 01:08:28,141 - INFO - Group Accuracy:

2022-01-12 01:08:28,141 - INFO - [0.9814458  0.9920482  0.98698795 0.9930121  0.98698795 0.9961446
 0.98240966 0.9826506  0.9898795  0.9766265  0.9927711  0.98578316
 0.9778313  0.98240966 0.97951806 0.9889157  0.99710846]
2022-01-12 01:08:28,142 - INFO - Epoch time: 435.2688217163086
2022-01-12 01:08:28,142 - INFO - 
Epoch: 89
2022-01-12 01:08:28,142 - INFO - 
Learning Rate: 0.0010
2022-01-12 01:10:04,298 - INFO - [Step=74500]	Loss=0.4725	233.4 examples/second
2022-01-12 01:12:12,124 - INFO - [Step=74750]	Loss=0.4566	250.3 examples/second
2022-01-12 01:14:19,034 - INFO - [Step=75000]	Loss=0.4794	252.1 examples/second
2022-01-12 01:15:43,209 - INFO - Test Loss=0.7938, Test top-1 acc=0.7969
2022-01-12 01:15:43,210 - INFO - Group Accuracy:

2022-01-12 01:15:43,210 - INFO - [0.9814458  0.9913253  0.98722893 0.9930121  0.98746985 0.9966265
 0.98240966 0.9826506  0.9901205  0.9771084  0.9927711  0.9853012
 0.9778313  0.9821687  0.97975904 0.9891566  0.99710846]
2022-01-12 01:15:43,211 - INFO - Epoch time: 435.0689477920532
2022-01-12 01:15:54,052 - INFO - Computing OOD Statistics...
2022-01-12 01:15:54,060 - INFO - 	Baseline.          AUROC: 0.3292. TNR@95TPR: 0.0200. AUPR OUT: 0.1206
2022-01-12 01:15:54,065 - INFO - 	ODIN (T=1000).     AUROC: 0.8948. TNR@95TPR: 0.5365. AUPR OUT: 0.6447
2022-01-12 01:15:54,065 - INFO - Printing Final Accuracy + OOD Detection stats
2022-01-12 01:15:54,065 - INFO - Top 1 Accuracy:  Min: 0.8005; Max: 0.8005; Avg: 0.8005; Std: 0.0000; Len: 1
2022-01-12 01:15:54,065 - INFO - Top 5 Accuracy:  Min: 0.9869; Max: 0.9869; Avg: 0.9869; Std: 0.0000; Len: 1
2022-01-12 01:15:54,065 - INFO - **********************************************************************
2022-01-12 01:15:54,065 - INFO - 	MSP (auroc): [0.3291750531537917] Min: 0.3292; Max: 0.3292; Avg: 0.3292; Std: 0.0000; Len: 1
2022-01-12 01:15:54,065 - INFO - 	MSP (tnr): [0.020000000000000018] Min: 0.0200; Max: 0.0200; Avg: 0.0200; Std: 0.0000; Len: 1
2022-01-12 01:15:54,065 - INFO - 	MSP (aupr): [0.12059888758195854] Min: 0.1206; Max: 0.1206; Avg: 0.1206; Std: 0.0000; Len: 1
2022-01-12 01:15:54,065 - INFO - 	ODIN (auroc): [0.8947671155209073] Min: 0.8948; Max: 0.8948; Avg: 0.8948; Std: 0.0000; Len: 1
2022-01-12 01:15:54,065 - INFO - 	ODIN (tnr): [0.5364705882352941] Min: 0.5365; Max: 0.5365; Avg: 0.5365; Std: 0.0000; Len: 1
2022-01-12 01:15:54,066 - INFO - 	ODIN (aupr): [0.6447368765734125] Min: 0.6447; Max: 0.6447; Avg: 0.6447; Std: 0.0000; Len: 1
