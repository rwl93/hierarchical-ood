2022-01-11 14:06:39,787 - INFO - ==> Preparing data..
2022-01-11 14:06:40,156 - INFO - checkpoint filename: experiments/coarse/mos/FC3_LRp1_R1/checkpoint.pt
2022-01-11 14:06:40,157 - INFO - log filename: experiments/coarse/mos/FC3_LRp1_R1/train.log
2022-01-11 14:06:40,157 - INFO - ********************************************************
2022-01-11 14:06:40,157 - INFO - Starting Iter: 0 / 1
2022-01-11 14:06:40,157 - INFO - ********************************************************
2022-01-11 14:06:43,474 - INFO - 
Epoch: 0
2022-01-11 14:06:43,474 - INFO - 
Learning Rate: 0.0100
2022-01-11 14:08:41,599 - INFO - [Step=250]	Loss=7.0758	270.9 examples/second
2022-01-11 14:10:37,853 - INFO - [Step=500]	Loss=5.4307	275.3 examples/second
2022-01-11 14:12:34,323 - INFO - [Step=750]	Loss=5.3290	274.8 examples/second
2022-01-11 14:13:22,067 - INFO - Test Loss=5.2818, Test top-1 acc=0.0289
2022-01-11 14:13:22,067 - INFO - Group Accuracy:

2022-01-11 14:13:22,067 - INFO - [0.939759   0.939759   0.939759   0.939759   0.939759   0.939759
 0.939759   0.939759   0.939759   0.939759   0.9518072  0.939759
 0.939759   0.939759   0.9385542  0.93903613 0.9510843 ]
2022-01-11 14:13:22,068 - INFO - Saving...
2022-01-11 14:13:22,224 - INFO - Epoch time: 398.75022530555725
2022-01-11 14:13:22,225 - INFO - 
Epoch: 1
2022-01-11 14:13:22,225 - INFO - 
Learning Rate: 0.0280
2022-01-11 14:14:41,328 - INFO - [Step=1000]	Loss=5.2723	252.0 examples/second
2022-01-11 14:16:37,658 - INFO - [Step=1250]	Loss=5.0731	275.1 examples/second
2022-01-11 14:18:34,117 - INFO - [Step=1500]	Loss=4.8605	274.8 examples/second
2022-01-11 14:20:01,213 - INFO - Test Loss=4.8248, Test top-1 acc=0.0677
2022-01-11 14:20:01,213 - INFO - Group Accuracy:

2022-01-11 14:20:01,213 - INFO - [0.939759   0.939759   0.939759   0.9404819  0.939759   0.94120485
 0.939759   0.939759   0.93903613 0.939759   0.9518072  0.939759
 0.939759   0.939759   0.939759   0.9395181  0.9518072 ]
2022-01-11 14:20:01,214 - INFO - Saving...
2022-01-11 14:20:01,467 - INFO - Epoch time: 399.2420537471771
2022-01-11 14:20:01,467 - INFO - 
Epoch: 2
2022-01-11 14:20:01,467 - INFO - 
Learning Rate: 0.0460
2022-01-11 14:20:40,898 - INFO - [Step=1750]	Loss=4.7140	252.4 examples/second
2022-01-11 14:22:37,238 - INFO - [Step=2000]	Loss=4.5785	275.1 examples/second
2022-01-11 14:24:33,675 - INFO - [Step=2250]	Loss=4.3714	274.8 examples/second
2022-01-11 14:26:30,142 - INFO - [Step=2500]	Loss=4.2938	274.8 examples/second
2022-01-11 14:26:40,114 - INFO - Test Loss=3.9973, Test top-1 acc=0.1227
2022-01-11 14:26:40,115 - INFO - Group Accuracy:

2022-01-11 14:26:40,115 - INFO - [0.94       0.9448193  0.940241   0.95325303 0.93831325 0.94915664
 0.9404819  0.94192773 0.9431325  0.939759   0.95277107 0.940241
 0.939759   0.94       0.939759   0.94096386 0.95566267]
2022-01-11 14:26:40,116 - INFO - Saving...
2022-01-11 14:26:40,366 - INFO - Epoch time: 398.8988609313965
2022-01-11 14:26:40,366 - INFO - 
Epoch: 3
2022-01-11 14:26:40,366 - INFO - 
Learning Rate: 0.0640
2022-01-11 14:28:36,439 - INFO - [Step=2750]	Loss=4.1772	253.4 examples/second
2022-01-11 14:30:33,416 - INFO - [Step=3000]	Loss=3.9921	273.6 examples/second
2022-01-11 14:32:29,895 - INFO - [Step=3250]	Loss=3.8361	274.7 examples/second
2022-01-11 14:33:19,663 - INFO - Test Loss=3.5836, Test top-1 acc=0.1920
2022-01-11 14:33:19,664 - INFO - Group Accuracy:

2022-01-11 14:33:19,664 - INFO - [0.94216865 0.9474699  0.9407229  0.96024096 0.9392771  0.9551807
 0.9453012  0.94578314 0.9481928  0.94144577 0.9469879  0.94240963
 0.940241   0.9404819  0.9404819  0.94433737 0.9580723 ]
2022-01-11 14:33:19,665 - INFO - Saving...
2022-01-11 14:33:19,895 - INFO - Epoch time: 399.5286922454834
2022-01-11 14:33:19,895 - INFO - 
Epoch: 4
2022-01-11 14:33:19,895 - INFO - 
Learning Rate: 0.1000
2022-01-11 14:34:36,540 - INFO - [Step=3500]	Loss=3.7935	252.7 examples/second
2022-01-11 14:36:33,218 - INFO - [Step=3750]	Loss=3.6932	274.3 examples/second
2022-01-11 14:38:29,589 - INFO - [Step=4000]	Loss=3.5440	275.0 examples/second
2022-01-11 14:39:58,937 - INFO - Test Loss=3.4203, Test top-1 acc=0.2345
2022-01-11 14:39:58,938 - INFO - Group Accuracy:

2022-01-11 14:39:58,938 - INFO - [0.94144577 0.94554216 0.94554216 0.9592771  0.94554216 0.96433735
 0.9448193  0.95036143 0.9559036  0.94192773 0.95421684 0.9436145
 0.9407229  0.94240963 0.9407229  0.94939756 0.9498795 ]
2022-01-11 14:39:58,939 - INFO - Saving...
2022-01-11 14:39:59,187 - INFO - Epoch time: 399.29216051101685
2022-01-11 14:39:59,187 - INFO - 
Epoch: 5
2022-01-11 14:39:59,187 - INFO - 
Learning Rate: 0.1000
2022-01-11 14:40:36,426 - INFO - [Step=4250]	Loss=3.4235	252.3 examples/second
2022-01-11 14:42:33,021 - INFO - [Step=4500]	Loss=3.3121	274.5 examples/second
2022-01-11 14:44:29,444 - INFO - [Step=4750]	Loss=3.1954	274.9 examples/second
2022-01-11 14:46:26,132 - INFO - [Step=5000]	Loss=3.1138	274.2 examples/second
2022-01-11 14:46:38,794 - INFO - Test Loss=2.8708, Test top-1 acc=0.3202
2022-01-11 14:46:38,794 - INFO - Group Accuracy:

2022-01-11 14:46:38,794 - INFO - [0.9510843  0.95638555 0.9460241  0.96843374 0.94144577 0.9691566
 0.94891566 0.95421684 0.9578313  0.9469879  0.9580723  0.94578314
 0.94192773 0.9440964  0.9366265  0.95036143 0.97228914]
2022-01-11 14:46:38,795 - INFO - Saving...
2022-01-11 14:46:39,041 - INFO - Epoch time: 399.85411858558655
2022-01-11 14:46:39,042 - INFO - 
Epoch: 6
2022-01-11 14:46:39,042 - INFO - 
Learning Rate: 0.1000
2022-01-11 14:48:33,009 - INFO - [Step=5250]	Loss=3.0327	252.2 examples/second
2022-01-11 14:50:29,367 - INFO - [Step=5500]	Loss=2.9600	275.0 examples/second
2022-01-11 14:52:26,174 - INFO - [Step=5750]	Loss=2.8728	274.0 examples/second
2022-01-11 14:53:18,270 - INFO - Test Loss=2.8075, Test top-1 acc=0.3214
2022-01-11 14:53:18,270 - INFO - Group Accuracy:

2022-01-11 14:53:18,270 - INFO - [0.94963855 0.94939756 0.94915664 0.9657831  0.9501205  0.97108436
 0.9510843  0.9453012  0.96361446 0.94554216 0.9578313  0.9518072
 0.94289154 0.94192773 0.946506   0.9561446  0.97301203]
2022-01-11 14:53:18,271 - INFO - Saving...
2022-01-11 14:53:18,517 - INFO - Epoch time: 399.4748966693878
2022-01-11 14:53:18,517 - INFO - 
Epoch: 7
2022-01-11 14:53:18,517 - INFO - 
Learning Rate: 0.1000
2022-01-11 14:54:32,719 - INFO - [Step=6000]	Loss=2.7961	252.9 examples/second
2022-01-11 14:56:29,501 - INFO - [Step=6250]	Loss=2.7333	274.0 examples/second
2022-01-11 14:58:26,066 - INFO - [Step=6500]	Loss=2.6781	274.5 examples/second
2022-01-11 14:59:58,293 - INFO - Test Loss=2.8166, Test top-1 acc=0.3663
2022-01-11 14:59:58,293 - INFO - Group Accuracy:

2022-01-11 14:59:58,293 - INFO - [0.9518072  0.9546988  0.95325303 0.9691566  0.94578314 0.97108436
 0.9453012  0.9595181  0.95036143 0.9551807  0.9662651  0.9590362
 0.94578314 0.94168675 0.94843376 0.9578313  0.9739759 ]
2022-01-11 14:59:58,294 - INFO - Saving...
2022-01-11 14:59:58,559 - INFO - Epoch time: 400.0422341823578
2022-01-11 14:59:58,559 - INFO - 
Epoch: 8
2022-01-11 14:59:58,559 - INFO - 
Learning Rate: 0.1000
2022-01-11 15:00:33,508 - INFO - [Step=6750]	Loss=2.5977	251.1 examples/second
2022-01-11 15:02:30,074 - INFO - [Step=7000]	Loss=2.5333	274.5 examples/second
2022-01-11 15:04:26,755 - INFO - [Step=7250]	Loss=2.5065	274.3 examples/second
2022-01-11 15:06:23,397 - INFO - [Step=7500]	Loss=2.4567	274.3 examples/second
2022-01-11 15:06:38,872 - INFO - Test Loss=2.4644, Test top-1 acc=0.4010
2022-01-11 15:06:38,872 - INFO - Group Accuracy:

2022-01-11 15:06:38,872 - INFO - [0.95325303 0.95975906 0.9544578  0.97156626 0.95325303 0.97566265
 0.95301205 0.96433735 0.96891564 0.9479518  0.95710844 0.9561446
 0.94506025 0.9481928  0.9486747  0.96       0.9807229 ]
2022-01-11 15:06:38,873 - INFO - Saving...
2022-01-11 15:06:39,109 - INFO - Epoch time: 400.54992508888245
2022-01-11 15:06:39,109 - INFO - 
Epoch: 9
2022-01-11 15:06:39,110 - INFO - 
Learning Rate: 0.1000
2022-01-11 15:08:30,881 - INFO - [Step=7750]	Loss=2.3670	251.0 examples/second
2022-01-11 15:10:27,683 - INFO - [Step=8000]	Loss=2.3497	274.0 examples/second
2022-01-11 15:12:24,096 - INFO - [Step=8250]	Loss=2.3149	274.9 examples/second
2022-01-11 15:13:18,449 - INFO - Test Loss=2.3556, Test top-1 acc=0.4306
2022-01-11 15:13:18,449 - INFO - Group Accuracy:

2022-01-11 15:13:18,449 - INFO - [0.9573494  0.96385545 0.9363856  0.9727711  0.9592771  0.9814458
 0.9551807  0.9631325  0.9725301  0.9628916  0.96698797 0.9590362
 0.9477109  0.93807226 0.9554217  0.96096385 0.9809638 ]
2022-01-11 15:13:18,450 - INFO - Saving...
2022-01-11 15:13:18,681 - INFO - Epoch time: 399.5714011192322
2022-01-11 15:13:18,681 - INFO - 
Epoch: 10
2022-01-11 15:13:18,681 - INFO - 
Learning Rate: 0.1000
2022-01-11 15:14:30,728 - INFO - [Step=8500]	Loss=2.2889	252.7 examples/second
2022-01-11 15:16:27,268 - INFO - [Step=8750]	Loss=2.1983	274.6 examples/second
2022-01-11 15:18:23,634 - INFO - [Step=9000]	Loss=2.1955	275.0 examples/second
2022-01-11 15:19:57,733 - INFO - Test Loss=2.0416, Test top-1 acc=0.4711
2022-01-11 15:19:57,733 - INFO - Group Accuracy:

2022-01-11 15:19:57,733 - INFO - [0.96       0.96891564 0.9559036  0.9819277  0.95421684 0.9840964
 0.9583132  0.96       0.9701205  0.9628916  0.97108436 0.96361446
 0.9508434  0.9515663  0.95662653 0.96385545 0.9812048 ]
2022-01-11 15:19:57,735 - INFO - Saving...
2022-01-11 15:19:57,994 - INFO - Epoch time: 399.31311559677124
2022-01-11 15:19:57,994 - INFO - 
Epoch: 11
2022-01-11 15:19:57,995 - INFO - 
Learning Rate: 0.1000
2022-01-11 15:20:30,409 - INFO - [Step=9250]	Loss=2.1476	252.4 examples/second
2022-01-11 15:22:27,034 - INFO - [Step=9500]	Loss=2.1328	274.4 examples/second
2022-01-11 15:24:23,304 - INFO - [Step=9750]	Loss=2.1101	275.2 examples/second
2022-01-11 15:26:19,975 - INFO - [Step=10000]	Loss=2.0750	274.3 examples/second
2022-01-11 15:26:36,937 - INFO - Test Loss=2.2530, Test top-1 acc=0.4759
2022-01-11 15:26:36,938 - INFO - Group Accuracy:

2022-01-11 15:26:36,938 - INFO - [0.9554217  0.9698795  0.9474699  0.9739759  0.9573494  0.98313254
 0.9631325  0.96024096 0.97590363 0.96072286 0.96698797 0.9587952
 0.946506   0.94963855 0.9549398  0.96506023 0.9766265 ]
2022-01-11 15:26:36,939 - INFO - Saving...
2022-01-11 15:26:37,176 - INFO - Epoch time: 399.18131852149963
2022-01-11 15:26:37,176 - INFO - 
Epoch: 12
2022-01-11 15:26:37,176 - INFO - 
Learning Rate: 0.1000
2022-01-11 15:28:26,023 - INFO - [Step=10250]	Loss=2.0050	253.9 examples/second
2022-01-11 15:30:22,406 - INFO - [Step=10500]	Loss=2.0175	275.0 examples/second
2022-01-11 15:32:18,923 - INFO - [Step=10750]	Loss=2.0021	274.6 examples/second
2022-01-11 15:33:15,742 - INFO - Test Loss=2.1065, Test top-1 acc=0.4945
2022-01-11 15:33:15,742 - INFO - Group Accuracy:

2022-01-11 15:33:15,742 - INFO - [0.9655422  0.97108436 0.9580723  0.9713253  0.9655422  0.9853012
 0.9546988  0.9612048  0.973253   0.9624096  0.9746988  0.9624096
 0.9510843  0.95349395 0.9440964  0.9657831  0.98024094]
2022-01-11 15:33:15,743 - INFO - Saving...
2022-01-11 15:33:15,984 - INFO - Epoch time: 398.8074519634247
2022-01-11 15:33:15,984 - INFO - 
Epoch: 13
2022-01-11 15:33:15,984 - INFO - 
Learning Rate: 0.1000
2022-01-11 15:34:25,396 - INFO - [Step=11000]	Loss=1.9733	253.0 examples/second
2022-01-11 15:36:21,918 - INFO - [Step=11250]	Loss=1.9027	274.6 examples/second
2022-01-11 15:38:18,496 - INFO - [Step=11500]	Loss=1.9285	274.5 examples/second
2022-01-11 15:39:54,745 - INFO - Test Loss=1.9538, Test top-1 acc=0.5296
2022-01-11 15:39:54,746 - INFO - Group Accuracy:

2022-01-11 15:39:54,758 - INFO - [0.96457833 0.97108436 0.96819276 0.97590363 0.96506023 0.9821687
 0.9595181  0.9660241  0.9725301  0.9653012  0.9691566  0.9628916
 0.95662653 0.9585542  0.95686746 0.9698795  0.98578316]
2022-01-11 15:39:54,759 - INFO - Saving...
2022-01-11 15:39:55,039 - INFO - Epoch time: 399.0549011230469
2022-01-11 15:39:55,039 - INFO - 
Epoch: 14
2022-01-11 15:39:55,039 - INFO - 
Learning Rate: 0.1000
2022-01-11 15:40:25,029 - INFO - [Step=11750]	Loss=1.8984	252.9 examples/second
2022-01-11 15:42:21,325 - INFO - [Step=12000]	Loss=1.8704	275.2 examples/second
2022-01-11 15:44:18,162 - INFO - [Step=12250]	Loss=1.8712	273.9 examples/second
2022-01-11 15:46:14,821 - INFO - [Step=12500]	Loss=1.8513	274.3 examples/second
2022-01-11 15:46:34,473 - INFO - Test Loss=2.2671, Test top-1 acc=0.4966
2022-01-11 15:46:34,473 - INFO - Group Accuracy:

2022-01-11 15:46:34,473 - INFO - [0.9590362  0.973494   0.95638555 0.9785542  0.95421684 0.9850602
 0.95253015 0.9621687  0.9778313  0.95566267 0.9708434  0.96168673
 0.9546988  0.94915664 0.9520482  0.966506   0.9778313 ]
2022-01-11 15:46:34,474 - INFO - Epoch time: 399.43549370765686
2022-01-11 15:46:34,474 - INFO - 
Epoch: 15
2022-01-11 15:46:34,475 - INFO - 
Learning Rate: 0.1000
2022-01-11 15:48:21,412 - INFO - [Step=12750]	Loss=1.7933	252.8 examples/second
2022-01-11 15:50:17,951 - INFO - [Step=13000]	Loss=1.8139	274.6 examples/second
2022-01-11 15:52:14,487 - INFO - [Step=13250]	Loss=1.7847	274.6 examples/second
2022-01-11 15:53:13,484 - INFO - Test Loss=2.0793, Test top-1 acc=0.5173
2022-01-11 15:53:13,484 - INFO - Group Accuracy:

2022-01-11 15:53:13,484 - INFO - [0.96       0.9766265  0.96361446 0.98313254 0.9655422  0.98433733
 0.95710844 0.9626506  0.96843374 0.95975906 0.9696385  0.96698797
 0.9440964  0.9551807  0.9549398  0.96168673 0.9787952 ]
2022-01-11 15:53:13,485 - INFO - Epoch time: 399.0102813243866
2022-01-11 15:53:13,485 - INFO - 
Epoch: 16
2022-01-11 15:53:13,485 - INFO - 
Learning Rate: 0.1000
2022-01-11 15:54:20,902 - INFO - [Step=13500]	Loss=1.7737	253.1 examples/second
2022-01-11 15:56:17,720 - INFO - [Step=13750]	Loss=1.7532	273.9 examples/second
2022-01-11 15:58:14,206 - INFO - [Step=14000]	Loss=1.7701	274.7 examples/second
2022-01-11 15:59:53,403 - INFO - Test Loss=1.6610, Test top-1 acc=0.5807
2022-01-11 15:59:53,403 - INFO - Group Accuracy:

2022-01-11 15:59:53,403 - INFO - [0.966747   0.9778313  0.96819276 0.97903615 0.96771085 0.98698795
 0.96168673 0.97108436 0.97831327 0.9674699  0.9778313  0.97204816
 0.9578313  0.96048194 0.96168673 0.97493976 0.9884337 ]
2022-01-11 15:59:53,404 - INFO - Saving...
2022-01-11 15:59:53,637 - INFO - Epoch time: 400.1523187160492
2022-01-11 15:59:53,637 - INFO - 
Epoch: 17
2022-01-11 15:59:53,637 - INFO - 
Learning Rate: 0.1000
2022-01-11 16:00:21,339 - INFO - [Step=14250]	Loss=1.7143	251.7 examples/second
2022-01-11 16:02:17,771 - INFO - [Step=14500]	Loss=1.7257	274.8 examples/second
2022-01-11 16:04:14,190 - INFO - [Step=14750]	Loss=1.6813	274.9 examples/second
2022-01-11 16:06:11,094 - INFO - [Step=15000]	Loss=1.6917	273.7 examples/second
2022-01-11 16:06:33,202 - INFO - Test Loss=1.9100, Test top-1 acc=0.5506
2022-01-11 16:06:33,202 - INFO - Group Accuracy:

2022-01-11 16:06:33,202 - INFO - [0.96385545 0.97566265 0.96409637 0.9819277  0.96843374 0.9819277
 0.9621687  0.9660241  0.9787952  0.9662651  0.97228914 0.97108436
 0.9585542  0.95662653 0.96024096 0.966506   0.98771083]
2022-01-11 16:06:33,203 - INFO - Epoch time: 399.56580114364624
2022-01-11 16:06:33,203 - INFO - 
Epoch: 18
2022-01-11 16:06:33,203 - INFO - 
Learning Rate: 0.1000
2022-01-11 16:08:17,440 - INFO - [Step=15250]	Loss=1.6652	253.3 examples/second
2022-01-11 16:10:13,795 - INFO - [Step=15500]	Loss=1.6699	275.0 examples/second
2022-01-11 16:12:10,444 - INFO - [Step=15750]	Loss=1.6687	274.3 examples/second
2022-01-11 16:13:11,855 - INFO - Test Loss=1.9386, Test top-1 acc=0.5778
2022-01-11 16:13:11,855 - INFO - Group Accuracy:

2022-01-11 16:13:11,855 - INFO - [0.9672289  0.97614455 0.9701205  0.98       0.96409637 0.98746985
 0.9583132  0.9725301  0.9804819  0.96843374 0.97638553 0.9727711
 0.9592771  0.9592771  0.96361446 0.9706024  0.9881928 ]
2022-01-11 16:13:11,856 - INFO - Epoch time: 398.6527283191681
2022-01-11 16:13:11,856 - INFO - 
Epoch: 19
2022-01-11 16:13:11,856 - INFO - 
Learning Rate: 0.1000
2022-01-11 16:14:16,753 - INFO - [Step=16000]	Loss=1.6465	253.3 examples/second
2022-01-11 16:16:13,272 - INFO - [Step=16250]	Loss=1.6337	274.6 examples/second
2022-01-11 16:18:10,024 - INFO - [Step=16500]	Loss=1.6185	274.1 examples/second
2022-01-11 16:19:51,004 - INFO - Test Loss=1.7220, Test top-1 acc=0.5595
2022-01-11 16:19:51,004 - INFO - Group Accuracy:

2022-01-11 16:19:51,004 - INFO - [0.9672289  0.97927713 0.96409637 0.9809638  0.9701205  0.9879518
 0.9614458  0.9725301  0.97614455 0.9660241  0.9773494  0.9703615
 0.9595181  0.9477109  0.9590362  0.97180724 0.9853012 ]
2022-01-11 16:19:51,005 - INFO - Epoch time: 399.1491310596466
2022-01-11 16:19:51,005 - INFO - 
Epoch: 20
2022-01-11 16:19:51,005 - INFO - 
Learning Rate: 0.1000
2022-01-11 16:20:16,291 - INFO - [Step=16750]	Loss=1.6053	253.4 examples/second
2022-01-11 16:22:12,888 - INFO - [Step=17000]	Loss=1.6041	274.4 examples/second
2022-01-11 16:24:09,393 - INFO - [Step=17250]	Loss=1.6000	274.7 examples/second
2022-01-11 16:26:05,488 - INFO - [Step=17500]	Loss=1.6004	275.6 examples/second
2022-01-11 16:26:29,480 - INFO - Test Loss=1.6213, Test top-1 acc=0.5810
2022-01-11 16:26:29,480 - INFO - Group Accuracy:

2022-01-11 16:26:29,480 - INFO - [0.97228914 0.97614455 0.9725301  0.9812048  0.966506   0.9840964
 0.96843374 0.97638553 0.9816868  0.96819276 0.98024094 0.9727711
 0.9539759  0.9612048  0.9626506  0.97204816 0.98963857]
2022-01-11 16:26:29,481 - INFO - Saving...
2022-01-11 16:26:29,723 - INFO - Epoch time: 398.7181923389435
2022-01-11 16:26:29,724 - INFO - 
Epoch: 21
2022-01-11 16:26:29,724 - INFO - 
Learning Rate: 0.1000
2022-01-11 16:28:12,273 - INFO - [Step=17750]	Loss=1.5617	252.4 examples/second
2022-01-11 16:30:08,808 - INFO - [Step=18000]	Loss=1.5675	274.6 examples/second
2022-01-11 16:32:05,064 - INFO - [Step=18250]	Loss=1.5701	275.3 examples/second
2022-01-11 16:33:08,667 - INFO - Test Loss=1.6003, Test top-1 acc=0.6075
2022-01-11 16:33:08,668 - INFO - Group Accuracy:

2022-01-11 16:33:08,668 - INFO - [0.96506023 0.96819276 0.97228914 0.9838554  0.9737349  0.9893976
 0.9693976  0.9686747  0.98578316 0.9708434  0.9742169  0.9672289
 0.96048194 0.9587952  0.96361446 0.97566265 0.9853012 ]
2022-01-11 16:33:08,669 - INFO - Saving...
2022-01-11 16:33:08,907 - INFO - Epoch time: 399.1835367679596
2022-01-11 16:33:08,907 - INFO - 
Epoch: 22
2022-01-11 16:33:08,907 - INFO - 
Learning Rate: 0.1000
2022-01-11 16:34:11,853 - INFO - [Step=18500]	Loss=1.5397	252.4 examples/second
2022-01-11 16:36:08,269 - INFO - [Step=18750]	Loss=1.5311	274.9 examples/second
2022-01-11 16:38:04,816 - INFO - [Step=19000]	Loss=1.5445	274.6 examples/second
2022-01-11 16:39:47,945 - INFO - Test Loss=1.5319, Test top-1 acc=0.5949
2022-01-11 16:39:47,946 - INFO - Group Accuracy:

2022-01-11 16:39:47,946 - INFO - [0.9686747  0.97903615 0.9698795  0.98361444 0.9657831  0.9853012
 0.96385545 0.966506   0.9821687  0.96409637 0.97228914 0.97204816
 0.96385545 0.96       0.9653012  0.9771084  0.98771083]
2022-01-11 16:39:47,946 - INFO - Epoch time: 399.0390295982361
2022-01-11 16:39:47,947 - INFO - 
Epoch: 23
2022-01-11 16:39:47,947 - INFO - 
Learning Rate: 0.1000
2022-01-11 16:40:10,946 - INFO - [Step=19250]	Loss=1.5232	253.7 examples/second
2022-01-11 16:42:07,226 - INFO - [Step=19500]	Loss=1.4931	275.2 examples/second
2022-01-11 16:44:03,549 - INFO - [Step=19750]	Loss=1.5281	275.1 examples/second
2022-01-11 16:45:59,914 - INFO - [Step=20000]	Loss=1.5218	275.0 examples/second
2022-01-11 16:46:26,359 - INFO - Test Loss=1.5057, Test top-1 acc=0.6253
2022-01-11 16:46:26,359 - INFO - Group Accuracy:

2022-01-11 16:46:26,359 - INFO - [0.966506   0.9785542  0.97180724 0.98313254 0.9706024  0.98746985
 0.9624096  0.9766265  0.9809638  0.9672289  0.97975904 0.96698797
 0.9612048  0.9686747  0.9583132  0.9746988  0.98746985]
2022-01-11 16:46:26,360 - INFO - Saving...
2022-01-11 16:46:26,619 - INFO - Epoch time: 398.67216396331787
2022-01-11 16:46:26,619 - INFO - 
Epoch: 24
2022-01-11 16:46:26,619 - INFO - 
Learning Rate: 0.1000
2022-01-11 16:48:06,300 - INFO - [Step=20250]	Loss=1.4815	253.2 examples/second
2022-01-11 16:50:02,290 - INFO - [Step=20500]	Loss=1.4828	275.9 examples/second
2022-01-11 16:51:58,602 - INFO - [Step=20750]	Loss=1.4972	275.1 examples/second
2022-01-11 16:53:04,440 - INFO - Test Loss=2.2438, Test top-1 acc=0.5431
2022-01-11 16:53:04,441 - INFO - Group Accuracy:

2022-01-11 16:53:04,441 - INFO - [0.9631325  0.97204816 0.9356626  0.9739759  0.9628916  0.9850602
 0.9633735  0.96385545 0.9833735  0.96506023 0.966747   0.9672289
 0.95759034 0.9583132  0.95975906 0.966747   0.98240966]
2022-01-11 16:53:04,442 - INFO - Epoch time: 397.82270884513855
2022-01-11 16:53:04,442 - INFO - 
Epoch: 25
2022-01-11 16:53:04,442 - INFO - 
Learning Rate: 0.1000
2022-01-11 16:54:04,565 - INFO - [Step=21000]	Loss=1.4683	254.0 examples/second
2022-01-11 16:56:00,660 - INFO - [Step=21250]	Loss=1.4666	275.6 examples/second
2022-01-11 16:57:56,907 - INFO - [Step=21500]	Loss=1.4898	275.3 examples/second
2022-01-11 16:59:42,169 - INFO - Test Loss=1.5839, Test top-1 acc=0.6113
2022-01-11 16:59:42,170 - INFO - Group Accuracy:

2022-01-11 16:59:42,170 - INFO - [0.96891564 0.9742169  0.97204816 0.9821687  0.9662651  0.98746985
 0.9626506  0.9686747  0.9821687  0.96409637 0.97542167 0.96385545
 0.9585542  0.9633735  0.96506023 0.9771084  0.9879518 ]
2022-01-11 16:59:42,171 - INFO - Epoch time: 397.7289671897888
2022-01-11 16:59:42,171 - INFO - 
Epoch: 26
2022-01-11 16:59:42,171 - INFO - 
Learning Rate: 0.1000
2022-01-11 17:00:02,644 - INFO - [Step=21750]	Loss=1.4725	254.5 examples/second
2022-01-11 17:01:58,790 - INFO - [Step=22000]	Loss=1.4412	275.5 examples/second
2022-01-11 17:03:54,985 - INFO - [Step=22250]	Loss=1.4629	275.4 examples/second
2022-01-11 17:05:51,284 - INFO - [Step=22500]	Loss=1.4751	275.2 examples/second
2022-01-11 17:06:20,157 - INFO - Test Loss=1.7194, Test top-1 acc=0.5973
2022-01-11 17:06:20,158 - INFO - Group Accuracy:

2022-01-11 17:06:20,158 - INFO - [0.96891564 0.97542167 0.9698795  0.9787952  0.9633735  0.9853012
 0.96843374 0.9691566  0.9814458  0.9691566  0.9780723  0.97108436
 0.94843376 0.9657831  0.96433735 0.9746988  0.9898795 ]
2022-01-11 17:06:20,158 - INFO - Epoch time: 397.9875223636627
2022-01-11 17:06:20,158 - INFO - 
Epoch: 27
2022-01-11 17:06:20,158 - INFO - 
Learning Rate: 0.1000
2022-01-11 17:07:57,483 - INFO - [Step=22750]	Loss=1.4088	253.6 examples/second
2022-01-11 17:09:53,661 - INFO - [Step=23000]	Loss=1.4288	275.4 examples/second
2022-01-11 17:11:49,872 - INFO - [Step=23250]	Loss=1.4396	275.4 examples/second
2022-01-11 17:12:58,036 - INFO - Test Loss=1.5617, Test top-1 acc=0.6325
2022-01-11 17:12:58,036 - INFO - Group Accuracy:

2022-01-11 17:12:58,036 - INFO - [0.9703615  0.97927713 0.96795183 0.98626506 0.97301203 0.9898795
 0.97228914 0.97566265 0.97903615 0.9706024  0.9766265  0.973494
 0.9631325  0.96361446 0.9539759  0.9766265  0.9881928 ]
2022-01-11 17:12:58,037 - INFO - Saving...
2022-01-11 17:12:58,271 - INFO - Epoch time: 398.1122319698334
2022-01-11 17:12:58,271 - INFO - 
Epoch: 28
2022-01-11 17:12:58,271 - INFO - 
Learning Rate: 0.1000
2022-01-11 17:13:55,873 - INFO - [Step=23500]	Loss=1.4355	254.0 examples/second
2022-01-11 17:15:52,152 - INFO - [Step=23750]	Loss=1.4078	275.2 examples/second
2022-01-11 17:17:48,392 - INFO - [Step=24000]	Loss=1.4369	275.3 examples/second
2022-01-11 17:19:36,139 - INFO - Test Loss=1.5897, Test top-1 acc=0.6101
2022-01-11 17:19:36,139 - INFO - Group Accuracy:

2022-01-11 17:19:36,139 - INFO - [0.9742169  0.9787952  0.9691566  0.98024094 0.97108436 0.9838554
 0.96795183 0.96771085 0.9848193  0.96481925 0.9807229  0.96409637
 0.9578313  0.9653012  0.9653012  0.9768675  0.98771083]
2022-01-11 17:19:36,140 - INFO - Epoch time: 397.86885690689087
2022-01-11 17:19:36,140 - INFO - 
Epoch: 29
2022-01-11 17:19:36,140 - INFO - 
Learning Rate: 0.0100
2022-01-11 17:19:54,498 - INFO - [Step=24250]	Loss=1.3711	253.8 examples/second
2022-01-11 17:21:50,687 - INFO - [Step=24500]	Loss=1.1117	275.4 examples/second
2022-01-11 17:23:46,804 - INFO - [Step=24750]	Loss=1.0379	275.6 examples/second
2022-01-11 17:25:42,855 - INFO - [Step=25000]	Loss=1.0068	275.7 examples/second
2022-01-11 17:26:14,485 - INFO - Test Loss=0.9599, Test top-1 acc=0.7434
2022-01-11 17:26:14,485 - INFO - Group Accuracy:

2022-01-11 17:26:14,485 - INFO - [0.97566265 0.98722893 0.9826506  0.9920482  0.9809638  0.993494
 0.97831327 0.9812048  0.9893976  0.97614455 0.9889157  0.9807229
 0.9737349  0.9771084  0.97180724 0.9855422  0.99421686]
2022-01-11 17:26:14,486 - INFO - Saving...
2022-01-11 17:26:14,716 - INFO - Epoch time: 398.5757987499237
2022-01-11 17:26:14,716 - INFO - 
Epoch: 30
2022-01-11 17:26:14,716 - INFO - 
Learning Rate: 0.0100
2022-01-11 17:27:49,584 - INFO - [Step=25250]	Loss=0.9678	252.5 examples/second
2022-01-11 17:29:45,658 - INFO - [Step=25500]	Loss=0.9661	275.7 examples/second
2022-01-11 17:31:41,770 - INFO - [Step=25750]	Loss=0.9521	275.6 examples/second
2022-01-11 17:32:52,644 - INFO - Test Loss=0.9779, Test top-1 acc=0.7439
2022-01-11 17:32:52,644 - INFO - Group Accuracy:

2022-01-11 17:32:52,644 - INFO - [0.97614455 0.98746985 0.98289156 0.9922892  0.9807229  0.9939759
 0.97951806 0.9816868  0.98963857 0.97614455 0.9893976  0.9816868
 0.9739759  0.97831327 0.97445786 0.98578316 0.99373496]
2022-01-11 17:32:52,645 - INFO - Saving...
2022-01-11 17:32:52,908 - INFO - Epoch time: 398.1925382614136
2022-01-11 17:32:52,908 - INFO - 
Epoch: 31
2022-01-11 17:32:52,909 - INFO - 
Learning Rate: 0.0100
2022-01-11 17:33:48,172 - INFO - [Step=26000]	Loss=0.9479	253.2 examples/second
2022-01-11 17:35:44,398 - INFO - [Step=26250]	Loss=0.9249	275.3 examples/second
2022-01-11 17:37:40,682 - INFO - [Step=26500]	Loss=0.9294	275.2 examples/second
2022-01-11 17:39:30,742 - INFO - Test Loss=0.9338, Test top-1 acc=0.7496
2022-01-11 17:39:30,743 - INFO - Group Accuracy:

2022-01-11 17:39:30,743 - INFO - [0.97638553 0.9898795  0.9819277  0.9920482  0.9809638  0.99421686
 0.97951806 0.98289156 0.9901205  0.97638553 0.9891566  0.9821687
 0.97445786 0.9775904  0.9742169  0.98698795 0.9939759 ]
2022-01-11 17:39:30,743 - INFO - Saving...
2022-01-11 17:39:30,980 - INFO - Epoch time: 398.07107639312744
2022-01-11 17:39:30,980 - INFO - 
Epoch: 32
2022-01-11 17:39:30,980 - INFO - 
Learning Rate: 0.0100
2022-01-11 17:39:46,900 - INFO - [Step=26750]	Loss=0.9195	253.5 examples/second
2022-01-11 17:41:42,806 - INFO - [Step=27000]	Loss=0.9093	276.1 examples/second
2022-01-11 17:43:38,957 - INFO - [Step=27250]	Loss=0.9059	275.5 examples/second
2022-01-11 17:45:35,021 - INFO - [Step=27500]	Loss=0.8950	275.7 examples/second
2022-01-11 17:46:08,875 - INFO - Test Loss=0.9667, Test top-1 acc=0.7508
2022-01-11 17:46:08,876 - INFO - Group Accuracy:

2022-01-11 17:46:08,876 - INFO - [0.97590363 0.9889157  0.9821687  0.9927711  0.9812048  0.9939759
 0.9804819  0.98289156 0.99084336 0.9766265  0.9879518  0.9816868
 0.97445786 0.9778313  0.97445786 0.98722893 0.993253  ]
2022-01-11 17:46:08,877 - INFO - Saving...
2022-01-11 17:46:09,227 - INFO - Epoch time: 398.2475731372833
2022-01-11 17:46:09,228 - INFO - 
Epoch: 33
2022-01-11 17:46:09,228 - INFO - 
Learning Rate: 0.0100
2022-01-11 17:47:41,998 - INFO - [Step=27750]	Loss=0.8737	252.0 examples/second
2022-01-11 17:49:38,324 - INFO - [Step=28000]	Loss=0.8664	275.1 examples/second
2022-01-11 17:51:34,509 - INFO - [Step=28250]	Loss=0.8840	275.4 examples/second
2022-01-11 17:52:48,001 - INFO - Test Loss=1.0194, Test top-1 acc=0.7484
2022-01-11 17:52:48,002 - INFO - Group Accuracy:

2022-01-11 17:52:48,002 - INFO - [0.97614455 0.98746985 0.98361444 0.9918072  0.9819277  0.99445784
 0.98024094 0.9816868  0.99036145 0.97638553 0.9891566  0.9816868
 0.97542167 0.97975904 0.9742169  0.98698795 0.99373496]
2022-01-11 17:52:48,003 - INFO - Epoch time: 398.77514815330505
2022-01-11 17:52:48,003 - INFO - 
Epoch: 34
2022-01-11 17:52:48,003 - INFO - 
Learning Rate: 0.0100
2022-01-11 17:53:41,434 - INFO - [Step=28500]	Loss=0.8573	252.1 examples/second
2022-01-11 17:55:37,605 - INFO - [Step=28750]	Loss=0.8566	275.5 examples/second
2022-01-11 17:57:34,089 - INFO - [Step=29000]	Loss=0.8642	274.7 examples/second
2022-01-11 17:59:26,693 - INFO - Test Loss=1.0213, Test top-1 acc=0.7492
2022-01-11 17:59:26,693 - INFO - Group Accuracy:

2022-01-11 17:59:26,693 - INFO - [0.9773494  0.9884337  0.9850602  0.993253   0.9816868  0.99445784
 0.9807229  0.9812048  0.99060243 0.97493976 0.9884337  0.9821687
 0.97493976 0.9778313  0.9778313  0.98746985 0.993494  ]
2022-01-11 17:59:26,694 - INFO - Epoch time: 398.69079661369324
2022-01-11 17:59:26,694 - INFO - 
Epoch: 35
2022-01-11 17:59:26,694 - INFO - 
Learning Rate: 0.0100
2022-01-11 17:59:40,540 - INFO - [Step=29250]	Loss=0.8804	253.1 examples/second
2022-01-11 18:01:36,726 - INFO - [Step=29500]	Loss=0.8504	275.4 examples/second
2022-01-11 18:03:32,930 - INFO - [Step=29750]	Loss=0.8459	275.4 examples/second
2022-01-11 18:05:29,137 - INFO - [Step=30000]	Loss=0.8500	275.4 examples/second
2022-01-11 18:06:04,516 - INFO - Test Loss=0.9174, Test top-1 acc=0.7530
2022-01-11 18:06:04,516 - INFO - Group Accuracy:

2022-01-11 18:06:04,516 - INFO - [0.9773494  0.9891566  0.9816868  0.9927711  0.98289156 0.993494
 0.97903615 0.98024094 0.9893976  0.97638553 0.9884337  0.9833735
 0.97518075 0.97951806 0.9775904  0.98722893 0.99493974]
2022-01-11 18:06:04,517 - INFO - Saving...
2022-01-11 18:06:04,750 - INFO - Epoch time: 398.05619263648987
2022-01-11 18:06:04,750 - INFO - 
Epoch: 36
2022-01-11 18:06:04,750 - INFO - 
Learning Rate: 0.0100
2022-01-11 18:07:35,065 - INFO - [Step=30250]	Loss=0.8503	254.1 examples/second
2022-01-11 18:09:31,201 - INFO - [Step=30500]	Loss=0.8386	275.5 examples/second
2022-01-11 18:11:27,172 - INFO - [Step=30750]	Loss=0.8362	275.9 examples/second
2022-01-11 18:12:42,099 - INFO - Test Loss=0.9683, Test top-1 acc=0.7559
2022-01-11 18:12:42,100 - INFO - Group Accuracy:

2022-01-11 18:12:42,100 - INFO - [0.97638553 0.98963857 0.9833735  0.9918072  0.9819277  0.99373496
 0.9807229  0.9838554  0.9901205  0.97542167 0.98771083 0.98240966
 0.97542167 0.97927713 0.97542167 0.98722893 0.99421686]
2022-01-11 18:12:42,101 - INFO - Saving...
2022-01-11 18:12:42,406 - INFO - Epoch time: 397.6561381816864
2022-01-11 18:12:42,407 - INFO - 
Epoch: 37
2022-01-11 18:12:42,407 - INFO - 
Learning Rate: 0.0100
2022-01-11 18:13:33,327 - INFO - [Step=31000]	Loss=0.8334	253.7 examples/second
2022-01-11 18:15:29,483 - INFO - [Step=31250]	Loss=0.8172	275.5 examples/second
2022-01-11 18:17:25,598 - INFO - [Step=31500]	Loss=0.8281	275.6 examples/second
2022-01-11 18:19:20,074 - INFO - Test Loss=0.9307, Test top-1 acc=0.7643
2022-01-11 18:19:20,074 - INFO - Group Accuracy:

2022-01-11 18:19:20,074 - INFO - [0.97831327 0.9889157  0.9840964  0.9930121  0.98361444 0.9939759
 0.9807229  0.98289156 0.99060243 0.97590363 0.9884337  0.9816868
 0.97493976 0.9787952  0.9771084  0.98746985 0.99493974]
2022-01-11 18:19:20,075 - INFO - Saving...
2022-01-11 18:19:20,351 - INFO - Epoch time: 397.9444389343262
2022-01-11 18:19:20,351 - INFO - 
Epoch: 38
2022-01-11 18:19:20,351 - INFO - 
Learning Rate: 0.0100
2022-01-11 18:19:32,480 - INFO - [Step=31750]	Loss=0.8242	252.2 examples/second
2022-01-11 18:21:28,683 - INFO - [Step=32000]	Loss=0.8054	275.4 examples/second
2022-01-11 18:23:24,844 - INFO - [Step=32250]	Loss=0.8109	275.5 examples/second
2022-01-11 18:25:21,087 - INFO - [Step=32500]	Loss=0.8287	275.3 examples/second
2022-01-11 18:25:58,655 - INFO - Test Loss=1.1067, Test top-1 acc=0.7593
2022-01-11 18:25:58,656 - INFO - Group Accuracy:

2022-01-11 18:25:58,656 - INFO - [0.9775904  0.98722893 0.98433733 0.9927711  0.98313254 0.993494
 0.97975904 0.9819277  0.99036145 0.9766265  0.9879518  0.9819277
 0.9746988  0.9780723  0.97638553 0.98578316 0.99373496]
2022-01-11 18:25:58,657 - INFO - Epoch time: 398.3052554130554
2022-01-11 18:25:58,657 - INFO - 
Epoch: 39
2022-01-11 18:25:58,657 - INFO - 
Learning Rate: 0.0100
2022-01-11 18:27:27,572 - INFO - [Step=32750]	Loss=0.7998	253.0 examples/second
2022-01-11 18:29:23,657 - INFO - [Step=33000]	Loss=0.8097	275.7 examples/second
2022-01-11 18:31:19,755 - INFO - [Step=33250]	Loss=0.7947	275.6 examples/second
2022-01-11 18:32:37,113 - INFO - Test Loss=0.9645, Test top-1 acc=0.7624
2022-01-11 18:32:37,113 - INFO - Group Accuracy:

2022-01-11 18:32:37,113 - INFO - [0.9773494  0.9886747  0.9848193  0.99421686 0.9826506  0.99421686
 0.9816868  0.9819277  0.99060243 0.97590363 0.99084336 0.9814458
 0.9773494  0.97927713 0.9768675  0.98722893 0.99493974]
2022-01-11 18:32:37,114 - INFO - Epoch time: 398.45732402801514
2022-01-11 18:32:37,114 - INFO - 
Epoch: 40
2022-01-11 18:32:37,114 - INFO - 
Learning Rate: 0.0100
2022-01-11 18:33:25,839 - INFO - [Step=33500]	Loss=0.8010	253.8 examples/second
2022-01-11 18:35:22,375 - INFO - [Step=33750]	Loss=0.7817	274.6 examples/second
2022-01-11 18:37:18,507 - INFO - [Step=34000]	Loss=0.7949	275.5 examples/second
2022-01-11 18:39:15,652 - INFO - Test Loss=0.9581, Test top-1 acc=0.7525
2022-01-11 18:39:15,653 - INFO - Group Accuracy:

2022-01-11 18:39:15,653 - INFO - [0.9771084  0.9889157  0.9840964  0.9927711  0.9816868  0.99373496
 0.9814458  0.97951806 0.9901205  0.9768675  0.9901205  0.9812048
 0.97590363 0.97927713 0.97566265 0.98698795 0.99493974]
2022-01-11 18:39:15,654 - INFO - Epoch time: 398.53991198539734
2022-01-11 18:39:15,654 - INFO - 
Epoch: 41
2022-01-11 18:39:15,654 - INFO - 
Learning Rate: 0.0100
2022-01-11 18:39:24,645 - INFO - [Step=34250]	Loss=0.7972	253.7 examples/second
2022-01-11 18:41:21,240 - INFO - [Step=34500]	Loss=0.7739	274.5 examples/second
2022-01-11 18:43:17,441 - INFO - [Step=34750]	Loss=0.7896	275.4 examples/second
2022-01-11 18:45:13,807 - INFO - [Step=35000]	Loss=0.7818	275.0 examples/second
2022-01-11 18:45:54,212 - INFO - Test Loss=0.9348, Test top-1 acc=0.7680
2022-01-11 18:45:54,213 - INFO - Group Accuracy:

2022-01-11 18:45:54,213 - INFO - [0.9766265  0.9898795  0.9850602  0.9927711  0.98361444 0.9939759
 0.9814458  0.9804819  0.99036145 0.97614455 0.99108434 0.9826506
 0.97614455 0.97903615 0.97590363 0.98771083 0.9951807 ]
2022-01-11 18:45:54,214 - INFO - Saving...
2022-01-11 18:45:54,473 - INFO - Epoch time: 398.8186557292938
2022-01-11 18:45:54,473 - INFO - 
Epoch: 42
2022-01-11 18:45:54,473 - INFO - 
Learning Rate: 0.0100
2022-01-11 18:47:20,755 - INFO - [Step=35250]	Loss=0.7673	252.1 examples/second
2022-01-11 18:49:17,186 - INFO - [Step=35500]	Loss=0.7734	274.8 examples/second
2022-01-11 18:51:13,808 - INFO - [Step=35750]	Loss=0.7869	274.4 examples/second
2022-01-11 18:52:33,729 - INFO - Test Loss=0.9087, Test top-1 acc=0.7578
2022-01-11 18:52:33,729 - INFO - Group Accuracy:

2022-01-11 18:52:33,729 - INFO - [0.9771084  0.9889157  0.98313254 0.9920482  0.9821687  0.99373496
 0.9814458  0.9816868  0.99036145 0.9768675  0.99036145 0.98024094
 0.97590363 0.9785542  0.97566265 0.9884337  0.9954217 ]
2022-01-11 18:52:33,730 - INFO - Epoch time: 399.2569570541382
2022-01-11 18:52:33,730 - INFO - 
Epoch: 43
2022-01-11 18:52:33,730 - INFO - 
Learning Rate: 0.0100
2022-01-11 18:53:20,154 - INFO - [Step=36000]	Loss=0.7522	253.3 examples/second
2022-01-11 18:55:16,529 - INFO - [Step=36250]	Loss=0.7537	275.0 examples/second
2022-01-11 18:57:12,986 - INFO - [Step=36500]	Loss=0.7726	274.8 examples/second
2022-01-11 18:59:12,695 - INFO - Test Loss=0.9259, Test top-1 acc=0.7627
2022-01-11 18:59:12,695 - INFO - Group Accuracy:

2022-01-11 18:59:12,695 - INFO - [0.9771084  0.9889157  0.9855422  0.9925301  0.9826506  0.9939759
 0.97927713 0.9812048  0.99036145 0.97542167 0.98771083 0.98433733
 0.97518075 0.9787952  0.9773494  0.9879518  0.99445784]
2022-01-11 18:59:12,696 - INFO - Epoch time: 398.965993642807
2022-01-11 18:59:12,696 - INFO - 
Epoch: 44
2022-01-11 18:59:12,696 - INFO - 
Learning Rate: 0.0100
2022-01-11 18:59:19,819 - INFO - [Step=36750]	Loss=0.7620	252.3 examples/second
2022-01-11 19:01:16,641 - INFO - [Step=37000]	Loss=0.7368	273.9 examples/second
2022-01-11 19:03:12,901 - INFO - [Step=37250]	Loss=0.7545	275.2 examples/second
2022-01-11 19:05:09,456 - INFO - [Step=37500]	Loss=0.7600	274.5 examples/second
2022-01-11 19:05:52,158 - INFO - Test Loss=0.8756, Test top-1 acc=0.7629
2022-01-11 19:05:52,159 - INFO - Group Accuracy:

2022-01-11 19:05:52,159 - INFO - [0.9780723  0.9889157  0.9809638  0.99060243 0.9826506  0.99373496
 0.9807229  0.98289156 0.9898795  0.97542167 0.9893976  0.9814458
 0.9746988  0.9807229  0.9775904  0.9886747  0.9959036 ]
2022-01-11 19:05:52,159 - INFO - Epoch time: 399.4633252620697
2022-01-11 19:05:52,159 - INFO - 
Epoch: 45
2022-01-11 19:05:52,159 - INFO - 
Learning Rate: 0.0100
2022-01-11 19:07:15,717 - INFO - [Step=37750]	Loss=0.7427	253.4 examples/second
2022-01-11 19:09:12,008 - INFO - [Step=38000]	Loss=0.7550	275.2 examples/second
2022-01-11 19:11:08,515 - INFO - [Step=38250]	Loss=0.7585	274.7 examples/second
2022-01-11 19:12:30,894 - INFO - Test Loss=0.9009, Test top-1 acc=0.7665
2022-01-11 19:12:30,894 - INFO - Group Accuracy:

2022-01-11 19:12:30,894 - INFO - [0.97831327 0.9891566  0.9840964  0.99036145 0.9833735  0.993494
 0.9804819  0.98361444 0.98963857 0.97614455 0.99036145 0.98361444
 0.97614455 0.9804819  0.97638553 0.98771083 0.9946988 ]
2022-01-11 19:12:30,896 - INFO - Epoch time: 398.73619842529297
2022-01-11 19:12:30,896 - INFO - 
Epoch: 46
2022-01-11 19:12:30,896 - INFO - 
Learning Rate: 0.0100
2022-01-11 19:13:15,143 - INFO - [Step=38500]	Loss=0.7450	252.7 examples/second
2022-01-11 19:15:11,398 - INFO - [Step=38750]	Loss=0.7406	275.3 examples/second
2022-01-11 19:17:07,590 - INFO - [Step=39000]	Loss=0.7287	275.4 examples/second
2022-01-11 19:19:09,546 - INFO - Test Loss=0.9850, Test top-1 acc=0.7629
2022-01-11 19:19:09,546 - INFO - Group Accuracy:

2022-01-11 19:19:09,546 - INFO - [0.9773494  0.9891566  0.9819277  0.9918072  0.9819277  0.993253
 0.9814458  0.9819277  0.9891566  0.9742169  0.99060243 0.98289156
 0.9766265  0.98       0.97518075 0.9884337  0.99445784]
2022-01-11 19:19:09,548 - INFO - Epoch time: 398.65184593200684
2022-01-11 19:19:09,548 - INFO - 
Epoch: 47
2022-01-11 19:19:09,548 - INFO - 
Learning Rate: 0.0100
2022-01-11 19:19:13,923 - INFO - [Step=39250]	Loss=0.7372	253.3 examples/second
2022-01-11 19:21:10,253 - INFO - [Step=39500]	Loss=0.7359	275.1 examples/second
2022-01-11 19:23:06,533 - INFO - [Step=39750]	Loss=0.7328	275.2 examples/second
2022-01-11 19:25:02,679 - INFO - [Step=40000]	Loss=0.7386	275.5 examples/second
2022-01-11 19:25:47,494 - INFO - Test Loss=0.9837, Test top-1 acc=0.7602
2022-01-11 19:25:47,494 - INFO - Group Accuracy:

2022-01-11 19:25:47,494 - INFO - [0.9768675  0.9886747  0.9848193  0.9922892  0.9838554  0.9939759
 0.98024094 0.9807229  0.99036145 0.9746988  0.9891566  0.98240966
 0.97542167 0.9816868  0.97542167 0.9898795  0.9959036 ]
2022-01-11 19:25:47,495 - INFO - Epoch time: 397.9477002620697
2022-01-11 19:25:47,495 - INFO - 
Epoch: 48
2022-01-11 19:25:47,495 - INFO - 
Learning Rate: 0.0100
2022-01-11 19:27:08,667 - INFO - [Step=40250]	Loss=0.7125	254.0 examples/second
2022-01-11 19:29:04,856 - INFO - [Step=40500]	Loss=0.7324	275.4 examples/second
2022-01-11 19:31:01,279 - INFO - [Step=40750]	Loss=0.7234	274.9 examples/second
2022-01-11 19:32:25,933 - INFO - Test Loss=1.0194, Test top-1 acc=0.7617
2022-01-11 19:32:25,934 - INFO - Group Accuracy:

2022-01-11 19:32:25,934 - INFO - [0.9768675  0.98963857 0.9855422  0.99084336 0.98289156 0.99421686
 0.98024094 0.9807229  0.99036145 0.9768675  0.98963857 0.98433733
 0.97518075 0.97975904 0.9778313  0.98746985 0.9951807 ]
2022-01-11 19:32:25,934 - INFO - Epoch time: 398.43887424468994
2022-01-11 19:32:25,934 - INFO - 
Epoch: 49
2022-01-11 19:32:25,934 - INFO - 
Learning Rate: 0.0100
2022-01-11 19:33:07,603 - INFO - [Step=41000]	Loss=0.7097	253.3 examples/second
2022-01-11 19:35:03,758 - INFO - [Step=41250]	Loss=0.7102	275.5 examples/second
2022-01-11 19:36:59,963 - INFO - [Step=41500]	Loss=0.7165	275.4 examples/second
2022-01-11 19:38:56,403 - INFO - [Step=41750]	Loss=0.7246	274.8 examples/second
2022-01-11 19:39:04,003 - INFO - Test Loss=0.9468, Test top-1 acc=0.7701
2022-01-11 19:39:04,003 - INFO - Group Accuracy:

2022-01-11 19:39:04,004 - INFO - [0.9785542  0.9891566  0.9853012  0.99156624 0.9819277  0.993494
 0.9775904  0.9814458  0.9893976  0.9780723  0.9891566  0.9826506
 0.97493976 0.9804819  0.97542167 0.98771083 0.9951807 ]
2022-01-11 19:39:04,004 - INFO - Saving...
2022-01-11 19:39:04,236 - INFO - Epoch time: 398.30143189430237
2022-01-11 19:39:04,236 - INFO - 
Epoch: 50
2022-01-11 19:39:04,236 - INFO - 
Learning Rate: 0.0100
2022-01-11 19:41:02,776 - INFO - [Step=42000]	Loss=0.7059	253.2 examples/second
2022-01-11 19:42:59,214 - INFO - [Step=42250]	Loss=0.7064	274.8 examples/second
2022-01-11 19:44:55,464 - INFO - [Step=42500]	Loss=0.7314	275.3 examples/second
2022-01-11 19:45:42,803 - INFO - Test Loss=0.9170, Test top-1 acc=0.7680
2022-01-11 19:45:42,804 - INFO - Group Accuracy:

2022-01-11 19:45:42,804 - INFO - [0.9780723  0.9884337  0.9848193  0.993494   0.9826506  0.9930121
 0.9809638  0.98240966 0.9891566  0.9771084  0.99036145 0.98289156
 0.97638553 0.97927713 0.9773494  0.9881928  0.99493974]
2022-01-11 19:45:42,805 - INFO - Epoch time: 398.56883478164673
2022-01-11 19:45:42,805 - INFO - 
Epoch: 51
2022-01-11 19:45:42,805 - INFO - 
Learning Rate: 0.0100
2022-01-11 19:47:01,592 - INFO - [Step=42750]	Loss=0.6943	253.7 examples/second
2022-01-11 19:48:58,105 - INFO - [Step=43000]	Loss=0.7030	274.6 examples/second
2022-01-11 19:50:54,516 - INFO - [Step=43250]	Loss=0.7112	274.9 examples/second
2022-01-11 19:52:21,095 - INFO - Test Loss=0.9717, Test top-1 acc=0.7704
2022-01-11 19:52:21,096 - INFO - Group Accuracy:

2022-01-11 19:52:21,096 - INFO - [0.9778313  0.9889157  0.98578316 0.9930121  0.9838554  0.993494
 0.9804819  0.9819277  0.99084336 0.9771084  0.99036145 0.9848193
 0.9766265  0.98024094 0.9768675  0.9889157  0.9961446 ]
2022-01-11 19:52:21,097 - INFO - Saving...
2022-01-11 19:52:21,327 - INFO - Epoch time: 398.52203822135925
2022-01-11 19:52:21,327 - INFO - 
Epoch: 52
2022-01-11 19:52:21,327 - INFO - 
Learning Rate: 0.0100
2022-01-11 19:53:00,595 - INFO - [Step=43500]	Loss=0.7194	253.8 examples/second
2022-01-11 19:54:57,065 - INFO - [Step=43750]	Loss=0.6907	274.7 examples/second
2022-01-11 19:56:53,477 - INFO - [Step=44000]	Loss=0.7310	274.9 examples/second
2022-01-11 19:58:49,908 - INFO - [Step=44250]	Loss=0.7009	274.8 examples/second
2022-01-11 19:58:59,756 - INFO - Test Loss=0.9008, Test top-1 acc=0.7687
2022-01-11 19:58:59,756 - INFO - Group Accuracy:

2022-01-11 19:58:59,756 - INFO - [0.97614455 0.98963857 0.98578316 0.9920482  0.9838554  0.9939759
 0.9804819  0.98361444 0.9901205  0.9771084  0.98674697 0.98313254
 0.97590363 0.9785542  0.9766265  0.98698795 0.99493974]
2022-01-11 19:58:59,757 - INFO - Epoch time: 398.4301609992981
2022-01-11 19:58:59,758 - INFO - 
Epoch: 53
2022-01-11 19:58:59,758 - INFO - 
Learning Rate: 0.0100
2022-01-11 20:00:56,357 - INFO - [Step=44500]	Loss=0.6827	253.1 examples/second
2022-01-11 20:02:52,914 - INFO - [Step=44750]	Loss=0.6921	274.5 examples/second
2022-01-11 20:04:49,371 - INFO - [Step=45000]	Loss=0.6947	274.8 examples/second
2022-01-11 20:05:38,981 - INFO - Test Loss=0.9714, Test top-1 acc=0.7639
2022-01-11 20:05:38,981 - INFO - Group Accuracy:

2022-01-11 20:05:38,981 - INFO - [0.9785542  0.9901205  0.9850602  0.993253   0.9816868  0.9939759
 0.97927713 0.98024094 0.9893976  0.9768675  0.9891566  0.9812048
 0.97493976 0.9804819  0.9766265  0.9879518  0.99493974]
2022-01-11 20:05:38,982 - INFO - Epoch time: 399.22454261779785
2022-01-11 20:05:38,982 - INFO - 
Epoch: 54
2022-01-11 20:05:38,982 - INFO - 
Learning Rate: 0.0100
2022-01-11 20:06:55,539 - INFO - [Step=45250]	Loss=0.6837	253.6 examples/second
2022-01-11 20:08:51,906 - INFO - [Step=45500]	Loss=0.6842	275.0 examples/second
2022-01-11 20:10:48,467 - INFO - [Step=45750]	Loss=0.7003	274.5 examples/second
2022-01-11 20:12:17,752 - INFO - Test Loss=0.9443, Test top-1 acc=0.7670
2022-01-11 20:12:17,753 - INFO - Group Accuracy:

2022-01-11 20:12:17,753 - INFO - [0.9785542  0.9891566  0.9826506  0.9918072  0.98361444 0.993253
 0.97975904 0.9838554  0.98963857 0.97614455 0.99036145 0.9826506
 0.97831327 0.9809638  0.9766265  0.9886747  0.9954217 ]
2022-01-11 20:12:17,754 - INFO - Epoch time: 398.7712912559509
2022-01-11 20:12:17,754 - INFO - 
Epoch: 55
2022-01-11 20:12:17,754 - INFO - 
Learning Rate: 0.0100
2022-01-11 20:12:54,976 - INFO - [Step=46000]	Loss=0.6951	252.9 examples/second
2022-01-11 20:14:51,490 - INFO - [Step=46250]	Loss=0.6663	274.6 examples/second
2022-01-11 20:16:47,882 - INFO - [Step=46500]	Loss=0.6942	274.9 examples/second
2022-01-11 20:18:44,467 - INFO - [Step=46750]	Loss=0.6877	274.5 examples/second
2022-01-11 20:18:56,527 - INFO - Test Loss=0.9897, Test top-1 acc=0.7733
2022-01-11 20:18:56,527 - INFO - Group Accuracy:

2022-01-11 20:18:56,527 - INFO - [0.9785542  0.99060243 0.9845783  0.9922892  0.98361444 0.9930121
 0.98024094 0.9819277  0.9898795  0.9771084  0.9893976  0.9838554
 0.9785542  0.98024094 0.9773494  0.98771083 0.99493974]
2022-01-11 20:18:56,528 - INFO - Saving...
2022-01-11 20:18:56,769 - INFO - Epoch time: 399.0151126384735
2022-01-11 20:18:56,769 - INFO - 
Epoch: 56
2022-01-11 20:18:56,769 - INFO - 
Learning Rate: 0.0100
2022-01-11 20:20:50,495 - INFO - [Step=47000]	Loss=0.6627	253.9 examples/second
2022-01-11 20:22:46,851 - INFO - [Step=47250]	Loss=0.6668	275.0 examples/second
2022-01-11 20:24:43,053 - INFO - [Step=47500]	Loss=0.6906	275.4 examples/second
2022-01-11 20:25:34,522 - INFO - Test Loss=0.9618, Test top-1 acc=0.7600
2022-01-11 20:25:34,523 - INFO - Group Accuracy:

2022-01-11 20:25:34,523 - INFO - [0.9780723  0.9891566  0.9833735  0.99060243 0.9812048  0.99421686
 0.97927713 0.98024094 0.9901205  0.9746988  0.9891566  0.9826506
 0.97566265 0.9785542  0.97518075 0.9886747  0.9956626 ]
2022-01-11 20:25:34,524 - INFO - Epoch time: 397.7550759315491
2022-01-11 20:25:34,524 - INFO - 
Epoch: 57
2022-01-11 20:25:34,524 - INFO - 
Learning Rate: 0.0100
2022-01-11 20:26:48,645 - INFO - [Step=47750]	Loss=0.6761	254.8 examples/second
2022-01-11 20:28:44,687 - INFO - [Step=48000]	Loss=0.6609	275.8 examples/second
2022-01-11 20:30:40,898 - INFO - [Step=48250]	Loss=0.6795	275.4 examples/second
2022-01-11 20:32:12,236 - INFO - Test Loss=0.9573, Test top-1 acc=0.7595
2022-01-11 20:32:12,237 - INFO - Group Accuracy:

2022-01-11 20:32:12,237 - INFO - [0.9780723  0.9879518  0.9838554  0.9913253  0.9833735  0.99445784
 0.9771084  0.9826506  0.99036145 0.97542167 0.9893976  0.9838554
 0.97590363 0.97831327 0.9773494  0.98674697 0.99421686]
2022-01-11 20:32:12,237 - INFO - Epoch time: 397.71331000328064
2022-01-11 20:32:12,237 - INFO - 
Epoch: 58
2022-01-11 20:32:12,238 - INFO - 
Learning Rate: 0.0100
2022-01-11 20:32:46,745 - INFO - [Step=48500]	Loss=0.6830	254.3 examples/second
2022-01-11 20:34:42,878 - INFO - [Step=48750]	Loss=0.6766	275.5 examples/second
2022-01-11 20:36:38,922 - INFO - [Step=49000]	Loss=0.6717	275.8 examples/second
2022-01-11 20:38:35,400 - INFO - [Step=49250]	Loss=0.6768	274.7 examples/second
2022-01-11 20:38:50,335 - INFO - Test Loss=0.9988, Test top-1 acc=0.7636
2022-01-11 20:38:50,336 - INFO - Group Accuracy:

2022-01-11 20:38:50,336 - INFO - [0.9771084  0.99084336 0.98433733 0.9927711  0.9833735  0.9939759
 0.9785542  0.9814458  0.9891566  0.9766265  0.9901205  0.9838554
 0.97590363 0.97831327 0.97590363 0.98626506 0.9968675 ]
2022-01-11 20:38:50,337 - INFO - Epoch time: 398.099401473999
2022-01-11 20:38:50,337 - INFO - 
Epoch: 59
2022-01-11 20:38:50,337 - INFO - 
Learning Rate: 0.0010
2022-01-11 20:40:42,331 - INFO - [Step=49500]	Loss=0.6291	252.1 examples/second
2022-01-11 20:42:39,135 - INFO - [Step=49750]	Loss=0.6014	274.0 examples/second
2022-01-11 20:44:36,050 - INFO - [Step=50000]	Loss=0.5823	273.7 examples/second
2022-01-11 20:45:30,554 - INFO - Test Loss=0.9469, Test top-1 acc=0.7773
2022-01-11 20:45:30,555 - INFO - Group Accuracy:

2022-01-11 20:45:30,555 - INFO - [0.97975904 0.9918072  0.9855422  0.9930121  0.9845783  0.9939759
 0.9821687  0.9816868  0.9901205  0.9778313  0.9898795  0.9860241
 0.9775904  0.98       0.9778313  0.9879518  0.9961446 ]
2022-01-11 20:45:30,556 - INFO - Saving...
2022-01-11 20:45:30,826 - INFO - Epoch time: 400.48888969421387
2022-01-11 20:45:30,826 - INFO - 
Epoch: 60
2022-01-11 20:45:30,826 - INFO - 
Learning Rate: 0.0010
2022-01-11 20:46:43,323 - INFO - [Step=50250]	Loss=0.5745	251.4 examples/second
2022-01-11 20:48:40,427 - INFO - [Step=50500]	Loss=0.5677	273.3 examples/second
2022-01-11 20:50:37,337 - INFO - [Step=50750]	Loss=0.5587	273.7 examples/second
2022-01-11 20:52:11,723 - INFO - Test Loss=0.8619, Test top-1 acc=0.7812
2022-01-11 20:52:11,723 - INFO - Group Accuracy:

2022-01-11 20:52:11,723 - INFO - [0.97927713 0.99156624 0.98578316 0.9925301  0.98578316 0.993253
 0.9826506  0.98240966 0.99036145 0.9785542  0.99036145 0.98650604
 0.9771084  0.97903615 0.9766265  0.9881928  0.9963855 ]
2022-01-11 20:52:11,724 - INFO - Saving...
2022-01-11 20:52:11,973 - INFO - Epoch time: 401.14713764190674
2022-01-11 20:52:11,973 - INFO - 
Epoch: 61
2022-01-11 20:52:11,974 - INFO - 
Learning Rate: 0.0010
2022-01-11 20:52:44,570 - INFO - [Step=51000]	Loss=0.5717	251.5 examples/second
2022-01-11 20:54:41,875 - INFO - [Step=51250]	Loss=0.5773	272.8 examples/second
2022-01-11 20:56:39,023 - INFO - [Step=51500]	Loss=0.5681	273.2 examples/second
2022-01-11 20:58:36,303 - INFO - [Step=51750]	Loss=0.5711	272.9 examples/second
2022-01-11 20:58:53,330 - INFO - Test Loss=0.9796, Test top-1 acc=0.7795
2022-01-11 20:58:53,330 - INFO - Group Accuracy:

2022-01-11 20:58:53,330 - INFO - [0.97975904 0.99084336 0.9855422  0.9922892  0.9848193  0.993253
 0.9816868  0.9814458  0.99108434 0.9785542  0.9901205  0.9860241
 0.9775904  0.98       0.9771084  0.9881928  0.9956626 ]
2022-01-11 20:58:53,331 - INFO - Epoch time: 401.3571455478668
2022-01-11 20:58:53,331 - INFO - 
Epoch: 62
2022-01-11 20:58:53,331 - INFO - 
Learning Rate: 0.0010
2022-01-11 21:00:42,701 - INFO - [Step=52000]	Loss=0.5603	253.2 examples/second
2022-01-11 21:02:39,818 - INFO - [Step=52250]	Loss=0.5469	273.2 examples/second
2022-01-11 21:04:36,768 - INFO - [Step=52500]	Loss=0.5543	273.6 examples/second
2022-01-11 21:05:34,481 - INFO - Test Loss=0.9506, Test top-1 acc=0.7786
2022-01-11 21:05:34,481 - INFO - Group Accuracy:

2022-01-11 21:05:34,481 - INFO - [0.98       0.99084336 0.9855422  0.9920482  0.9848193  0.9939759
 0.9812048  0.98240966 0.99108434 0.9771084  0.99060243 0.9860241
 0.9775904  0.98024094 0.9773494  0.9884337  0.9961446 ]
2022-01-11 21:05:34,482 - INFO - Epoch time: 401.1510167121887
2022-01-11 21:05:34,482 - INFO - 
Epoch: 63
2022-01-11 21:05:34,482 - INFO - 
Learning Rate: 0.0010
2022-01-11 21:06:44,483 - INFO - [Step=52750]	Loss=0.5524	250.6 examples/second
2022-01-11 21:08:41,196 - INFO - [Step=53000]	Loss=0.5485	274.2 examples/second
2022-01-11 21:10:38,307 - INFO - [Step=53250]	Loss=0.5574	273.2 examples/second
2022-01-11 21:12:15,030 - INFO - Test Loss=1.0130, Test top-1 acc=0.7773
2022-01-11 21:12:15,031 - INFO - Group Accuracy:

2022-01-11 21:12:15,031 - INFO - [0.97975904 0.99084336 0.9860241  0.9930121  0.9853012  0.993253
 0.9821687  0.98313254 0.9901205  0.9775904  0.99084336 0.9855422
 0.9775904  0.98       0.9771084  0.9881928  0.9956626 ]
2022-01-11 21:12:15,032 - INFO - Epoch time: 400.54999327659607
2022-01-11 21:12:15,032 - INFO - 
Epoch: 64
2022-01-11 21:12:15,032 - INFO - 
Learning Rate: 0.0010
2022-01-11 21:12:45,205 - INFO - [Step=53500]	Loss=0.5584	252.2 examples/second
2022-01-11 21:14:42,160 - INFO - [Step=53750]	Loss=0.5515	273.6 examples/second
2022-01-11 21:16:39,052 - INFO - [Step=54000]	Loss=0.5395	273.8 examples/second
2022-01-11 21:18:36,088 - INFO - [Step=54250]	Loss=0.5608	273.4 examples/second
2022-01-11 21:18:55,708 - INFO - Test Loss=0.8952, Test top-1 acc=0.7759
2022-01-11 21:18:55,709 - INFO - Group Accuracy:

2022-01-11 21:18:55,709 - INFO - [0.97927713 0.9913253  0.9853012  0.9918072  0.98650604 0.993494
 0.98289156 0.9816868  0.98963857 0.9771084  0.99036145 0.9850602
 0.9778313  0.97951806 0.9778313  0.9889157  0.9961446 ]
2022-01-11 21:18:55,710 - INFO - Epoch time: 400.67780327796936
2022-01-11 21:18:55,710 - INFO - 
Epoch: 65
2022-01-11 21:18:55,710 - INFO - 
Learning Rate: 0.0010
2022-01-11 21:20:43,334 - INFO - [Step=54500]	Loss=0.5522	251.5 examples/second
2022-01-11 21:22:40,221 - INFO - [Step=54750]	Loss=0.5427	273.8 examples/second
2022-01-11 21:24:37,111 - INFO - [Step=55000]	Loss=0.5381	273.8 examples/second
2022-01-11 21:25:36,529 - INFO - Test Loss=0.9474, Test top-1 acc=0.7783
2022-01-11 21:25:36,529 - INFO - Group Accuracy:

2022-01-11 21:25:36,530 - INFO - [0.97975904 0.99036145 0.9855422  0.9918072  0.9855422  0.993494
 0.9819277  0.9819277  0.99060243 0.9787952  0.9913253  0.9855422
 0.9787952  0.97951806 0.9780723  0.9889157  0.9966265 ]
2022-01-11 21:25:36,530 - INFO - Epoch time: 400.82051062583923
2022-01-11 21:25:36,530 - INFO - 
Epoch: 66
2022-01-11 21:25:36,530 - INFO - 
Learning Rate: 0.0010
2022-01-11 21:26:44,516 - INFO - [Step=55250]	Loss=0.5439	251.2 examples/second
2022-01-11 21:28:41,478 - INFO - [Step=55500]	Loss=0.5340	273.6 examples/second
2022-01-11 21:30:38,319 - INFO - [Step=55750]	Loss=0.5452	273.9 examples/second
2022-01-11 21:32:17,339 - INFO - Test Loss=0.9445, Test top-1 acc=0.7790
2022-01-11 21:32:17,339 - INFO - Group Accuracy:

2022-01-11 21:32:17,339 - INFO - [0.9787952  0.99036145 0.9860241  0.9922892  0.9853012  0.993253
 0.98240966 0.9821687  0.99084336 0.9780723  0.9913253  0.98578316
 0.9778313  0.98024094 0.9775904  0.9893976  0.9959036 ]
2022-01-11 21:32:17,339 - INFO - Epoch time: 400.80916595458984
2022-01-11 21:32:17,340 - INFO - 
Epoch: 67
2022-01-11 21:32:17,340 - INFO - 
Learning Rate: 0.0010
2022-01-11 21:32:45,316 - INFO - [Step=56000]	Loss=0.5349	252.0 examples/second
2022-01-11 21:34:42,473 - INFO - [Step=56250]	Loss=0.5323	273.1 examples/second
2022-01-11 21:36:39,514 - INFO - [Step=56500]	Loss=0.5282	273.4 examples/second
2022-01-11 21:38:36,442 - INFO - [Step=56750]	Loss=0.5443	273.7 examples/second
2022-01-11 21:38:58,128 - INFO - Test Loss=1.0233, Test top-1 acc=0.7773
2022-01-11 21:38:58,129 - INFO - Group Accuracy:

2022-01-11 21:38:58,129 - INFO - [0.97903615 0.98963857 0.98626506 0.9930121  0.98578316 0.99373496
 0.9821687  0.98240966 0.9898795  0.9787952  0.99084336 0.9850602
 0.9785542  0.9804819  0.9773494  0.9881928  0.9959036 ]
2022-01-11 21:38:58,130 - INFO - Epoch time: 400.7902994155884
2022-01-11 21:38:58,130 - INFO - 
Epoch: 68
2022-01-11 21:38:58,130 - INFO - 
Learning Rate: 0.0010
2022-01-11 21:40:43,510 - INFO - [Step=57000]	Loss=0.5251	251.8 examples/second
2022-01-11 21:42:40,505 - INFO - [Step=57250]	Loss=0.5310	273.5 examples/second
2022-01-11 21:44:37,194 - INFO - [Step=57500]	Loss=0.5323	274.2 examples/second
2022-01-11 21:45:38,761 - INFO - Test Loss=0.9295, Test top-1 acc=0.7800
2022-01-11 21:45:38,761 - INFO - Group Accuracy:

2022-01-11 21:45:38,761 - INFO - [0.97831327 0.99084336 0.9860241  0.9922892  0.9860241  0.993494
 0.98313254 0.98313254 0.9901205  0.9775904  0.9901205  0.9845783
 0.9780723  0.97975904 0.9778313  0.98963857 0.9959036 ]
2022-01-11 21:45:38,762 - INFO - Epoch time: 400.6325626373291
2022-01-11 21:45:38,763 - INFO - 
Epoch: 69
2022-01-11 21:45:38,763 - INFO - 
Learning Rate: 0.0010
2022-01-11 21:46:44,225 - INFO - [Step=57750]	Loss=0.5396	251.9 examples/second
2022-01-11 21:48:41,500 - INFO - [Step=58000]	Loss=0.5248	272.9 examples/second
2022-01-11 21:50:38,286 - INFO - [Step=58250]	Loss=0.5321	274.0 examples/second
2022-01-11 21:52:19,791 - INFO - Test Loss=0.9000, Test top-1 acc=0.7788
2022-01-11 21:52:19,791 - INFO - Group Accuracy:

2022-01-11 21:52:19,791 - INFO - [0.9787952  0.99060243 0.9853012  0.9920482  0.9848193  0.993494
 0.98240966 0.98289156 0.99084336 0.97903615 0.99108434 0.9860241
 0.9771084  0.97975904 0.9775904  0.9881928  0.9959036 ]
2022-01-11 21:52:19,792 - INFO - Epoch time: 401.0293848514557
2022-01-11 21:52:19,792 - INFO - 
Epoch: 70
2022-01-11 21:52:19,792 - INFO - 
Learning Rate: 0.0010
2022-01-11 21:52:45,411 - INFO - [Step=58500]	Loss=0.5311	251.7 examples/second
2022-01-11 21:54:42,366 - INFO - [Step=58750]	Loss=0.5246	273.6 examples/second
2022-01-11 21:56:39,841 - INFO - [Step=59000]	Loss=0.5385	272.4 examples/second
2022-01-11 21:58:36,830 - INFO - [Step=59250]	Loss=0.5407	273.5 examples/second
2022-01-11 21:59:01,141 - INFO - Test Loss=0.9815, Test top-1 acc=0.7761
2022-01-11 21:59:01,142 - INFO - Group Accuracy:

2022-01-11 21:59:01,142 - INFO - [0.9785542  0.99036145 0.9850602  0.99156624 0.9860241  0.9939759
 0.9816868  0.9819277  0.9901205  0.97831327 0.99060243 0.9850602
 0.9766265  0.98       0.9775904  0.9893976  0.9959036 ]
2022-01-11 21:59:01,143 - INFO - Epoch time: 401.3508515357971
2022-01-11 21:59:01,143 - INFO - 
Epoch: 71
2022-01-11 21:59:01,143 - INFO - 
Learning Rate: 0.0010
2022-01-11 22:00:44,061 - INFO - [Step=59500]	Loss=0.5274	251.5 examples/second
2022-01-11 22:02:41,169 - INFO - [Step=59750]	Loss=0.5340	273.3 examples/second
2022-01-11 22:04:38,008 - INFO - [Step=60000]	Loss=0.5239	273.9 examples/second
2022-01-11 22:05:42,208 - INFO - Test Loss=0.9318, Test top-1 acc=0.7778
2022-01-11 22:05:42,208 - INFO - Group Accuracy:

2022-01-11 22:05:42,208 - INFO - [0.9785542  0.99036145 0.9853012  0.9922892  0.9860241  0.99373496
 0.9816868  0.98361444 0.99036145 0.9780723  0.99108434 0.98626506
 0.9775904  0.98024094 0.9778313  0.9889157  0.9959036 ]
2022-01-11 22:05:42,209 - INFO - Epoch time: 401.0657844543457
2022-01-11 22:05:42,209 - INFO - 
Epoch: 72
2022-01-11 22:05:42,209 - INFO - 
Learning Rate: 0.0010
2022-01-11 22:06:45,236 - INFO - [Step=60250]	Loss=0.5281	251.5 examples/second
2022-01-11 22:08:42,021 - INFO - [Step=60500]	Loss=0.5268	274.0 examples/second
2022-01-11 22:10:39,111 - INFO - [Step=60750]	Loss=0.5212	273.3 examples/second
2022-01-11 22:12:23,075 - INFO - Test Loss=0.8958, Test top-1 acc=0.7798
2022-01-11 22:12:23,075 - INFO - Group Accuracy:

2022-01-11 22:12:23,075 - INFO - [0.97831327 0.99156624 0.9855422  0.993253   0.9850602  0.99373496
 0.98313254 0.9826506  0.99060243 0.9780723  0.9913253  0.9860241
 0.9768675  0.9807229  0.97638553 0.9891566  0.9966265 ]
2022-01-11 22:12:23,076 - INFO - Epoch time: 400.86705565452576
2022-01-11 22:12:23,076 - INFO - 
Epoch: 73
2022-01-11 22:12:23,076 - INFO - 
Learning Rate: 0.0010
2022-01-11 22:12:46,537 - INFO - [Step=61000]	Loss=0.5240	251.1 examples/second
2022-01-11 22:14:43,473 - INFO - [Step=61250]	Loss=0.5162	273.7 examples/second
2022-01-11 22:16:40,718 - INFO - [Step=61500]	Loss=0.5227	272.9 examples/second
2022-01-11 22:18:37,947 - INFO - [Step=61750]	Loss=0.5322	273.0 examples/second
2022-01-11 22:19:04,754 - INFO - Test Loss=0.9329, Test top-1 acc=0.7798
2022-01-11 22:19:04,754 - INFO - Group Accuracy:

2022-01-11 22:19:04,754 - INFO - [0.97927713 0.99084336 0.9853012  0.9930121  0.9855422  0.9939759
 0.98240966 0.98289156 0.99060243 0.9780723  0.9920482  0.9860241
 0.9780723  0.98       0.9775904  0.9884337  0.9961446 ]
2022-01-11 22:19:04,755 - INFO - Epoch time: 401.6795208454132
2022-01-11 22:19:04,756 - INFO - 
Epoch: 74
2022-01-11 22:19:04,756 - INFO - 
Learning Rate: 0.0010
2022-01-11 22:20:45,106 - INFO - [Step=62000]	Loss=0.5162	251.7 examples/second
2022-01-11 22:22:41,935 - INFO - [Step=62250]	Loss=0.5219	273.9 examples/second
2022-01-11 22:24:38,862 - INFO - [Step=62500]	Loss=0.5114	273.7 examples/second
2022-01-11 22:25:45,397 - INFO - Test Loss=0.9478, Test top-1 acc=0.7786
2022-01-11 22:25:45,398 - INFO - Group Accuracy:

2022-01-11 22:25:45,398 - INFO - [0.97903615 0.99108434 0.98578316 0.9927711  0.9855422  0.993494
 0.9821687  0.98240966 0.9913253  0.9780723  0.99108434 0.98626506
 0.9768675  0.97951806 0.9768675  0.9891566  0.9961446 ]
2022-01-11 22:25:45,399 - INFO - Epoch time: 400.64312648773193
2022-01-11 22:25:45,399 - INFO - 
Epoch: 75
2022-01-11 22:25:45,399 - INFO - 
Learning Rate: 0.0010
2022-01-11 22:26:45,982 - INFO - [Step=62750]	Loss=0.5167	251.7 examples/second
2022-01-11 22:28:42,876 - INFO - [Step=63000]	Loss=0.5117	273.8 examples/second
2022-01-11 22:30:40,032 - INFO - [Step=63250]	Loss=0.5143	273.1 examples/second
2022-01-11 22:32:26,105 - INFO - Test Loss=0.9080, Test top-1 acc=0.7802
2022-01-11 22:32:26,105 - INFO - Group Accuracy:

2022-01-11 22:32:26,105 - INFO - [0.97927713 0.99036145 0.9855422  0.99156624 0.9848193  0.99373496
 0.9821687  0.98289156 0.9898795  0.9780723  0.9913253  0.98578316
 0.9785542  0.97927713 0.9771084  0.98963857 0.9963855 ]
2022-01-11 22:32:26,106 - INFO - Epoch time: 400.7076027393341
2022-01-11 22:32:26,106 - INFO - 
Epoch: 76
2022-01-11 22:32:26,107 - INFO - 
Learning Rate: 0.0010
2022-01-11 22:32:46,994 - INFO - [Step=63500]	Loss=0.5215	252.0 examples/second
2022-01-11 22:34:44,042 - INFO - [Step=63750]	Loss=0.5046	273.4 examples/second
2022-01-11 22:36:41,075 - INFO - [Step=64000]	Loss=0.5125	273.4 examples/second
2022-01-11 22:38:38,368 - INFO - [Step=64250]	Loss=0.5151	272.8 examples/second
2022-01-11 22:39:07,526 - INFO - Test Loss=0.9237, Test top-1 acc=0.7814
2022-01-11 22:39:07,527 - INFO - Group Accuracy:

2022-01-11 22:39:07,527 - INFO - [0.97927713 0.99108434 0.9850602  0.9918072  0.98626506 0.993494
 0.9821687  0.98313254 0.99084336 0.9785542  0.99060243 0.98650604
 0.9775904  0.98024094 0.9771084  0.9886747  0.9966265 ]
2022-01-11 22:39:07,528 - INFO - Saving...
2022-01-11 22:39:07,763 - INFO - Epoch time: 401.65648770332336
2022-01-11 22:39:07,763 - INFO - 
Epoch: 77
2022-01-11 22:39:07,763 - INFO - 
Learning Rate: 0.0010
2022-01-11 22:40:45,960 - INFO - [Step=64500]	Loss=0.5057	250.8 examples/second
2022-01-11 22:42:42,765 - INFO - [Step=64750]	Loss=0.5028	274.0 examples/second
2022-01-11 22:44:39,848 - INFO - [Step=65000]	Loss=0.5228	273.3 examples/second
2022-01-11 22:45:48,650 - INFO - Test Loss=0.9297, Test top-1 acc=0.7783
2022-01-11 22:45:48,650 - INFO - Group Accuracy:

2022-01-11 22:45:48,650 - INFO - [0.97927713 0.99060243 0.9853012  0.9930121  0.98626506 0.993494
 0.9814458  0.98313254 0.9913253  0.97831327 0.99060243 0.9853012
 0.9775904  0.97975904 0.9773494  0.9891566  0.9966265 ]
2022-01-11 22:45:48,651 - INFO - Epoch time: 400.8881165981293
2022-01-11 22:45:48,651 - INFO - 
Epoch: 78
2022-01-11 22:45:48,651 - INFO - 
Learning Rate: 0.0010
2022-01-11 22:46:46,762 - INFO - [Step=65250]	Loss=0.5134	252.1 examples/second
2022-01-11 22:48:43,724 - INFO - [Step=65500]	Loss=0.5193	273.6 examples/second
2022-01-11 22:50:40,604 - INFO - [Step=65750]	Loss=0.5107	273.8 examples/second
2022-01-11 22:52:29,375 - INFO - Test Loss=0.9010, Test top-1 acc=0.7807
2022-01-11 22:52:29,375 - INFO - Group Accuracy:

2022-01-11 22:52:29,375 - INFO - [0.9787952  0.99060243 0.9860241  0.9918072  0.98433733 0.993253
 0.98240966 0.9821687  0.99060243 0.9780723  0.9913253  0.98578316
 0.9778313  0.98       0.9773494  0.9893976  0.9963855 ]
2022-01-11 22:52:29,376 - INFO - Epoch time: 400.7248589992523
2022-01-11 22:52:29,376 - INFO - 
Epoch: 79
2022-01-11 22:52:29,376 - INFO - 
Learning Rate: 0.0010
2022-01-11 22:52:47,965 - INFO - [Step=66000]	Loss=0.5204	251.3 examples/second
2022-01-11 22:54:44,585 - INFO - [Step=66250]	Loss=0.5122	274.4 examples/second
2022-01-11 22:56:41,356 - INFO - [Step=66500]	Loss=0.5038	274.0 examples/second
2022-01-11 22:58:38,129 - INFO - [Step=66750]	Loss=0.5168	274.0 examples/second
2022-01-11 22:59:09,286 - INFO - Test Loss=0.9009, Test top-1 acc=0.7802
2022-01-11 22:59:09,286 - INFO - Group Accuracy:

2022-01-11 22:59:09,286 - INFO - [0.9804819  0.9901205  0.98722893 0.9918072  0.9853012  0.99373496
 0.9821687  0.98289156 0.99084336 0.9780723  0.99060243 0.98626506
 0.9773494  0.97975904 0.9778313  0.98963857 0.9966265 ]
2022-01-11 22:59:09,288 - INFO - Epoch time: 399.9112448692322
2022-01-11 22:59:09,288 - INFO - 
Epoch: 80
2022-01-11 22:59:09,288 - INFO - 
Learning Rate: 0.0010
2022-01-11 23:00:44,396 - INFO - [Step=67000]	Loss=0.5044	253.4 examples/second
2022-01-11 23:02:40,909 - INFO - [Step=67250]	Loss=0.5089	274.6 examples/second
2022-01-11 23:04:37,577 - INFO - [Step=67500]	Loss=0.5055	274.3 examples/second
2022-01-11 23:05:48,231 - INFO - Test Loss=1.0834, Test top-1 acc=0.7802
2022-01-11 23:05:48,231 - INFO - Group Accuracy:

2022-01-11 23:05:48,231 - INFO - [0.97927713 0.9898795  0.9853012  0.9927711  0.9845783  0.99373496
 0.98240966 0.9833735  0.98963857 0.9775904  0.99108434 0.9848193
 0.9768675  0.97903615 0.9768675  0.9893976  0.9963855 ]
2022-01-11 23:05:48,232 - INFO - Epoch time: 398.94420289993286
2022-01-11 23:05:48,232 - INFO - 
Epoch: 81
2022-01-11 23:05:48,232 - INFO - 
Learning Rate: 0.0010
2022-01-11 23:06:44,404 - INFO - [Step=67750]	Loss=0.5003	252.3 examples/second
2022-01-11 23:08:40,768 - INFO - [Step=68000]	Loss=0.5030	275.0 examples/second
2022-01-11 23:10:37,307 - INFO - [Step=68250]	Loss=0.5062	274.6 examples/second
2022-01-11 23:12:28,374 - INFO - Test Loss=0.9211, Test top-1 acc=0.7807
2022-01-11 23:12:28,375 - INFO - Group Accuracy:

2022-01-11 23:12:28,375 - INFO - [0.97975904 0.9898795  0.98578316 0.9925301  0.9850602  0.99373496
 0.98240966 0.9819277  0.9913253  0.97903615 0.9918072  0.9860241
 0.9768675  0.97975904 0.9778313  0.9893976  0.9968675 ]
2022-01-11 23:12:28,375 - INFO - Epoch time: 400.1434314250946
2022-01-11 23:12:28,375 - INFO - 
Epoch: 82
2022-01-11 23:12:28,375 - INFO - 
Learning Rate: 0.0010
2022-01-11 23:12:44,893 - INFO - [Step=68500]	Loss=0.5065	250.8 examples/second
2022-01-11 23:14:41,734 - INFO - [Step=68750]	Loss=0.4931	273.9 examples/second
2022-01-11 23:16:38,437 - INFO - [Step=69000]	Loss=0.4985	274.2 examples/second
2022-01-11 23:18:35,347 - INFO - [Step=69250]	Loss=0.5078	273.7 examples/second
2022-01-11 23:19:08,687 - INFO - Test Loss=0.9832, Test top-1 acc=0.7817
2022-01-11 23:19:08,687 - INFO - Group Accuracy:

2022-01-11 23:19:08,687 - INFO - [0.98       0.99084336 0.9860241  0.9920482  0.9845783  0.9939759
 0.9816868  0.98289156 0.99060243 0.9780723  0.9918072  0.9850602
 0.9773494  0.98       0.9773494  0.9891566  0.9961446 ]
2022-01-11 23:19:08,688 - INFO - Saving...
2022-01-11 23:19:08,921 - INFO - Epoch time: 400.54549622535706
2022-01-11 23:19:08,921 - INFO - 
Epoch: 83
2022-01-11 23:19:08,921 - INFO - 
Learning Rate: 0.0010
2022-01-11 23:20:42,024 - INFO - [Step=69500]	Loss=0.5003	252.6 examples/second
2022-01-11 23:22:38,838 - INFO - [Step=69750]	Loss=0.5006	273.9 examples/second
2022-01-11 23:24:35,951 - INFO - [Step=70000]	Loss=0.5096	273.2 examples/second
2022-01-11 23:25:49,419 - INFO - Test Loss=0.9944, Test top-1 acc=0.7800
2022-01-11 23:25:49,419 - INFO - Group Accuracy:

2022-01-11 23:25:49,419 - INFO - [0.9787952  0.9901205  0.98626506 0.993494   0.9845783  0.9930121
 0.98313254 0.9826506  0.99036145 0.9778313  0.99036145 0.9855422
 0.97590363 0.98024094 0.9775904  0.98963857 0.9966265 ]
2022-01-11 23:25:49,420 - INFO - Epoch time: 400.49900913238525
2022-01-11 23:25:49,420 - INFO - 
Epoch: 84
2022-01-11 23:25:49,420 - INFO - 
Learning Rate: 0.0010
2022-01-11 23:26:42,876 - INFO - [Step=70250]	Loss=0.5037	252.1 examples/second
2022-01-11 23:28:39,394 - INFO - [Step=70500]	Loss=0.4868	274.6 examples/second
2022-01-11 23:30:35,858 - INFO - [Step=70750]	Loss=0.4908	274.8 examples/second
2022-01-11 23:32:28,330 - INFO - Test Loss=0.8759, Test top-1 acc=0.7843
2022-01-11 23:32:28,330 - INFO - Group Accuracy:

2022-01-11 23:32:28,330 - INFO - [0.97927713 0.99084336 0.98674697 0.9930121  0.9855422  0.99373496
 0.9821687  0.98313254 0.99108434 0.9778313  0.99156624 0.98578316
 0.9768675  0.97903615 0.9771084  0.9901205  0.9966265 ]
2022-01-11 23:32:28,331 - INFO - Saving...
2022-01-11 23:32:28,597 - INFO - Epoch time: 399.17622923851013
2022-01-11 23:32:28,597 - INFO - 
Epoch: 85
2022-01-11 23:32:28,597 - INFO - 
Learning Rate: 0.0010
2022-01-11 23:32:42,200 - INFO - [Step=71000]	Loss=0.4950	253.3 examples/second
2022-01-11 23:34:38,906 - INFO - [Step=71250]	Loss=0.4883	274.2 examples/second
2022-01-11 23:36:35,330 - INFO - [Step=71500]	Loss=0.4855	274.9 examples/second
2022-01-11 23:38:31,899 - INFO - [Step=71750]	Loss=0.4990	274.5 examples/second
2022-01-11 23:39:07,767 - INFO - Test Loss=0.9360, Test top-1 acc=0.7848
2022-01-11 23:39:07,768 - INFO - Group Accuracy:

2022-01-11 23:39:07,768 - INFO - [0.97903615 0.99084336 0.98626506 0.993494   0.9848193  0.99373496
 0.9821687  0.9840964  0.99084336 0.9787952  0.9913253  0.9855422
 0.9780723  0.97975904 0.97614455 0.98963857 0.9961446 ]
2022-01-11 23:39:07,768 - INFO - Saving...
2022-01-11 23:39:08,041 - INFO - Epoch time: 399.44412112236023
2022-01-11 23:39:08,041 - INFO - 
Epoch: 86
2022-01-11 23:39:08,041 - INFO - 
Learning Rate: 0.0010
2022-01-11 23:40:38,874 - INFO - [Step=72000]	Loss=0.4863	252.0 examples/second
2022-01-11 23:42:35,461 - INFO - [Step=72250]	Loss=0.4887	274.5 examples/second
2022-01-11 23:44:31,916 - INFO - [Step=72500]	Loss=0.4965	274.8 examples/second
2022-01-11 23:45:47,449 - INFO - Test Loss=0.9967, Test top-1 acc=0.7822
2022-01-11 23:45:47,449 - INFO - Group Accuracy:

2022-01-11 23:45:47,449 - INFO - [0.97927713 0.9898795  0.9853012  0.9925301  0.9848193  0.9939759
 0.9826506  0.9821687  0.99084336 0.9778313  0.99084336 0.98626506
 0.9773494  0.97951806 0.9773494  0.9893976  0.9966265 ]
2022-01-11 23:45:47,450 - INFO - Epoch time: 399.40880703926086
2022-01-11 23:45:47,450 - INFO - 
Epoch: 87
2022-01-11 23:45:47,450 - INFO - 
Learning Rate: 0.0010
2022-01-11 23:46:38,384 - INFO - [Step=72750]	Loss=0.4886	253.0 examples/second
2022-01-11 23:48:36,256 - INFO - [Step=73000]	Loss=0.5014	271.5 examples/second
2022-01-11 23:50:33,728 - INFO - [Step=73250]	Loss=0.4867	272.4 examples/second
2022-01-11 23:52:29,083 - INFO - Test Loss=0.9412, Test top-1 acc=0.7807
2022-01-11 23:52:29,083 - INFO - Group Accuracy:

2022-01-11 23:52:29,083 - INFO - [0.9804819  0.99108434 0.98698795 0.9927711  0.9848193  0.99373496
 0.98313254 0.9826506  0.99060243 0.9768675  0.99108434 0.9848193
 0.9778313  0.98024094 0.9773494  0.9891566  0.9959036 ]
2022-01-11 23:52:29,084 - INFO - Epoch time: 401.6339921951294
2022-01-11 23:52:29,084 - INFO - 
Epoch: 88
2022-01-11 23:52:29,084 - INFO - 
Learning Rate: 0.0010
2022-01-11 23:52:40,681 - INFO - [Step=73500]	Loss=0.4817	252.1 examples/second
2022-01-11 23:54:37,500 - INFO - [Step=73750]	Loss=0.4806	273.9 examples/second
2022-01-11 23:56:34,442 - INFO - [Step=74000]	Loss=0.4844	273.6 examples/second
2022-01-11 23:58:31,228 - INFO - [Step=74250]	Loss=0.4866	274.0 examples/second
2022-01-11 23:59:09,450 - INFO - Test Loss=0.9331, Test top-1 acc=0.7846
2022-01-11 23:59:09,451 - INFO - Group Accuracy:

2022-01-11 23:59:09,451 - INFO - [0.97951806 0.9913253  0.9860241  0.9930121  0.9850602  0.993494
 0.9826506  0.9840964  0.9913253  0.9773494  0.99156624 0.9845783
 0.9780723  0.98024094 0.9768675  0.98963857 0.9966265 ]
2022-01-11 23:59:09,451 - INFO - Epoch time: 400.3672387599945
2022-01-11 23:59:09,451 - INFO - 
Epoch: 89
2022-01-11 23:59:09,451 - INFO - 
Learning Rate: 0.0010
2022-01-12 00:00:37,714 - INFO - [Step=74500]	Loss=0.4804	253.0 examples/second
2022-01-12 00:02:34,382 - INFO - [Step=74750]	Loss=0.4804	274.3 examples/second
2022-01-12 00:04:31,272 - INFO - [Step=75000]	Loss=0.4908	273.8 examples/second
2022-01-12 00:05:49,221 - INFO - Test Loss=0.9733, Test top-1 acc=0.7783
2022-01-12 00:05:49,221 - INFO - Group Accuracy:

2022-01-12 00:05:49,221 - INFO - [0.97831327 0.99084336 0.9850602  0.9930121  0.98433733 0.9939759
 0.9840964  0.9833735  0.99084336 0.9771084  0.99084336 0.9838554
 0.9771084  0.97975904 0.9766265  0.9893976  0.9956626 ]
2022-01-12 00:05:49,222 - INFO - Epoch time: 399.7707533836365
2022-01-12 00:05:59,926 - INFO - Computing OOD Statistics...
2022-01-12 00:05:59,933 - INFO - 	Baseline.          AUROC: 0.3336. TNR@95TPR: 0.0259. AUPR OUT: 0.1226
2022-01-12 00:05:59,939 - INFO - 	ODIN (T=1000).     AUROC: 0.8715. TNR@95TPR: 0.4224. AUPR OUT: 0.5679
2022-01-12 00:05:59,939 - INFO - Printing Final Accuracy + OOD Detection stats
2022-01-12 00:05:59,940 - INFO - Top 1 Accuracy:  Min: 0.7848; Max: 0.7848; Avg: 0.7848; Std: 0.0000; Len: 1
2022-01-12 00:05:59,940 - INFO - Top 5 Accuracy:  Min: 0.9859; Max: 0.9859; Avg: 0.9859; Std: 0.0000; Len: 1
2022-01-12 00:05:59,940 - INFO - **********************************************************************
2022-01-12 00:05:59,940 - INFO - 	MSP (auroc): [0.333630616583983] Min: 0.3336; Max: 0.3336; Avg: 0.3336; Std: 0.0000; Len: 1
2022-01-12 00:05:59,940 - INFO - 	MSP (tnr): [0.025882352941176467] Min: 0.0259; Max: 0.0259; Avg: 0.0259; Std: 0.0000; Len: 1
2022-01-12 00:05:59,940 - INFO - 	MSP (aupr): [0.12256728504332128] Min: 0.1226; Max: 0.1226; Avg: 0.1226; Std: 0.0000; Len: 1
2022-01-12 00:05:59,940 - INFO - 	ODIN (auroc): [0.8715455705173636] Min: 0.8715; Max: 0.8715; Avg: 0.8715; Std: 0.0000; Len: 1
2022-01-12 00:05:59,940 - INFO - 	ODIN (tnr): [0.4223529411764706] Min: 0.4224; Max: 0.4224; Avg: 0.4224; Std: 0.0000; Len: 1
2022-01-12 00:05:59,940 - INFO - 	ODIN (aupr): [0.5679385447978289] Min: 0.5679; Max: 0.5679; Avg: 0.5679; Std: 0.0000; Len: 1
