2022-01-11 01:07:57,185 - INFO - ==> Preparing data..
2022-01-11 01:07:57,567 - INFO - checkpoint filename: experiments/coarse/mos/adam_LRp1_R0/checkpoint.pt
2022-01-11 01:07:57,567 - INFO - log filename: experiments/coarse/mos/adam_LRp1_R0/train.log
2022-01-11 01:07:57,567 - INFO - ********************************************************
2022-01-11 01:07:57,567 - INFO - Starting Iter: 0 / 1
2022-01-11 01:07:57,567 - INFO - ********************************************************
2022-01-11 01:08:00,937 - INFO - 
Epoch: 0
2022-01-11 01:08:00,937 - INFO - 
Learning Rate: 0.0100
2022-01-11 01:10:16,534 - INFO - [Step=250]	Loss=6.3039	236.0 examples/second
2022-01-11 01:12:30,128 - INFO - [Step=500]	Loss=5.2564	239.5 examples/second
2022-01-11 01:14:42,164 - INFO - [Step=750]	Loss=5.1263	242.4 examples/second
2022-01-11 01:15:35,757 - INFO - Test Loss=4.9916, Test top-1 acc=0.0533
2022-01-11 01:15:35,758 - INFO - Group Accuracy:

2022-01-11 01:15:35,758 - INFO - [0.939759  0.939759  0.939759  0.939759  0.939759  0.939759  0.9395181
 0.939759  0.939759  0.939759  0.9518072 0.939759  0.939759  0.939759
 0.939759  0.939759  0.9518072]
2022-01-11 01:15:35,759 - INFO - Saving...
2022-01-11 01:15:36,076 - INFO - Epoch time: 455.1387505531311
2022-01-11 01:15:36,076 - INFO - 
Epoch: 1
2022-01-11 01:15:36,076 - INFO - 
Learning Rate: 0.0280
2022-01-11 01:17:06,747 - INFO - [Step=1000]	Loss=5.0738	221.3 examples/second
2022-01-11 01:19:20,177 - INFO - [Step=1250]	Loss=4.8783	239.8 examples/second
2022-01-11 01:21:33,668 - INFO - [Step=1500]	Loss=4.6315	239.7 examples/second
2022-01-11 01:23:13,464 - INFO - Test Loss=4.4162, Test top-1 acc=0.1373
2022-01-11 01:23:13,464 - INFO - Group Accuracy:

2022-01-11 01:23:13,464 - INFO - [0.94       0.939759   0.93710846 0.9407229  0.93903613 0.9395181
 0.94       0.94096386 0.93301207 0.939759   0.9518072  0.939759
 0.939759   0.939759   0.939759   0.94       0.9518072 ]
2022-01-11 01:23:13,466 - INFO - Saving...
2022-01-11 01:23:13,741 - INFO - Epoch time: 457.66444516181946
2022-01-11 01:23:13,741 - INFO - 
Epoch: 2
2022-01-11 01:23:13,741 - INFO - 
Learning Rate: 0.0460
2022-01-11 01:23:58,321 - INFO - [Step=1750]	Loss=4.4769	221.2 examples/second
2022-01-11 01:26:11,048 - INFO - [Step=2000]	Loss=4.3886	241.1 examples/second
2022-01-11 01:28:23,517 - INFO - [Step=2250]	Loss=4.1712	241.6 examples/second
2022-01-11 01:30:35,328 - INFO - [Step=2500]	Loss=4.0217	242.8 examples/second
2022-01-11 01:30:46,477 - INFO - Test Loss=3.8582, Test top-1 acc=0.2313
2022-01-11 01:30:46,478 - INFO - Group Accuracy:

2022-01-11 01:30:46,478 - INFO - [0.94144577 0.9433735  0.9366265  0.95349395 0.94216865 0.9561446
 0.94168675 0.9448193  0.94144577 0.939759   0.95277107 0.94096386
 0.94096386 0.9392771  0.9395181  0.9404819  0.9561446 ]
2022-01-11 01:30:46,479 - INFO - Saving...
2022-01-11 01:30:46,755 - INFO - Epoch time: 453.01361203193665
2022-01-11 01:30:46,755 - INFO - 
Epoch: 3
2022-01-11 01:30:46,755 - INFO - 
Learning Rate: 0.0640
2022-01-11 01:32:59,071 - INFO - [Step=2750]	Loss=3.9761	222.6 examples/second
2022-01-11 01:35:12,733 - INFO - [Step=3000]	Loss=3.8730	239.4 examples/second
2022-01-11 01:37:24,999 - INFO - [Step=3250]	Loss=3.7800	241.9 examples/second
2022-01-11 01:38:20,923 - INFO - Test Loss=4.2536, Test top-1 acc=0.2111
2022-01-11 01:38:20,924 - INFO - Group Accuracy:

2022-01-11 01:38:20,924 - INFO - [0.94       0.94433737 0.93807226 0.9469879  0.9385542  0.939759
 0.9438554  0.9320482  0.94433737 0.94       0.9518072  0.939759
 0.9392771  0.9387952  0.9385542  0.94144577 0.9472289 ]
2022-01-11 01:38:20,925 - INFO - Epoch time: 454.16985392570496
2022-01-11 01:38:20,925 - INFO - 
Epoch: 4
2022-01-11 01:38:20,925 - INFO - 
Learning Rate: 0.1000
2022-01-11 01:39:48,052 - INFO - [Step=3500]	Loss=3.8237	223.7 examples/second
2022-01-11 01:42:01,481 - INFO - [Step=3750]	Loss=3.9139	239.8 examples/second
2022-01-11 01:44:14,123 - INFO - [Step=4000]	Loss=3.7996	241.3 examples/second
2022-01-11 01:45:55,773 - INFO - Test Loss=4.0843, Test top-1 acc=0.2205
2022-01-11 01:45:55,774 - INFO - Group Accuracy:

2022-01-11 01:45:55,774 - INFO - [0.9392771  0.93445784 0.9387952  0.9472289  0.939759   0.9554217
 0.939759   0.94192773 0.94240963 0.9395181  0.9520482  0.940241
 0.9392771  0.9407229  0.939759   0.93783134 0.96457833]
2022-01-11 01:45:55,775 - INFO - Epoch time: 454.84999442100525
2022-01-11 01:45:55,775 - INFO - 
Epoch: 5
2022-01-11 01:45:55,775 - INFO - 
Learning Rate: 0.1000
2022-01-11 01:46:36,852 - INFO - [Step=4250]	Loss=3.7061	224.2 examples/second
2022-01-11 01:48:49,822 - INFO - [Step=4500]	Loss=3.6047	240.7 examples/second
2022-01-11 01:51:02,446 - INFO - [Step=4750]	Loss=3.5320	241.3 examples/second
2022-01-11 01:53:14,439 - INFO - [Step=5000]	Loss=3.5297	242.4 examples/second
2022-01-11 01:53:27,977 - INFO - Test Loss=3.4582, Test top-1 acc=0.2937
2022-01-11 01:53:27,977 - INFO - Group Accuracy:

2022-01-11 01:53:27,977 - INFO - [0.9460241  0.94939756 0.94096386 0.9559036  0.9209639  0.9633735
 0.9433735  0.94843376 0.9472289  0.94289154 0.9559036  0.9407229
 0.939759   0.9426506  0.9387952  0.94891566 0.96891564]
2022-01-11 01:53:27,978 - INFO - Saving...
2022-01-11 01:53:28,214 - INFO - Epoch time: 452.4394745826721
2022-01-11 01:53:28,214 - INFO - 
Epoch: 6
2022-01-11 01:53:28,215 - INFO - 
Learning Rate: 0.1000
2022-01-11 01:55:38,574 - INFO - [Step=5250]	Loss=3.4741	222.0 examples/second
2022-01-11 01:57:50,873 - INFO - [Step=5500]	Loss=3.4111	241.9 examples/second
2022-01-11 02:00:03,185 - INFO - [Step=5750]	Loss=3.3814	241.9 examples/second
2022-01-11 02:01:02,201 - INFO - Test Loss=3.3681, Test top-1 acc=0.3224
2022-01-11 02:01:02,202 - INFO - Group Accuracy:

2022-01-11 02:01:02,202 - INFO - [0.946506   0.92650604 0.9438554  0.95228916 0.9431325  0.9657831
 0.94216865 0.9515663  0.95686746 0.9436145  0.9544578  0.94433737
 0.94120485 0.94240963 0.9438554  0.95253015 0.97204816]
2022-01-11 02:01:02,203 - INFO - Saving...
2022-01-11 02:01:02,444 - INFO - Epoch time: 454.2293815612793
2022-01-11 02:01:02,444 - INFO - 
Epoch: 7
2022-01-11 02:01:02,444 - INFO - 
Learning Rate: 0.1000
2022-01-11 02:02:28,273 - INFO - [Step=6000]	Loss=3.3112	220.6 examples/second
2022-01-11 02:04:40,426 - INFO - [Step=6250]	Loss=3.2760	242.1 examples/second
2022-01-11 02:06:52,930 - INFO - [Step=6500]	Loss=3.2530	241.5 examples/second
2022-01-11 02:08:37,881 - INFO - Test Loss=3.1436, Test top-1 acc=0.3624
2022-01-11 02:08:37,888 - INFO - Group Accuracy:

2022-01-11 02:08:37,888 - INFO - [0.94626504 0.95277107 0.93012047 0.96072286 0.9433735  0.9674699
 0.9426506  0.9518072  0.9612048  0.94626504 0.9561446  0.94891566
 0.94       0.94289154 0.9431325  0.9520482  0.9706024 ]
2022-01-11 02:08:37,889 - INFO - Saving...
2022-01-11 02:08:38,146 - INFO - Epoch time: 455.7013976573944
2022-01-11 02:08:38,146 - INFO - 
Epoch: 8
2022-01-11 02:08:38,146 - INFO - 
Learning Rate: 0.1000
2022-01-11 02:09:17,337 - INFO - [Step=6750]	Loss=3.2064	221.6 examples/second
2022-01-11 02:11:29,784 - INFO - [Step=7000]	Loss=3.1847	241.6 examples/second
2022-01-11 02:13:42,807 - INFO - [Step=7250]	Loss=3.1709	240.6 examples/second
2022-01-11 02:15:55,518 - INFO - [Step=7500]	Loss=3.1487	241.1 examples/second
2022-01-11 02:16:11,937 - INFO - Test Loss=2.9852, Test top-1 acc=0.3841
2022-01-11 02:16:11,937 - INFO - Group Accuracy:

2022-01-11 02:16:11,937 - INFO - [0.9513253  0.95325303 0.9448193  0.9621687  0.939759   0.97301203
 0.9436145  0.95421684 0.96457833 0.9479518  0.9549398  0.9498795
 0.94144577 0.94554216 0.9436145  0.95253015 0.9706024 ]
2022-01-11 02:16:11,938 - INFO - Saving...
2022-01-11 02:16:12,192 - INFO - Epoch time: 454.04610657691956
2022-01-11 02:16:12,192 - INFO - 
Epoch: 9
2022-01-11 02:16:12,192 - INFO - 
Learning Rate: 0.1000
2022-01-11 02:18:18,604 - INFO - [Step=7750]	Loss=3.0814	223.6 examples/second
2022-01-11 02:20:31,193 - INFO - [Step=8000]	Loss=3.0726	241.3 examples/second
2022-01-11 02:22:45,126 - INFO - [Step=8250]	Loss=3.0761	238.9 examples/second
2022-01-11 02:23:46,572 - INFO - Test Loss=3.0450, Test top-1 acc=0.3810
2022-01-11 02:23:46,572 - INFO - Group Accuracy:

2022-01-11 02:23:46,573 - INFO - [0.94843376 0.95759034 0.95228916 0.9686747  0.94626504 0.9701205
 0.9431325  0.95325303 0.95277107 0.94506025 0.9621687  0.94891566
 0.94216865 0.93831325 0.9426506  0.95325303 0.96457833]
2022-01-11 02:23:46,573 - INFO - Epoch time: 454.3811421394348
2022-01-11 02:23:46,573 - INFO - 
Epoch: 10
2022-01-11 02:23:46,573 - INFO - 
Learning Rate: 0.1000
2022-01-11 02:25:08,469 - INFO - [Step=8500]	Loss=2.9783	223.2 examples/second
2022-01-11 02:27:21,495 - INFO - [Step=8750]	Loss=3.0037	240.6 examples/second
2022-01-11 02:29:35,969 - INFO - [Step=9000]	Loss=2.9618	238.0 examples/second
2022-01-11 02:31:22,719 - INFO - Test Loss=2.9858, Test top-1 acc=0.3863
2022-01-11 02:31:22,719 - INFO - Group Accuracy:

2022-01-11 02:31:22,719 - INFO - [0.9510843  0.9573494  0.9498795  0.966747   0.9472289  0.9703615
 0.9477109  0.95373493 0.95975906 0.9325301  0.9515663  0.9513253
 0.94120485 0.9436145  0.9477109  0.95277107 0.97493976]
2022-01-11 02:31:22,720 - INFO - Saving...
2022-01-11 02:31:22,945 - INFO - Epoch time: 456.37199783325195
2022-01-11 02:31:22,946 - INFO - 
Epoch: 11
2022-01-11 02:31:22,946 - INFO - 
Learning Rate: 0.1000
2022-01-11 02:31:59,825 - INFO - [Step=9250]	Loss=2.9427	222.4 examples/second
2022-01-11 02:34:13,101 - INFO - [Step=9500]	Loss=2.9212	240.1 examples/second
2022-01-11 02:36:26,592 - INFO - [Step=9750]	Loss=2.8928	239.7 examples/second
2022-01-11 02:38:38,463 - INFO - [Step=10000]	Loss=2.8767	242.7 examples/second
2022-01-11 02:38:57,524 - INFO - Test Loss=2.7494, Test top-1 acc=0.4060
2022-01-11 02:38:57,524 - INFO - Group Accuracy:

2022-01-11 02:38:57,524 - INFO - [0.9518072  0.9580723  0.94891566 0.9662651  0.95301205 0.97204816
 0.94168675 0.9546988  0.9578313  0.94433737 0.96168673 0.95228916
 0.94120485 0.94289154 0.94963855 0.9580723  0.98024094]
2022-01-11 02:38:57,526 - INFO - Saving...
2022-01-11 02:38:57,789 - INFO - Epoch time: 454.84313702583313
2022-01-11 02:38:57,789 - INFO - 
Epoch: 12
2022-01-11 02:38:57,789 - INFO - 
Learning Rate: 0.1000
2022-01-11 02:41:02,733 - INFO - [Step=10250]	Loss=2.8239	221.8 examples/second
2022-01-11 02:43:15,817 - INFO - [Step=10500]	Loss=2.8252	240.5 examples/second
2022-01-11 02:45:28,416 - INFO - [Step=10750]	Loss=2.8254	241.3 examples/second
2022-01-11 02:46:32,299 - INFO - Test Loss=3.0683, Test top-1 acc=0.3865
2022-01-11 02:46:32,299 - INFO - Group Accuracy:

2022-01-11 02:46:32,299 - INFO - [0.9433735  0.9453012  0.9445783  0.9672289  0.9479518  0.97638553
 0.9481928  0.9469879  0.94554216 0.94891566 0.9628916  0.9477109
 0.93036145 0.94554216 0.94289154 0.9506024  0.97542167]
2022-01-11 02:46:32,300 - INFO - Epoch time: 454.5109691619873
2022-01-11 02:46:32,300 - INFO - 
Epoch: 13
2022-01-11 02:46:32,300 - INFO - 
Learning Rate: 0.1000
2022-01-11 02:47:51,834 - INFO - [Step=11000]	Loss=2.7857	223.1 examples/second
2022-01-11 02:50:04,030 - INFO - [Step=11250]	Loss=2.7732	242.1 examples/second
2022-01-11 02:52:16,503 - INFO - [Step=11500]	Loss=8.1773	241.6 examples/second
2022-01-11 02:54:04,505 - INFO - Test Loss=5.3670, Test top-1 acc=0.0193
2022-01-11 02:54:04,506 - INFO - Group Accuracy:

2022-01-11 02:54:04,506 - INFO - [0.939759  0.939759  0.939759  0.939759  0.939759  0.9395181 0.939759
 0.939759  0.9395181 0.939759  0.9518072 0.939759  0.939759  0.9395181
 0.9395181 0.939759  0.9515663]
2022-01-11 02:54:04,507 - INFO - Epoch time: 452.2067975997925
2022-01-11 02:54:04,507 - INFO - 
Epoch: 14
2022-01-11 02:54:04,507 - INFO - 
Learning Rate: 0.1000
2022-01-11 02:54:39,163 - INFO - [Step=11750]	Loss=5.3823	224.3 examples/second
2022-01-11 02:56:51,611 - INFO - [Step=12000]	Loss=5.3221	241.6 examples/second
2022-01-11 02:59:03,636 - INFO - [Step=12250]	Loss=5.2840	242.4 examples/second
2022-01-11 03:01:15,680 - INFO - [Step=12500]	Loss=5.2094	242.3 examples/second
2022-01-11 03:01:37,965 - INFO - Test Loss=5.1164, Test top-1 acc=0.0289
2022-01-11 03:01:37,965 - INFO - Group Accuracy:

2022-01-11 03:01:37,972 - INFO - [0.939759  0.939759  0.9387952 0.939759  0.939759  0.940241  0.939759
 0.939759  0.939759  0.939759  0.9518072 0.939759  0.939759  0.939759
 0.939759  0.939759  0.9518072]
2022-01-11 03:01:37,974 - INFO - Epoch time: 453.46715784072876
2022-01-11 03:01:37,974 - INFO - 
Epoch: 15
2022-01-11 03:01:37,974 - INFO - 
Learning Rate: 0.1000
2022-01-11 03:03:40,849 - INFO - [Step=12750]	Loss=5.1234	220.4 examples/second
2022-01-11 03:05:52,906 - INFO - [Step=13000]	Loss=5.0730	242.3 examples/second
2022-01-11 03:08:05,441 - INFO - [Step=13250]	Loss=4.9998	241.4 examples/second
2022-01-11 03:09:13,252 - INFO - Test Loss=4.8753, Test top-1 acc=0.0342
2022-01-11 03:09:13,252 - INFO - Group Accuracy:

2022-01-11 03:09:13,252 - INFO - [0.939759   0.9395181  0.93807226 0.9385542  0.939759   0.9404819
 0.9387952  0.939759   0.939759   0.939759   0.9518072  0.939759
 0.939759   0.939759   0.939759   0.939759   0.9518072 ]
2022-01-11 03:09:13,253 - INFO - Epoch time: 455.2788288593292
2022-01-11 03:09:13,253 - INFO - 
Epoch: 16
2022-01-11 03:09:13,253 - INFO - 
Learning Rate: 0.1000
2022-01-11 03:10:29,045 - INFO - [Step=13500]	Loss=4.9399	222.8 examples/second
2022-01-11 03:12:40,694 - INFO - [Step=13750]	Loss=4.8906	243.1 examples/second
2022-01-11 03:14:53,439 - INFO - [Step=14000]	Loss=4.8588	241.1 examples/second
2022-01-11 03:16:44,560 - INFO - Test Loss=4.7800, Test top-1 acc=0.0451
2022-01-11 03:16:44,560 - INFO - Group Accuracy:

2022-01-11 03:16:44,561 - INFO - [0.939759   0.939759   0.94       0.93903613 0.939759   0.94120485
 0.94       0.939759   0.939759   0.939759   0.9518072  0.939759
 0.939759   0.939759   0.939759   0.939759   0.9518072 ]
2022-01-11 03:16:44,562 - INFO - Epoch time: 451.30844712257385
2022-01-11 03:16:44,562 - INFO - 
Epoch: 17
2022-01-11 03:16:44,562 - INFO - 
Learning Rate: 0.1000
2022-01-11 03:17:15,632 - INFO - [Step=14250]	Loss=4.8250	225.0 examples/second
2022-01-11 03:19:27,329 - INFO - [Step=14500]	Loss=4.8114	243.0 examples/second
2022-01-11 03:21:39,887 - INFO - [Step=14750]	Loss=4.8100	241.4 examples/second
2022-01-11 03:23:52,063 - INFO - [Step=15000]	Loss=4.7854	242.1 examples/second
2022-01-11 03:24:16,512 - INFO - Test Loss=4.7793, Test top-1 acc=0.0511
2022-01-11 03:24:16,513 - INFO - Group Accuracy:

2022-01-11 03:24:16,513 - INFO - [0.939759   0.939759   0.939759   0.93783134 0.939759   0.94240963
 0.93301207 0.939759   0.939759   0.939759   0.9518072  0.939759
 0.939759   0.939759   0.939759   0.939759   0.9518072 ]
2022-01-11 03:24:16,514 - INFO - Epoch time: 451.9524300098419
2022-01-11 03:24:16,514 - INFO - 
Epoch: 18
2022-01-11 03:24:16,514 - INFO - 
Learning Rate: 0.1000
2022-01-11 03:26:15,724 - INFO - [Step=15250]	Loss=4.7782	222.7 examples/second
2022-01-11 03:28:28,500 - INFO - [Step=15500]	Loss=4.7481	241.0 examples/second
2022-01-11 03:30:41,405 - INFO - [Step=15750]	Loss=4.7390	240.8 examples/second
2022-01-11 03:31:50,753 - INFO - Test Loss=4.6673, Test top-1 acc=0.0600
2022-01-11 03:31:50,754 - INFO - Group Accuracy:

2022-01-11 03:31:50,754 - INFO - [0.94       0.93903613 0.939759   0.9448193  0.939759   0.9431325
 0.94096386 0.939759   0.939759   0.939759   0.9518072  0.939759
 0.939759   0.939759   0.9392771  0.9395181  0.9518072 ]
2022-01-11 03:31:50,755 - INFO - Epoch time: 454.24114441871643
2022-01-11 03:31:50,755 - INFO - 
Epoch: 19
2022-01-11 03:31:50,756 - INFO - 
Learning Rate: 0.1000
2022-01-11 03:33:03,943 - INFO - [Step=16000]	Loss=4.7239	224.5 examples/second
2022-01-11 03:35:16,969 - INFO - [Step=16250]	Loss=4.6915	240.6 examples/second
2022-01-11 03:37:29,733 - INFO - [Step=16500]	Loss=4.6867	241.0 examples/second
2022-01-11 03:39:24,032 - INFO - Test Loss=4.7047, Test top-1 acc=0.0414
2022-01-11 03:39:24,033 - INFO - Group Accuracy:

2022-01-11 03:39:24,033 - INFO - [0.94       0.939759   0.93710846 0.94144577 0.939759   0.94289154
 0.93759036 0.939759   0.94216865 0.9373494  0.94120485 0.939759
 0.9395181  0.93060243 0.939759   0.9253012  0.9518072 ]
2022-01-11 03:39:24,034 - INFO - Epoch time: 453.2781455516815
2022-01-11 03:39:24,034 - INFO - 
Epoch: 20
2022-01-11 03:39:24,034 - INFO - 
Learning Rate: 0.1000
2022-01-11 03:39:52,673 - INFO - [Step=16750]	Loss=4.6921	223.9 examples/second
2022-01-11 03:42:06,175 - INFO - [Step=17000]	Loss=4.6457	239.7 examples/second
2022-01-11 03:44:18,881 - INFO - [Step=17250]	Loss=4.6043	241.1 examples/second
2022-01-11 03:46:30,958 - INFO - [Step=17500]	Loss=4.5557	242.3 examples/second
2022-01-11 03:46:57,974 - INFO - Test Loss=4.4354, Test top-1 acc=0.0911
2022-01-11 03:46:57,974 - INFO - Group Accuracy:

2022-01-11 03:46:57,975 - INFO - [0.939759   0.94120485 0.9359036  0.94506025 0.939759   0.93903613
 0.940241   0.939759   0.9469879  0.939759   0.9518072  0.939759
 0.939759   0.939759   0.939759   0.93903613 0.9518072 ]
2022-01-11 03:46:57,975 - INFO - Epoch time: 453.94182872772217
2022-01-11 03:46:57,976 - INFO - 
Epoch: 21
2022-01-11 03:46:57,976 - INFO - 
Learning Rate: 0.1000
2022-01-11 03:48:55,092 - INFO - [Step=17750]	Loss=4.5030	222.0 examples/second
2022-01-11 03:51:07,455 - INFO - [Step=18000]	Loss=4.4532	241.8 examples/second
2022-01-11 03:53:19,717 - INFO - [Step=18250]	Loss=4.3982	241.9 examples/second
2022-01-11 03:54:32,216 - INFO - Test Loss=4.1509, Test top-1 acc=0.1046
2022-01-11 03:54:32,216 - INFO - Group Accuracy:

2022-01-11 03:54:32,217 - INFO - [0.939759   0.94144577 0.93903613 0.94554216 0.939759   0.95373493
 0.94168675 0.9433735  0.95228916 0.9395181  0.9518072  0.94
 0.939759   0.9395181  0.939759   0.939759   0.9518072 ]
2022-01-11 03:54:32,226 - INFO - Epoch time: 454.25050139427185
2022-01-11 03:54:32,226 - INFO - 
Epoch: 22
2022-01-11 03:54:32,226 - INFO - 
Learning Rate: 0.1000
2022-01-11 03:55:43,374 - INFO - [Step=18500]	Loss=4.3462	222.8 examples/second
2022-01-11 03:57:56,000 - INFO - [Step=18750]	Loss=4.2836	241.3 examples/second
2022-01-11 04:00:08,454 - INFO - [Step=19000]	Loss=4.2409	241.6 examples/second
2022-01-11 04:02:06,054 - INFO - Test Loss=3.9946, Test top-1 acc=0.1149
2022-01-11 04:02:06,054 - INFO - Group Accuracy:

2022-01-11 04:02:06,054 - INFO - [0.939759   0.9404819  0.94168675 0.95036143 0.939759   0.9583132
 0.9407229  0.9448193  0.95277107 0.939759   0.9520482  0.939759
 0.939759   0.9404819  0.9404819  0.9395181  0.9546988 ]
2022-01-11 04:02:06,055 - INFO - Epoch time: 453.82913422584534
2022-01-11 04:02:06,055 - INFO - 
Epoch: 23
2022-01-11 04:02:06,055 - INFO - 
Learning Rate: 0.1000
2022-01-11 04:02:32,090 - INFO - [Step=19250]	Loss=4.2181	222.8 examples/second
2022-01-11 04:04:44,843 - INFO - [Step=19500]	Loss=4.1777	241.1 examples/second
2022-01-11 04:06:56,506 - INFO - [Step=19750]	Loss=4.1533	243.0 examples/second
2022-01-11 04:09:09,562 - INFO - [Step=20000]	Loss=4.1061	240.5 examples/second
2022-01-11 04:09:39,120 - INFO - Test Loss=3.8838, Test top-1 acc=0.1340
2022-01-11 04:09:39,121 - INFO - Group Accuracy:

2022-01-11 04:09:39,121 - INFO - [0.939759   0.9433735  0.9436145  0.95421684 0.939759   0.9580723
 0.94216865 0.94554216 0.9561446  0.93831325 0.9520482  0.94
 0.939759   0.9392771  0.94168675 0.9433735  0.9554217 ]
2022-01-11 04:09:39,122 - INFO - Epoch time: 453.0662317276001
2022-01-11 04:09:39,122 - INFO - 
Epoch: 24
2022-01-11 04:09:39,122 - INFO - 
Learning Rate: 0.1000
2022-01-11 04:11:33,520 - INFO - [Step=20250]	Loss=4.0878	222.3 examples/second
2022-01-11 04:13:45,544 - INFO - [Step=20500]	Loss=4.0883	242.4 examples/second
2022-01-11 04:15:59,061 - INFO - [Step=20750]	Loss=4.0110	239.7 examples/second
2022-01-11 04:17:14,190 - INFO - Test Loss=3.8932, Test top-1 acc=0.1304
2022-01-11 04:17:14,190 - INFO - Group Accuracy:

2022-01-11 04:17:14,190 - INFO - [0.939759   0.9453012  0.94168675 0.9501205  0.939759   0.96072286
 0.94120485 0.94433737 0.9561446  0.940241   0.9518072  0.939759
 0.939759   0.94144577 0.94289154 0.9426506  0.9554217 ]
2022-01-11 04:17:14,191 - INFO - Epoch time: 455.069454908371
2022-01-11 04:17:14,191 - INFO - 
Epoch: 25
2022-01-11 04:17:14,191 - INFO - 
Learning Rate: 0.1000
2022-01-11 04:18:22,374 - INFO - [Step=21000]	Loss=3.9745	223.3 examples/second
2022-01-11 04:20:34,702 - INFO - [Step=21250]	Loss=3.9312	241.8 examples/second
2022-01-11 04:22:47,638 - INFO - [Step=21500]	Loss=3.8812	240.7 examples/second
2022-01-11 04:24:47,578 - INFO - Test Loss=3.7491, Test top-1 acc=0.1207
2022-01-11 04:24:47,579 - INFO - Group Accuracy:

2022-01-11 04:24:47,579 - INFO - [0.939759   0.94506025 0.94120485 0.95228916 0.9392771  0.9595181
 0.9404819  0.9453012  0.9554217  0.94168675 0.95036143 0.9395181
 0.939759   0.94168675 0.9438554  0.9440964  0.96      ]
2022-01-11 04:24:47,580 - INFO - Epoch time: 453.3886525630951
2022-01-11 04:24:47,580 - INFO - 
Epoch: 26
2022-01-11 04:24:47,580 - INFO - 
Learning Rate: 0.1000
2022-01-11 04:25:11,290 - INFO - [Step=21750]	Loss=3.8558	222.8 examples/second
2022-01-11 04:27:24,353 - INFO - [Step=22000]	Loss=3.8144	240.5 examples/second
2022-01-11 04:29:36,517 - INFO - [Step=22250]	Loss=3.7738	242.1 examples/second
2022-01-11 04:31:49,220 - INFO - [Step=22500]	Loss=3.7200	241.1 examples/second
2022-01-11 04:32:21,544 - INFO - Test Loss=3.5874, Test top-1 acc=0.1511
2022-01-11 04:32:21,544 - INFO - Group Accuracy:

2022-01-11 04:32:21,544 - INFO - [0.94       0.9453012  0.9404819  0.95566267 0.9404819  0.96819276
 0.94       0.94216865 0.9546988  0.94096386 0.9520482  0.9395181
 0.939759   0.94216865 0.9436145  0.9508434  0.96481925]
2022-01-11 04:32:21,545 - INFO - Epoch time: 453.96540665626526
2022-01-11 04:32:21,546 - INFO - 
Epoch: 27
2022-01-11 04:32:21,546 - INFO - 
Learning Rate: 0.1000
2022-01-11 04:34:12,945 - INFO - [Step=22750]	Loss=3.6968	222.6 examples/second
2022-01-11 04:36:26,101 - INFO - [Step=23000]	Loss=3.6641	240.3 examples/second
2022-01-11 04:38:38,809 - INFO - [Step=23250]	Loss=3.6423	241.1 examples/second
2022-01-11 04:39:55,733 - INFO - Test Loss=3.3322, Test top-1 acc=0.1711
2022-01-11 04:39:55,733 - INFO - Group Accuracy:

2022-01-11 04:39:55,733 - INFO - [0.9431325  0.95228916 0.94096386 0.9590362  0.9407229  0.9713253
 0.9448193  0.94506025 0.95975906 0.9426506  0.95373493 0.94144577
 0.94       0.94289154 0.9431325  0.9506024  0.9657831 ]
2022-01-11 04:39:55,734 - INFO - Epoch time: 454.1880416870117
2022-01-11 04:39:55,734 - INFO - 
Epoch: 28
2022-01-11 04:39:55,734 - INFO - 
Learning Rate: 0.1000
2022-01-11 04:41:01,720 - INFO - [Step=23500]	Loss=3.6270	223.9 examples/second
2022-01-11 04:43:14,316 - INFO - [Step=23750]	Loss=3.5951	241.3 examples/second
2022-01-11 04:45:26,686 - INFO - [Step=24000]	Loss=3.5431	241.7 examples/second
2022-01-11 04:47:28,739 - INFO - Test Loss=3.5077, Test top-1 acc=0.1446
2022-01-11 04:47:28,740 - INFO - Group Accuracy:

2022-01-11 04:47:28,740 - INFO - [0.9438554  0.9513253  0.9404819  0.9539759  0.94168675 0.96698797
 0.9438554  0.94915664 0.95686746 0.9404819  0.9520482  0.93710846
 0.9395181  0.94289154 0.9440964  0.95036143 0.97156626]
2022-01-11 04:47:28,741 - INFO - Epoch time: 453.00775718688965
2022-01-11 04:47:28,741 - INFO - 
Epoch: 29
2022-01-11 04:47:28,742 - INFO - 
Learning Rate: 0.0100
2022-01-11 04:47:49,291 - INFO - [Step=24250]	Loss=3.5109	224.4 examples/second
2022-01-11 04:50:02,459 - INFO - [Step=24500]	Loss=3.1943	240.3 examples/second
2022-01-11 04:52:14,065 - INFO - [Step=24750]	Loss=3.1554	243.2 examples/second
2022-01-11 04:54:26,726 - INFO - [Step=25000]	Loss=3.1341	241.2 examples/second
2022-01-11 04:55:02,190 - INFO - Test Loss=2.7754, Test top-1 acc=0.1858
2022-01-11 04:55:02,191 - INFO - Group Accuracy:

2022-01-11 04:55:02,191 - INFO - [0.9486747  0.95759034 0.94626504 0.9674699  0.9436145  0.97614455
 0.9498795  0.9520482  0.96795183 0.9518072  0.9614458  0.9438554
 0.9407229  0.9433735  0.9460241  0.9549398  0.9766265 ]
2022-01-11 04:55:02,192 - INFO - Epoch time: 453.45036792755127
2022-01-11 04:55:02,192 - INFO - 
Epoch: 30
2022-01-11 04:55:02,192 - INFO - 
Learning Rate: 0.0100
2022-01-11 04:56:49,562 - INFO - [Step=25250]	Loss=3.0657	224.0 examples/second
2022-01-11 04:59:01,643 - INFO - [Step=25500]	Loss=3.0755	242.3 examples/second
2022-01-11 05:01:14,634 - INFO - [Step=25750]	Loss=3.0432	240.6 examples/second
2022-01-11 05:02:35,014 - INFO - Test Loss=2.7170, Test top-1 acc=0.1908
2022-01-11 05:02:35,015 - INFO - Group Accuracy:

2022-01-11 05:02:35,015 - INFO - [0.94963855 0.9578313  0.9479518  0.96819276 0.9433735  0.9780723
 0.9508434  0.95301205 0.96819276 0.9520482  0.96168673 0.9460241
 0.9404819  0.9431325  0.946506   0.95686746 0.97638553]
2022-01-11 05:02:35,016 - INFO - Epoch time: 452.823703289032
2022-01-11 05:02:35,016 - INFO - 
Epoch: 31
2022-01-11 05:02:35,016 - INFO - 
Learning Rate: 0.0100
2022-01-11 05:03:37,820 - INFO - [Step=26000]	Loss=3.0264	223.5 examples/second
2022-01-11 05:05:50,372 - INFO - [Step=26250]	Loss=2.9967	241.4 examples/second
2022-01-11 05:08:03,031 - INFO - [Step=26500]	Loss=3.0127	241.2 examples/second
2022-01-11 05:10:07,326 - INFO - Test Loss=2.6659, Test top-1 acc=0.1925
2022-01-11 05:10:07,326 - INFO - Group Accuracy:

2022-01-11 05:10:07,326 - INFO - [0.9486747  0.9573494  0.94891566 0.96843374 0.94506025 0.9785542
 0.9498795  0.9559036  0.96771085 0.95349395 0.96096385 0.94843376
 0.94144577 0.9431325  0.9472289  0.95638555 0.97638553]
2022-01-11 05:10:07,327 - INFO - Epoch time: 452.3114814758301
2022-01-11 05:10:07,327 - INFO - 
Epoch: 32
2022-01-11 05:10:07,327 - INFO - 
Learning Rate: 0.0100
2022-01-11 05:10:25,884 - INFO - [Step=26750]	Loss=2.9975	224.0 examples/second
2022-01-11 05:12:38,786 - INFO - [Step=27000]	Loss=2.9715	240.8 examples/second
2022-01-11 05:14:51,598 - INFO - [Step=27250]	Loss=2.9632	240.9 examples/second
2022-01-11 05:17:03,759 - INFO - [Step=27500]	Loss=2.9431	242.1 examples/second
2022-01-11 05:17:41,902 - INFO - Test Loss=2.6301, Test top-1 acc=0.1945
2022-01-11 05:17:41,903 - INFO - Group Accuracy:

2022-01-11 05:17:41,903 - INFO - [0.94915664 0.9592771  0.9477109  0.9686747  0.94626504 0.97614455
 0.94939756 0.9578313  0.96795183 0.95228916 0.96409637 0.9481928
 0.9407229  0.9438554  0.9486747  0.96       0.9768675 ]
2022-01-11 05:17:41,904 - INFO - Epoch time: 454.5764539241791
2022-01-11 05:17:41,904 - INFO - 
Epoch: 33
2022-01-11 05:17:41,904 - INFO - 
Learning Rate: 0.0100
2022-01-11 05:19:27,428 - INFO - [Step=27750]	Loss=2.9114	222.7 examples/second
2022-01-11 05:21:40,480 - INFO - [Step=28000]	Loss=2.9213	240.5 examples/second
2022-01-11 05:23:52,852 - INFO - [Step=28250]	Loss=2.9287	241.7 examples/second
2022-01-11 05:25:15,503 - INFO - Test Loss=2.5880, Test top-1 acc=0.1978
2022-01-11 05:25:15,503 - INFO - Group Accuracy:

2022-01-11 05:25:15,503 - INFO - [0.95228916 0.96024096 0.9501205  0.9691566  0.946747   0.9768675
 0.9510843  0.9587952  0.96891564 0.95301205 0.96385545 0.9506024
 0.94168675 0.9438554  0.9498795  0.9580723  0.97975904]
2022-01-11 05:25:15,504 - INFO - Epoch time: 453.5999376773834
2022-01-11 05:25:15,504 - INFO - 
Epoch: 34
2022-01-11 05:25:15,504 - INFO - 
Learning Rate: 0.0100
2022-01-11 05:26:15,392 - INFO - [Step=28500]	Loss=2.9215	224.5 examples/second
2022-01-11 05:28:29,153 - INFO - [Step=28750]	Loss=2.8988	239.2 examples/second
2022-01-11 05:30:41,202 - INFO - [Step=29000]	Loss=2.8904	242.3 examples/second
2022-01-11 05:32:49,161 - INFO - Test Loss=2.5832, Test top-1 acc=0.1904
2022-01-11 05:32:49,161 - INFO - Group Accuracy:

2022-01-11 05:32:49,161 - INFO - [0.9498795  0.96048194 0.94963855 0.9698795  0.9460241  0.9775904
 0.9513253  0.9578313  0.9674699  0.9554217  0.96433735 0.9510843
 0.940241   0.9445783  0.94963855 0.96048194 0.98024094]
2022-01-11 05:32:49,162 - INFO - Epoch time: 453.6581118106842
2022-01-11 05:32:49,162 - INFO - 
Epoch: 35
2022-01-11 05:32:49,162 - INFO - 
Learning Rate: 0.0100
2022-01-11 05:33:04,594 - INFO - [Step=29250]	Loss=2.8620	223.2 examples/second
2022-01-11 05:35:17,623 - INFO - [Step=29500]	Loss=2.8686	240.5 examples/second
2022-01-11 05:37:29,154 - INFO - [Step=29750]	Loss=2.8268	243.3 examples/second
2022-01-11 05:39:40,792 - INFO - [Step=30000]	Loss=2.8231	243.1 examples/second
2022-01-11 05:40:20,752 - INFO - Test Loss=2.5193, Test top-1 acc=0.1961
2022-01-11 05:40:20,753 - INFO - Group Accuracy:

2022-01-11 05:40:20,753 - INFO - [0.9508434  0.9592771  0.9498795  0.97108436 0.9469879  0.9787952
 0.95228916 0.95759034 0.9686747  0.9561446  0.96506023 0.95228916
 0.94096386 0.9445783  0.94939756 0.9587952  0.98024094]
2022-01-11 05:40:20,754 - INFO - Epoch time: 451.5917010307312
2022-01-11 05:40:20,754 - INFO - 
Epoch: 36
2022-01-11 05:40:20,754 - INFO - 
Learning Rate: 0.0100
2022-01-11 05:42:02,980 - INFO - [Step=30250]	Loss=2.8154	225.1 examples/second
2022-01-11 05:44:14,231 - INFO - [Step=30500]	Loss=2.8047	243.8 examples/second
2022-01-11 05:46:25,746 - INFO - [Step=30750]	Loss=2.8006	243.3 examples/second
2022-01-11 05:47:50,810 - INFO - Test Loss=2.5100, Test top-1 acc=0.2034
2022-01-11 05:47:50,811 - INFO - Group Accuracy:

2022-01-11 05:47:50,811 - INFO - [0.9508434  0.96       0.94939756 0.9706024  0.9477109  0.97903615
 0.9510843  0.9595181  0.9713253  0.9561446  0.96361446 0.95277107
 0.940241   0.94554216 0.9508434  0.9631325  0.9809638 ]
2022-01-11 05:47:50,812 - INFO - Epoch time: 450.0582027435303
2022-01-11 05:47:50,812 - INFO - 
Epoch: 37
2022-01-11 05:47:50,812 - INFO - 
Learning Rate: 0.0100
2022-01-11 05:48:48,865 - INFO - [Step=31000]	Loss=2.7919	223.6 examples/second
2022-01-11 05:51:00,648 - INFO - [Step=31250]	Loss=2.7607	242.8 examples/second
2022-01-11 05:53:12,307 - INFO - [Step=31500]	Loss=2.7618	243.1 examples/second
2022-01-11 05:55:22,602 - INFO - Test Loss=2.4347, Test top-1 acc=0.2101
2022-01-11 05:55:22,603 - INFO - Group Accuracy:

2022-01-11 05:55:22,603 - INFO - [0.9518072  0.9612048  0.9506024  0.97156626 0.95036143 0.9787952
 0.9515663  0.96072286 0.9739759  0.95566267 0.96409637 0.9549398
 0.94240963 0.94554216 0.9498795  0.9621687  0.98313254]
2022-01-11 05:55:22,604 - INFO - Epoch time: 451.7913737297058
2022-01-11 05:55:22,604 - INFO - 
Epoch: 38
2022-01-11 05:55:22,604 - INFO - 
Learning Rate: 0.0100
2022-01-11 05:55:35,851 - INFO - [Step=31750]	Loss=2.7421	222.9 examples/second
2022-01-11 05:57:46,708 - INFO - [Step=32000]	Loss=2.7309	244.5 examples/second
2022-01-11 05:59:58,069 - INFO - [Step=32250]	Loss=2.7267	243.6 examples/second
2022-01-11 06:02:10,590 - INFO - [Step=32500]	Loss=2.7051	241.5 examples/second
2022-01-11 06:02:53,017 - INFO - Test Loss=2.4345, Test top-1 acc=0.2012
2022-01-11 06:02:53,018 - INFO - Group Accuracy:

2022-01-11 06:02:53,018 - INFO - [0.9515663  0.96096385 0.9508434  0.97156626 0.94915664 0.97975904
 0.95228916 0.96072286 0.9727711  0.9559036  0.96481925 0.9551807
 0.94096386 0.94506025 0.9513253  0.9626506  0.9826506 ]
2022-01-11 06:02:53,019 - INFO - Epoch time: 450.4150631427765
2022-01-11 06:02:53,019 - INFO - 
Epoch: 39
2022-01-11 06:02:53,019 - INFO - 
Learning Rate: 0.0100
2022-01-11 06:04:32,661 - INFO - [Step=32750]	Loss=2.6718	225.2 examples/second
2022-01-11 06:06:44,374 - INFO - [Step=33000]	Loss=2.6895	243.0 examples/second
2022-01-11 06:08:56,084 - INFO - [Step=33250]	Loss=2.6961	243.0 examples/second
2022-01-11 06:10:22,626 - INFO - Test Loss=2.3999, Test top-1 acc=0.2087
2022-01-11 06:10:22,627 - INFO - Group Accuracy:

2022-01-11 06:10:22,627 - INFO - [0.9515663  0.9633735  0.9518072  0.9725301  0.9501205  0.9778313
 0.9518072  0.96096385 0.9703615  0.95686746 0.96409637 0.95710844
 0.9431325  0.94506025 0.9513253  0.9662651  0.98289156]
2022-01-11 06:10:22,627 - INFO - Epoch time: 449.60845136642456
2022-01-11 06:10:22,627 - INFO - 
Epoch: 40
2022-01-11 06:10:22,627 - INFO - 
Learning Rate: 0.0100
2022-01-11 06:11:16,729 - INFO - [Step=33500]	Loss=2.6551	227.5 examples/second
2022-01-11 06:13:29,012 - INFO - [Step=33750]	Loss=2.6451	241.9 examples/second
2022-01-11 06:15:40,087 - INFO - [Step=34000]	Loss=2.6296	244.1 examples/second
2022-01-11 06:17:51,386 - INFO - Test Loss=2.3382, Test top-1 acc=0.2108
2022-01-11 06:17:51,386 - INFO - Group Accuracy:

2022-01-11 06:17:51,387 - INFO - [0.95421684 0.9631325  0.95277107 0.9725301  0.95036143 0.9814458
 0.95228916 0.9631325  0.97445786 0.9578313  0.9662651  0.9551807
 0.94216865 0.9436145  0.9539759  0.96457833 0.9840964 ]
2022-01-11 06:17:51,387 - INFO - Epoch time: 448.759952545166
2022-01-11 06:17:51,387 - INFO - 
Epoch: 41
2022-01-11 06:17:51,387 - INFO - 
Learning Rate: 0.0100
2022-01-11 06:18:01,354 - INFO - [Step=34250]	Loss=2.6187	226.5 examples/second
2022-01-11 06:20:12,652 - INFO - [Step=34500]	Loss=2.6226	243.7 examples/second
2022-01-11 06:22:24,522 - INFO - [Step=34750]	Loss=2.6118	242.7 examples/second
2022-01-11 06:24:36,184 - INFO - [Step=35000]	Loss=2.5744	243.0 examples/second
2022-01-11 06:25:21,323 - INFO - Test Loss=2.3167, Test top-1 acc=0.2128
2022-01-11 06:25:21,324 - INFO - Group Accuracy:

2022-01-11 06:25:21,324 - INFO - [0.95277107 0.9633735  0.95325303 0.973253   0.95036143 0.97975904
 0.95253015 0.9592771  0.97566265 0.95759034 0.9660241  0.9580723
 0.94192773 0.9426506  0.95036143 0.96433735 0.9850602 ]
2022-01-11 06:25:21,325 - INFO - Epoch time: 449.9374704360962
2022-01-11 06:25:21,325 - INFO - 
Epoch: 42
2022-01-11 06:25:21,325 - INFO - 
Learning Rate: 0.0100
2022-01-11 06:26:58,230 - INFO - [Step=35250]	Loss=2.5859	225.3 examples/second
2022-01-11 06:29:10,076 - INFO - [Step=35500]	Loss=2.5705	242.7 examples/second
2022-01-11 06:31:21,009 - INFO - [Step=35750]	Loss=2.5562	244.4 examples/second
2022-01-11 06:32:50,523 - INFO - Test Loss=2.2520, Test top-1 acc=0.2193
2022-01-11 06:32:50,523 - INFO - Group Accuracy:

2022-01-11 06:32:50,533 - INFO - [0.9546988  0.9631325  0.9549398  0.9746988  0.95301205 0.98
 0.95253015 0.9631325  0.97542167 0.9590362  0.96795183 0.9580723
 0.9431325  0.9460241  0.9510843  0.96457833 0.9845783 ]
2022-01-11 06:32:50,534 - INFO - Epoch time: 449.20877385139465
2022-01-11 06:32:50,534 - INFO - 
Epoch: 43
2022-01-11 06:32:50,534 - INFO - 
Learning Rate: 0.0100
2022-01-11 06:33:42,891 - INFO - [Step=36000]	Loss=2.5409	225.5 examples/second
2022-01-11 06:35:54,787 - INFO - [Step=36250]	Loss=2.5311	242.6 examples/second
2022-01-11 06:38:05,746 - INFO - [Step=36500]	Loss=2.5198	244.4 examples/second
2022-01-11 06:40:20,944 - INFO - Test Loss=2.2414, Test top-1 acc=0.2190
2022-01-11 06:40:20,945 - INFO - Group Accuracy:

2022-01-11 06:40:20,945 - INFO - [0.9546988  0.96433735 0.9554217  0.97566265 0.9513253  0.98
 0.9539759  0.96361446 0.9768675  0.9614458  0.96891564 0.9587952
 0.94144577 0.94506025 0.9513253  0.96506023 0.9845783 ]
2022-01-11 06:40:20,946 - INFO - Epoch time: 450.41199564933777
2022-01-11 06:40:20,946 - INFO - 
Epoch: 44
2022-01-11 06:40:20,946 - INFO - 
Learning Rate: 0.0100
2022-01-11 06:40:28,394 - INFO - [Step=36750]	Loss=2.5282	224.3 examples/second
2022-01-11 06:42:40,582 - INFO - [Step=37000]	Loss=2.5149	242.1 examples/second
2022-01-11 06:44:52,038 - INFO - [Step=37250]	Loss=2.4865	243.4 examples/second
2022-01-11 06:47:04,308 - INFO - [Step=37500]	Loss=2.4995	241.9 examples/second
2022-01-11 06:47:52,565 - INFO - Test Loss=2.2110, Test top-1 acc=0.2169
2022-01-11 06:47:52,566 - INFO - Group Accuracy:

2022-01-11 06:47:52,566 - INFO - [0.95301205 0.9653012  0.9561446  0.9742169  0.9506024  0.9809638
 0.95301205 0.9621687  0.97614455 0.9580723  0.9696385  0.9580723
 0.94216865 0.9440964  0.95228916 0.96409637 0.9860241 ]
2022-01-11 06:47:52,566 - INFO - Epoch time: 451.6205222606659
2022-01-11 06:47:52,566 - INFO - 
Epoch: 45
2022-01-11 06:47:52,566 - INFO - 
Learning Rate: 0.0100
2022-01-11 06:49:26,817 - INFO - [Step=37750]	Loss=2.4614	224.5 examples/second
2022-01-11 06:51:38,995 - INFO - [Step=38000]	Loss=2.4788	242.1 examples/second
2022-01-11 06:53:51,259 - INFO - [Step=38250]	Loss=2.4324	241.9 examples/second
2022-01-11 06:55:23,864 - INFO - Test Loss=2.1684, Test top-1 acc=0.2320
2022-01-11 06:55:23,864 - INFO - Group Accuracy:

2022-01-11 06:55:23,864 - INFO - [0.9539759  0.96457833 0.95710844 0.97638553 0.95373493 0.98289156
 0.9539759  0.96361446 0.9785542  0.9585542  0.9696385  0.9592771
 0.9426506  0.9448193  0.95301205 0.96433735 0.98698795]
2022-01-11 06:55:23,865 - INFO - Epoch time: 451.29842948913574
2022-01-11 06:55:23,865 - INFO - 
Epoch: 46
2022-01-11 06:55:23,865 - INFO - 
Learning Rate: 0.0100
2022-01-11 06:56:13,389 - INFO - [Step=38500]	Loss=2.4338	225.1 examples/second
2022-01-11 06:58:24,571 - INFO - [Step=38750]	Loss=2.4099	243.9 examples/second
2022-01-11 07:00:36,260 - INFO - [Step=39000]	Loss=2.4412	243.0 examples/second
2022-01-11 07:02:54,562 - INFO - Test Loss=2.1189, Test top-1 acc=0.2333
2022-01-11 07:02:54,563 - INFO - Group Accuracy:

2022-01-11 07:02:54,563 - INFO - [0.9561446  0.96506023 0.9583132  0.9785542  0.95349395 0.9807229
 0.95349395 0.9619277  0.9787952  0.9595181  0.97156626 0.9626506
 0.94433737 0.9477109  0.9546988  0.9657831  0.98674697]
2022-01-11 07:02:54,563 - INFO - Epoch time: 450.69842290878296
2022-01-11 07:02:54,563 - INFO - 
Epoch: 47
2022-01-11 07:02:54,563 - INFO - 
Learning Rate: 0.0100
2022-01-11 07:02:59,447 - INFO - [Step=39250]	Loss=2.4239	223.5 examples/second
2022-01-11 07:05:10,636 - INFO - [Step=39500]	Loss=2.3931	243.9 examples/second
2022-01-11 07:07:22,992 - INFO - [Step=39750]	Loss=2.4007	241.8 examples/second
2022-01-11 07:09:34,837 - INFO - [Step=40000]	Loss=2.3874	242.7 examples/second
2022-01-11 07:10:24,810 - INFO - Test Loss=2.1259, Test top-1 acc=0.2407
2022-01-11 07:10:24,810 - INFO - Group Accuracy:

2022-01-11 07:10:24,810 - INFO - [0.9549398  0.9655422  0.95686746 0.97590363 0.95373493 0.98361444
 0.95373493 0.9653012  0.97831327 0.96096385 0.97108436 0.9612048
 0.9426506  0.94578314 0.95421684 0.9660241  0.98698795]
2022-01-11 07:10:24,811 - INFO - Epoch time: 450.2473783493042
2022-01-11 07:10:24,811 - INFO - 
Epoch: 48
2022-01-11 07:10:24,811 - INFO - 
Learning Rate: 0.0100
2022-01-11 07:11:56,067 - INFO - [Step=40250]	Loss=2.3608	226.6 examples/second
2022-01-11 07:14:08,677 - INFO - [Step=40500]	Loss=2.3403	241.3 examples/second
2022-01-11 07:16:19,859 - INFO - [Step=40750]	Loss=2.3598	243.9 examples/second
2022-01-11 07:17:54,208 - INFO - Test Loss=2.0690, Test top-1 acc=0.2520
2022-01-11 07:17:54,208 - INFO - Group Accuracy:

2022-01-11 07:17:54,208 - INFO - [0.95638555 0.96506023 0.9587952  0.9775904  0.9551807  0.98433733
 0.9546988  0.96481925 0.9785542  0.96096385 0.97108436 0.9633735
 0.94626504 0.94433737 0.9546988  0.966747   0.9860241 ]
2022-01-11 07:17:54,209 - INFO - Epoch time: 449.3983733654022
2022-01-11 07:17:54,209 - INFO - 
Epoch: 49
2022-01-11 07:17:54,209 - INFO - 
Learning Rate: 0.0100
2022-01-11 07:18:41,108 - INFO - [Step=41000]	Loss=2.3558	226.6 examples/second
2022-01-11 07:20:53,596 - INFO - [Step=41250]	Loss=2.3092	241.5 examples/second
2022-01-11 07:23:04,058 - INFO - [Step=41500]	Loss=2.3171	245.3 examples/second
2022-01-11 07:25:15,292 - INFO - [Step=41750]	Loss=2.3219	243.8 examples/second
2022-01-11 07:25:23,479 - INFO - Test Loss=2.0242, Test top-1 acc=0.2518
2022-01-11 07:25:23,479 - INFO - Group Accuracy:

2022-01-11 07:25:23,479 - INFO - [0.9546988  0.9655422  0.9580723  0.97951806 0.95686746 0.9845783
 0.9561446  0.96433735 0.97927713 0.9633735  0.9693976  0.96409637
 0.94578314 0.94891566 0.9561446  0.9686747  0.98626506]
2022-01-11 07:25:23,480 - INFO - Epoch time: 449.2707107067108
2022-01-11 07:25:23,480 - INFO - 
Epoch: 50
2022-01-11 07:25:23,480 - INFO - 
Learning Rate: 0.0100
2022-01-11 07:27:38,037 - INFO - [Step=42000]	Loss=2.3053	224.2 examples/second
2022-01-11 07:29:49,799 - INFO - [Step=42250]	Loss=2.2875	242.9 examples/second
2022-01-11 07:32:01,380 - INFO - [Step=42500]	Loss=2.2988	243.2 examples/second
2022-01-11 07:32:54,960 - INFO - Test Loss=2.0231, Test top-1 acc=0.2569
2022-01-11 07:32:54,960 - INFO - Group Accuracy:

2022-01-11 07:32:54,960 - INFO - [0.9573494  0.9660241  0.95759034 0.9780723  0.9580723  0.9819277
 0.9561446  0.95975906 0.9787952  0.9626506  0.97180724 0.96433735
 0.94578314 0.9472289  0.9539759  0.966506   0.98722893]
2022-01-11 07:32:54,962 - INFO - Epoch time: 451.48159646987915
2022-01-11 07:32:54,962 - INFO - 
Epoch: 51
2022-01-11 07:32:54,962 - INFO - 
Learning Rate: 0.0100
2022-01-11 07:34:24,603 - INFO - [Step=42750]	Loss=2.2832	223.4 examples/second
2022-01-11 07:36:36,572 - INFO - [Step=43000]	Loss=2.2479	242.5 examples/second
2022-01-11 07:38:47,493 - INFO - [Step=43250]	Loss=2.2953	244.4 examples/second
2022-01-11 07:40:25,688 - INFO - Test Loss=2.0110, Test top-1 acc=0.2648
2022-01-11 07:40:25,689 - INFO - Group Accuracy:

2022-01-11 07:40:25,689 - INFO - [0.95638555 0.9686747  0.95759034 0.9775904  0.9590362  0.98240966
 0.95662653 0.96168673 0.97975904 0.96385545 0.97301203 0.96433735
 0.946506   0.94915664 0.95686746 0.966747   0.98650604]
2022-01-11 07:40:25,690 - INFO - Epoch time: 450.7280788421631
2022-01-11 07:40:25,690 - INFO - 
Epoch: 52
2022-01-11 07:40:25,690 - INFO - 
Learning Rate: 0.0100
2022-01-11 07:41:11,033 - INFO - [Step=43500]	Loss=2.2692	222.9 examples/second
2022-01-11 07:43:22,133 - INFO - [Step=43750]	Loss=2.2451	244.1 examples/second
2022-01-11 07:45:33,186 - INFO - [Step=44000]	Loss=2.2549	244.2 examples/second
2022-01-11 07:47:45,728 - INFO - [Step=44250]	Loss=2.2413	241.4 examples/second
2022-01-11 07:47:56,572 - INFO - Test Loss=1.9791, Test top-1 acc=0.2737
2022-01-11 07:47:56,572 - INFO - Group Accuracy:

2022-01-11 07:47:56,572 - INFO - [0.9583132  0.96891564 0.96       0.97951806 0.9585542  0.9860241
 0.95638555 0.966506   0.97951806 0.9619277  0.9737349  0.96481925
 0.9472289  0.9510843  0.95662653 0.96819276 0.98771083]
2022-01-11 07:47:56,573 - INFO - Epoch time: 450.8832697868347
2022-01-11 07:47:56,573 - INFO - 
Epoch: 53
2022-01-11 07:47:56,573 - INFO - 
Learning Rate: 0.0100
2022-01-11 07:50:07,253 - INFO - [Step=44500]	Loss=2.2174	226.1 examples/second
2022-01-11 07:52:18,410 - INFO - [Step=44750]	Loss=2.2317	244.0 examples/second
2022-01-11 07:54:29,916 - INFO - [Step=45000]	Loss=2.2248	243.3 examples/second
2022-01-11 07:55:25,553 - INFO - Test Loss=1.9915, Test top-1 acc=0.2602
2022-01-11 07:55:25,554 - INFO - Group Accuracy:

2022-01-11 07:55:25,554 - INFO - [0.9561446  0.96819276 0.95975906 0.97927713 0.95759034 0.98578316
 0.95710844 0.96433735 0.9807229  0.95975906 0.9742169  0.9653012
 0.9474699  0.9479518  0.95566267 0.96843374 0.98722893]
2022-01-11 07:55:25,555 - INFO - Epoch time: 448.9815442562103
2022-01-11 07:55:25,555 - INFO - 
Epoch: 54
2022-01-11 07:55:25,555 - INFO - 
Learning Rate: 0.0100
2022-01-11 07:56:51,255 - INFO - [Step=45250]	Loss=2.1868	226.4 examples/second
2022-01-11 07:59:01,879 - INFO - [Step=45500]	Loss=2.1771	245.0 examples/second
2022-01-11 08:01:13,426 - INFO - [Step=45750]	Loss=2.2038	243.3 examples/second
2022-01-11 08:02:52,730 - INFO - Test Loss=1.9314, Test top-1 acc=0.2757
2022-01-11 08:02:52,730 - INFO - Group Accuracy:

2022-01-11 08:02:52,731 - INFO - [0.9587952  0.9696385  0.9619277  0.9778313  0.9614458  0.98578316
 0.95662653 0.96409637 0.9816868  0.96433735 0.97542167 0.9655422
 0.9474699  0.9508434  0.95710844 0.96843374 0.98722893]
2022-01-11 08:02:52,731 - INFO - Epoch time: 447.17650055885315
2022-01-11 08:02:52,731 - INFO - 
Epoch: 55
2022-01-11 08:02:52,732 - INFO - 
Learning Rate: 0.0100
2022-01-11 08:03:34,107 - INFO - [Step=46000]	Loss=2.2013	227.5 examples/second
2022-01-11 08:05:45,697 - INFO - [Step=46250]	Loss=2.1456	243.2 examples/second
2022-01-11 08:07:57,762 - INFO - [Step=46500]	Loss=2.1729	242.3 examples/second
2022-01-11 08:10:09,827 - INFO - [Step=46750]	Loss=2.1547	242.3 examples/second
2022-01-11 08:10:23,338 - INFO - Test Loss=1.9253, Test top-1 acc=0.2701
2022-01-11 08:10:23,339 - INFO - Group Accuracy:

2022-01-11 08:10:23,339 - INFO - [0.95686746 0.9706024  0.9628916  0.9804819  0.9595181  0.98698795
 0.9561446  0.9633735  0.9814458  0.9631325  0.97445786 0.9657831
 0.946747   0.9486747  0.9546988  0.96891564 0.98722893]
2022-01-11 08:10:23,340 - INFO - Epoch time: 450.6084201335907
2022-01-11 08:10:23,340 - INFO - 
Epoch: 56
2022-01-11 08:10:23,340 - INFO - 
Learning Rate: 0.0100
2022-01-11 08:12:31,243 - INFO - [Step=47000]	Loss=2.1466	226.3 examples/second
2022-01-11 08:14:42,554 - INFO - [Step=47250]	Loss=2.1363	243.7 examples/second
2022-01-11 08:16:54,024 - INFO - [Step=47500]	Loss=2.1322	243.4 examples/second
2022-01-11 08:17:51,684 - INFO - Test Loss=1.9004, Test top-1 acc=0.2800
2022-01-11 08:17:51,685 - INFO - Group Accuracy:

2022-01-11 08:17:51,685 - INFO - [0.95975906 0.9696385  0.9626506  0.9787952  0.9612048  0.9860241
 0.9573494  0.9662651  0.9833735  0.9631325  0.9742169  0.966506
 0.946747   0.9513253  0.9578313  0.9693976  0.98722893]
2022-01-11 08:17:51,686 - INFO - Epoch time: 448.34557938575745
2022-01-11 08:17:51,686 - INFO - 
Epoch: 57
2022-01-11 08:17:51,686 - INFO - 
Learning Rate: 0.0100
2022-01-11 08:19:15,339 - INFO - [Step=47750]	Loss=2.1331	226.4 examples/second
2022-01-11 08:21:26,660 - INFO - [Step=48000]	Loss=2.1480	243.7 examples/second
2022-01-11 08:23:37,372 - INFO - [Step=48250]	Loss=2.1216	244.8 examples/second
2022-01-11 08:25:19,731 - INFO - Test Loss=1.8813, Test top-1 acc=0.2851
2022-01-11 08:25:19,731 - INFO - Group Accuracy:

2022-01-11 08:25:19,731 - INFO - [0.95975906 0.9708434  0.9612048  0.98289156 0.9614458  0.9855422
 0.9583132  0.9672289  0.98289156 0.9660241  0.973494   0.96795183
 0.9477109  0.95301205 0.95759034 0.9701205  0.98626506]
2022-01-11 08:25:19,732 - INFO - Epoch time: 448.0462176799774
2022-01-11 08:25:19,732 - INFO - 
Epoch: 58
2022-01-11 08:25:19,732 - INFO - 
Learning Rate: 0.0100
2022-01-11 08:25:59,033 - INFO - [Step=48500]	Loss=2.0993	225.9 examples/second
2022-01-11 08:28:10,565 - INFO - [Step=48750]	Loss=2.0980	243.3 examples/second
2022-01-11 08:30:22,182 - INFO - [Step=49000]	Loss=2.1028	243.1 examples/second
2022-01-11 08:32:33,353 - INFO - [Step=49250]	Loss=2.1115	244.0 examples/second
2022-01-11 08:32:49,525 - INFO - Test Loss=1.8642, Test top-1 acc=0.2935
2022-01-11 08:32:49,526 - INFO - Group Accuracy:

2022-01-11 08:32:49,526 - INFO - [0.96048194 0.97301203 0.96048194 0.9807229  0.95975906 0.9860241
 0.9583132  0.9657831  0.9816868  0.96481925 0.9737349  0.9696385
 0.94963855 0.95325303 0.9592771  0.9686747  0.9889157 ]
2022-01-11 08:32:49,527 - INFO - Epoch time: 449.7953290939331
2022-01-11 08:32:49,527 - INFO - 
Epoch: 59
2022-01-11 08:32:49,528 - INFO - 
Learning Rate: 0.0010
2022-01-11 08:34:54,292 - INFO - [Step=49500]	Loss=2.0372	227.0 examples/second
2022-01-11 08:37:04,763 - INFO - [Step=49750]	Loss=2.0101	245.3 examples/second
2022-01-11 08:39:15,137 - INFO - [Step=50000]	Loss=2.0063	245.4 examples/second
2022-01-11 08:40:15,978 - INFO - Test Loss=1.7819, Test top-1 acc=0.2952
2022-01-11 08:40:15,978 - INFO - Group Accuracy:

2022-01-11 08:40:15,978 - INFO - [0.96096385 0.9727711  0.9631325  0.98240966 0.96361446 0.98771083
 0.96096385 0.96819276 0.98240966 0.9657831  0.9768675  0.9693976
 0.9486747  0.95373493 0.9592771  0.97108436 0.9879518 ]
2022-01-11 08:40:15,979 - INFO - Epoch time: 446.4513900279999
2022-01-11 08:40:15,979 - INFO - 
Epoch: 60
2022-01-11 08:40:15,979 - INFO - 
Learning Rate: 0.0010
2022-01-11 08:41:36,535 - INFO - [Step=50250]	Loss=2.0038	226.3 examples/second
2022-01-11 08:43:46,840 - INFO - [Step=50500]	Loss=2.0152	245.6 examples/second
2022-01-11 08:45:57,396 - INFO - [Step=50750]	Loss=1.9923	245.1 examples/second
2022-01-11 08:47:41,786 - INFO - Test Loss=1.7794, Test top-1 acc=0.2969
2022-01-11 08:47:41,787 - INFO - Group Accuracy:

2022-01-11 08:47:41,787 - INFO - [0.96096385 0.973494   0.9624096  0.9816868  0.9633735  0.9881928
 0.96048194 0.96795183 0.9826506  0.9662651  0.9771084  0.9698795
 0.9486747  0.95325303 0.9590362  0.9696385  0.9881928 ]
2022-01-11 08:47:41,789 - INFO - Epoch time: 445.80993461608887
2022-01-11 08:47:41,789 - INFO - 
Epoch: 61
2022-01-11 08:47:41,789 - INFO - 
Learning Rate: 0.0010
2022-01-11 08:48:17,669 - INFO - [Step=51000]	Loss=2.0064	228.1 examples/second
2022-01-11 08:50:28,582 - INFO - [Step=51250]	Loss=1.9694	244.4 examples/second
2022-01-11 08:52:40,064 - INFO - [Step=51500]	Loss=1.9978	243.4 examples/second
2022-01-11 08:54:51,254 - INFO - [Step=51750]	Loss=2.0015	243.9 examples/second
2022-01-11 08:55:09,797 - INFO - Test Loss=1.7697, Test top-1 acc=0.2966
2022-01-11 08:55:09,798 - INFO - Group Accuracy:

2022-01-11 08:55:09,798 - INFO - [0.96096385 0.9742169  0.9614458  0.98240966 0.9653012  0.98771083
 0.9621687  0.9698795  0.9826506  0.96698797 0.9766265  0.9693976
 0.94963855 0.9520482  0.96096385 0.97108436 0.98746985]
2022-01-11 08:55:09,799 - INFO - Epoch time: 448.00961685180664
2022-01-11 08:55:09,799 - INFO - 
Epoch: 62
2022-01-11 08:55:09,799 - INFO - 
Learning Rate: 0.0010
2022-01-11 08:57:12,912 - INFO - [Step=52000]	Loss=1.9901	225.9 examples/second
2022-01-11 08:59:23,938 - INFO - [Step=52250]	Loss=1.9693	244.2 examples/second
2022-01-11 09:01:35,276 - INFO - [Step=52500]	Loss=1.9849	243.6 examples/second
2022-01-11 09:02:38,835 - INFO - Test Loss=1.7717, Test top-1 acc=0.2964
2022-01-11 09:02:38,836 - INFO - Group Accuracy:

2022-01-11 09:02:38,836 - INFO - [0.96168673 0.9739759  0.9633735  0.9821687  0.96361446 0.98674697
 0.96072286 0.96891564 0.9821687  0.9655422  0.97638553 0.9701205
 0.9508434  0.95301205 0.9585542  0.9703615  0.9881928 ]
2022-01-11 09:02:38,837 - INFO - Epoch time: 449.03812742233276
2022-01-11 09:02:38,837 - INFO - 
Epoch: 63
2022-01-11 09:02:38,837 - INFO - 
Learning Rate: 0.0010
2022-01-11 09:03:57,089 - INFO - [Step=52750]	Loss=1.9888	225.7 examples/second
2022-01-11 09:06:08,002 - INFO - [Step=53000]	Loss=1.9891	244.4 examples/second
2022-01-11 09:08:19,216 - INFO - [Step=53250]	Loss=1.9842	243.9 examples/second
2022-01-11 09:10:07,862 - INFO - Test Loss=1.7681, Test top-1 acc=0.2957
2022-01-11 09:10:07,862 - INFO - Group Accuracy:

2022-01-11 09:10:07,862 - INFO - [0.96096385 0.9739759  0.96168673 0.9812048  0.96506023 0.98746985
 0.9614458  0.9708434  0.98313254 0.9657831  0.97638553 0.9703615
 0.94915664 0.95349395 0.96       0.9713253  0.9891566 ]
2022-01-11 09:10:07,863 - INFO - Epoch time: 449.02614092826843
2022-01-11 09:10:07,863 - INFO - 
Epoch: 64
2022-01-11 09:10:07,863 - INFO - 
Learning Rate: 0.0010
2022-01-11 09:10:41,795 - INFO - [Step=53500]	Loss=1.9790	224.4 examples/second
2022-01-11 09:12:53,792 - INFO - [Step=53750]	Loss=1.9830	242.4 examples/second
2022-01-11 09:15:05,773 - INFO - [Step=54000]	Loss=1.9565	242.5 examples/second
2022-01-11 09:17:17,418 - INFO - [Step=54250]	Loss=1.9798	243.1 examples/second
2022-01-11 09:17:38,824 - INFO - Test Loss=1.7597, Test top-1 acc=0.2925
2022-01-11 09:17:38,824 - INFO - Group Accuracy:

2022-01-11 09:17:38,824 - INFO - [0.9619277  0.9737349  0.9612048  0.9821687  0.96361446 0.9884337
 0.96024096 0.96891564 0.98289156 0.9662651  0.97566265 0.9708434
 0.94963855 0.9539759  0.9590362  0.9708434  0.9884337 ]
2022-01-11 09:17:38,825 - INFO - Epoch time: 450.96228790283203
2022-01-11 09:17:38,825 - INFO - 
Epoch: 65
2022-01-11 09:17:38,825 - INFO - 
Learning Rate: 0.0010
2022-01-11 09:19:39,678 - INFO - [Step=54500]	Loss=1.9701	224.9 examples/second
2022-01-11 09:21:51,874 - INFO - [Step=54750]	Loss=1.9551	242.1 examples/second
2022-01-11 09:24:03,929 - INFO - [Step=55000]	Loss=1.9788	242.3 examples/second
2022-01-11 09:25:09,688 - INFO - Test Loss=1.7498, Test top-1 acc=0.2896
2022-01-11 09:25:09,688 - INFO - Group Accuracy:

2022-01-11 09:25:09,688 - INFO - [0.96096385 0.973253   0.9631325  0.98240966 0.9655422  0.9879518
 0.96168673 0.9698795  0.9826506  0.9660241  0.9773494  0.9701205
 0.9486747  0.95228916 0.96072286 0.97156626 0.9879518 ]
2022-01-11 09:25:09,689 - INFO - Epoch time: 450.86342096328735
2022-01-11 09:25:09,689 - INFO - 
Epoch: 66
2022-01-11 09:25:09,689 - INFO - 
Learning Rate: 0.0010
2022-01-11 09:26:26,025 - INFO - [Step=55250]	Loss=1.9735	225.2 examples/second
2022-01-11 09:28:38,494 - INFO - [Step=55500]	Loss=1.9648	241.6 examples/second
2022-01-11 09:30:50,663 - INFO - [Step=55750]	Loss=1.9691	242.1 examples/second
2022-01-11 09:32:42,888 - INFO - Test Loss=1.7562, Test top-1 acc=0.2947
2022-01-11 09:32:42,888 - INFO - Group Accuracy:

2022-01-11 09:32:42,889 - INFO - [0.9619277  0.9742169  0.9619277  0.9826506  0.96481925 0.9881928
 0.9624096  0.9691566  0.9826506  0.966747   0.9766265  0.9703615
 0.95036143 0.95228916 0.96       0.9706024  0.9879518 ]
2022-01-11 09:32:42,889 - INFO - Epoch time: 453.20035099983215
2022-01-11 09:32:42,889 - INFO - 
Epoch: 67
2022-01-11 09:32:42,889 - INFO - 
Learning Rate: 0.0010
2022-01-11 09:33:14,779 - INFO - [Step=56000]	Loss=1.9559	222.0 examples/second
2022-01-11 09:35:26,188 - INFO - [Step=56250]	Loss=1.9461	243.5 examples/second
2022-01-11 09:37:38,863 - INFO - [Step=56500]	Loss=1.9757	241.2 examples/second
2022-01-11 09:39:51,134 - INFO - [Step=56750]	Loss=1.9519	241.9 examples/second
2022-01-11 09:40:15,480 - INFO - Test Loss=1.7553, Test top-1 acc=0.2961
2022-01-11 09:40:15,481 - INFO - Group Accuracy:

2022-01-11 09:40:15,481 - INFO - [0.9614458  0.973494   0.9621687  0.9826506  0.96481925 0.98771083
 0.9619277  0.9696385  0.98289156 0.9662651  0.9773494  0.9696385
 0.94915664 0.95301205 0.9595181  0.97180724 0.9884337 ]
2022-01-11 09:40:15,482 - INFO - Epoch time: 452.5924389362335
2022-01-11 09:40:15,482 - INFO - 
Epoch: 68
2022-01-11 09:40:15,482 - INFO - 
Learning Rate: 0.0010
2022-01-11 09:42:13,976 - INFO - [Step=57000]	Loss=1.9734	224.0 examples/second
2022-01-11 09:44:26,636 - INFO - [Step=57250]	Loss=1.9490	241.2 examples/second
2022-01-11 09:46:39,561 - INFO - [Step=57500]	Loss=1.9426	240.7 examples/second
2022-01-11 09:47:48,787 - INFO - Test Loss=1.7548, Test top-1 acc=0.2923
2022-01-11 09:47:48,787 - INFO - Group Accuracy:

2022-01-11 09:47:48,787 - INFO - [0.9612048  0.9746988  0.9626506  0.9826506  0.96506023 0.98722893
 0.9626506  0.9701205  0.9838554  0.96698797 0.9775904  0.9698795
 0.9481928  0.95301205 0.96024096 0.97156626 0.9886747 ]
2022-01-11 09:47:48,788 - INFO - Epoch time: 453.3060259819031
2022-01-11 09:47:48,788 - INFO - 
Epoch: 69
2022-01-11 09:47:48,788 - INFO - 
Learning Rate: 0.0010
2022-01-11 09:49:02,589 - INFO - [Step=57750]	Loss=1.9605	223.7 examples/second
2022-01-11 09:51:14,690 - INFO - [Step=58000]	Loss=1.9622	242.2 examples/second
2022-01-11 09:53:27,763 - INFO - [Step=58250]	Loss=1.9397	240.5 examples/second
2022-01-11 09:55:22,333 - INFO - Test Loss=1.7487, Test top-1 acc=0.2964
2022-01-11 09:55:22,334 - INFO - Group Accuracy:

2022-01-11 09:55:22,334 - INFO - [0.96168673 0.9739759  0.96168673 0.98240966 0.96457833 0.98771083
 0.9621687  0.9703615  0.9838554  0.9655422  0.97831327 0.9703615
 0.94891566 0.95325303 0.95975906 0.9725301  0.9891566 ]
2022-01-11 09:55:22,334 - INFO - Epoch time: 453.5463218688965
2022-01-11 09:55:22,334 - INFO - 
Epoch: 70
2022-01-11 09:55:22,334 - INFO - 
Learning Rate: 0.0010
2022-01-11 09:55:50,908 - INFO - [Step=58500]	Loss=1.9250	223.6 examples/second
2022-01-11 09:58:02,784 - INFO - [Step=58750]	Loss=1.9512	242.7 examples/second
2022-01-11 10:00:15,452 - INFO - [Step=59000]	Loss=1.9520	241.2 examples/second
2022-01-11 10:02:26,726 - INFO - [Step=59250]	Loss=1.9519	243.8 examples/second
2022-01-11 10:02:53,506 - INFO - Test Loss=1.7466, Test top-1 acc=0.2889
2022-01-11 10:02:53,506 - INFO - Group Accuracy:

2022-01-11 10:02:53,506 - INFO - [0.9612048  0.973494   0.9628916  0.9821687  0.9655422  0.9879518
 0.9614458  0.9706024  0.9833735  0.9662651  0.9775904  0.9703615
 0.9477109  0.9520482  0.9587952  0.97156626 0.9881928 ]
2022-01-11 10:02:53,507 - INFO - Epoch time: 451.17236733436584
2022-01-11 10:02:53,507 - INFO - 
Epoch: 71
2022-01-11 10:02:53,507 - INFO - 
Learning Rate: 0.0010
2022-01-11 10:04:49,793 - INFO - [Step=59500]	Loss=1.9591	223.7 examples/second
2022-01-11 10:07:02,073 - INFO - [Step=59750]	Loss=1.9396	241.9 examples/second
2022-01-11 10:09:14,051 - INFO - [Step=60000]	Loss=1.9359	242.5 examples/second
2022-01-11 10:10:25,941 - INFO - Test Loss=1.7487, Test top-1 acc=0.2942
2022-01-11 10:10:25,941 - INFO - Group Accuracy:

2022-01-11 10:10:25,941 - INFO - [0.96168673 0.9739759  0.9624096  0.9826506  0.96361446 0.9879518
 0.9626506  0.9706024  0.98313254 0.966747   0.9773494  0.9708434
 0.94915664 0.95349395 0.96048194 0.9725301  0.9891566 ]
2022-01-11 10:10:25,942 - INFO - Epoch time: 452.43508434295654
2022-01-11 10:10:25,942 - INFO - 
Epoch: 72
2022-01-11 10:10:25,942 - INFO - 
Learning Rate: 0.0010
2022-01-11 10:11:37,381 - INFO - [Step=60250]	Loss=1.9466	223.3 examples/second
2022-01-11 10:13:50,187 - INFO - [Step=60500]	Loss=1.9534	241.0 examples/second
2022-01-11 10:16:02,149 - INFO - [Step=60750]	Loss=1.9470	242.5 examples/second
2022-01-11 10:17:59,459 - INFO - Test Loss=1.7440, Test top-1 acc=0.2949
2022-01-11 10:17:59,460 - INFO - Group Accuracy:

2022-01-11 10:17:59,460 - INFO - [0.96168673 0.9742169  0.9612048  0.9814458  0.9653012  0.98771083
 0.9621687  0.9713253  0.9833735  0.9662651  0.9778313  0.9696385
 0.94915664 0.9539759  0.95975906 0.97228914 0.9889157 ]
2022-01-11 10:17:59,461 - INFO - Epoch time: 453.51887798309326
2022-01-11 10:17:59,461 - INFO - 
Epoch: 73
2022-01-11 10:17:59,461 - INFO - 
Learning Rate: 0.0010
2022-01-11 10:18:26,603 - INFO - [Step=61000]	Loss=1.9665	221.5 examples/second
2022-01-11 10:20:39,010 - INFO - [Step=61250]	Loss=1.9447	241.7 examples/second
2022-01-11 10:22:52,141 - INFO - [Step=61500]	Loss=1.9475	240.4 examples/second
2022-01-11 10:25:04,848 - INFO - [Step=61750]	Loss=1.9336	241.1 examples/second
2022-01-11 10:25:34,284 - INFO - Test Loss=1.7392, Test top-1 acc=0.2981
2022-01-11 10:25:34,285 - INFO - Group Accuracy:

2022-01-11 10:25:34,285 - INFO - [0.9614458  0.9739759  0.9628916  0.9826506  0.9653012  0.9881928
 0.9621687  0.9698795  0.9833735  0.96771085 0.9773494  0.9706024
 0.94939756 0.9539759  0.96048194 0.9725301  0.9889157 ]
2022-01-11 10:25:34,286 - INFO - Epoch time: 454.82473707199097
2022-01-11 10:25:34,286 - INFO - 
Epoch: 74
2022-01-11 10:25:34,286 - INFO - 
Learning Rate: 0.0010
2022-01-11 10:27:27,924 - INFO - [Step=62000]	Loss=1.9458	223.7 examples/second
2022-01-11 10:29:39,540 - INFO - [Step=62250]	Loss=1.9272	243.1 examples/second
2022-01-11 10:31:51,911 - INFO - [Step=62500]	Loss=1.9288	241.7 examples/second
2022-01-11 10:33:07,774 - INFO - Test Loss=1.7378, Test top-1 acc=0.2949
2022-01-11 10:33:07,775 - INFO - Group Accuracy:

2022-01-11 10:33:07,775 - INFO - [0.96096385 0.9737349  0.9619277  0.98289156 0.96409637 0.98746985
 0.96361446 0.9703615  0.9833735  0.966747   0.9775904  0.9708434
 0.94891566 0.95373493 0.96024096 0.97156626 0.9881928 ]
2022-01-11 10:33:07,775 - INFO - Epoch time: 453.4895305633545
2022-01-11 10:33:07,775 - INFO - 
Epoch: 75
2022-01-11 10:33:07,775 - INFO - 
Learning Rate: 0.0010
2022-01-11 10:34:16,379 - INFO - [Step=62750]	Loss=1.9443	221.5 examples/second
2022-01-11 10:36:28,475 - INFO - [Step=63000]	Loss=1.9280	242.2 examples/second
2022-01-11 10:38:42,171 - INFO - [Step=63250]	Loss=1.9361	239.4 examples/second
2022-01-11 10:40:42,518 - INFO - Test Loss=1.7403, Test top-1 acc=0.2920
2022-01-11 10:40:42,519 - INFO - Group Accuracy:

2022-01-11 10:40:42,519 - INFO - [0.96048194 0.97566265 0.96457833 0.98240966 0.96506023 0.98722893
 0.9624096  0.9701205  0.98289156 0.9660241  0.9768675  0.97108436
 0.94963855 0.9544578  0.96048194 0.97180724 0.9886747 ]
2022-01-11 10:40:42,520 - INFO - Epoch time: 454.7441945075989
2022-01-11 10:40:42,520 - INFO - 
Epoch: 76
2022-01-11 10:40:42,520 - INFO - 
Learning Rate: 0.0010
2022-01-11 10:41:06,080 - INFO - [Step=63500]	Loss=1.9412	222.4 examples/second
2022-01-11 10:43:17,683 - INFO - [Step=63750]	Loss=1.9113	243.2 examples/second
2022-01-11 10:45:30,779 - INFO - [Step=64000]	Loss=1.9433	240.4 examples/second
2022-01-11 10:47:43,130 - INFO - [Step=64250]	Loss=1.9247	241.8 examples/second
2022-01-11 10:48:15,387 - INFO - Test Loss=1.7366, Test top-1 acc=0.2928
2022-01-11 10:48:15,388 - INFO - Group Accuracy:

2022-01-11 10:48:15,388 - INFO - [0.9619277  0.973253   0.9626506  0.98313254 0.96433735 0.98722893
 0.96168673 0.9708434  0.98313254 0.9660241  0.9771084  0.9703615
 0.94963855 0.95373493 0.96048194 0.97204816 0.9879518 ]
2022-01-11 10:48:15,388 - INFO - Epoch time: 452.8687813282013
2022-01-11 10:48:15,389 - INFO - 
Epoch: 77
2022-01-11 10:48:15,389 - INFO - 
Learning Rate: 0.0010
2022-01-11 10:50:06,643 - INFO - [Step=64500]	Loss=1.9308	223.0 examples/second
2022-01-11 10:52:19,612 - INFO - [Step=64750]	Loss=1.9493	240.7 examples/second
2022-01-11 10:54:32,600 - INFO - [Step=65000]	Loss=1.9107	240.6 examples/second
2022-01-11 10:55:50,158 - INFO - Test Loss=1.7419, Test top-1 acc=0.2947
2022-01-11 10:55:50,158 - INFO - Group Accuracy:

2022-01-11 10:55:50,158 - INFO - [0.9614458  0.9737349  0.9626506  0.98289156 0.96457833 0.98626506
 0.9626506  0.9701205  0.9833735  0.9674699  0.9775904  0.9696385
 0.9506024  0.95349395 0.96       0.9725301  0.9884337 ]
2022-01-11 10:55:50,159 - INFO - Epoch time: 454.77068042755127
2022-01-11 10:55:50,159 - INFO - 
Epoch: 78
2022-01-11 10:55:50,159 - INFO - 
Learning Rate: 0.0010
2022-01-11 10:56:55,931 - INFO - [Step=65250]	Loss=1.9255	223.3 examples/second
2022-01-11 10:59:09,373 - INFO - [Step=65500]	Loss=1.9328	239.8 examples/second
2022-01-11 11:01:22,842 - INFO - [Step=65750]	Loss=1.9384	239.8 examples/second
2022-01-11 11:03:25,139 - INFO - Test Loss=1.7429, Test top-1 acc=0.2935
2022-01-11 11:03:25,139 - INFO - Group Accuracy:

2022-01-11 11:03:25,140 - INFO - [0.9624096  0.97445786 0.9626506  0.9826506  0.96433735 0.9879518
 0.9631325  0.9703615  0.98313254 0.96698797 0.9780723  0.9708434
 0.9498795  0.9546988  0.96072286 0.97228914 0.98746985]
2022-01-11 11:03:25,140 - INFO - Epoch time: 454.9808828830719
2022-01-11 11:03:25,140 - INFO - 
Epoch: 79
2022-01-11 11:03:25,140 - INFO - 
Learning Rate: 0.0010
2022-01-11 11:03:45,876 - INFO - [Step=66000]	Loss=1.9124	223.7 examples/second
2022-01-11 11:05:59,009 - INFO - [Step=66250]	Loss=1.9251	240.4 examples/second
2022-01-11 11:08:12,405 - INFO - [Step=66500]	Loss=1.9397	239.9 examples/second
2022-01-11 11:10:24,832 - INFO - [Step=66750]	Loss=1.9175	241.6 examples/second
2022-01-11 11:10:59,885 - INFO - Test Loss=1.7335, Test top-1 acc=0.2904
2022-01-11 11:10:59,886 - INFO - Group Accuracy:

2022-01-11 11:10:59,886 - INFO - [0.96168673 0.9739759  0.9624096  0.9826506  0.96385545 0.98746985
 0.9624096  0.9706024  0.98240966 0.9674699  0.9787952  0.9708434
 0.94939756 0.9539759  0.9592771  0.9725301  0.9886747 ]
2022-01-11 11:10:59,887 - INFO - Epoch time: 454.74667382240295
2022-01-11 11:10:59,887 - INFO - 
Epoch: 80
2022-01-11 11:10:59,887 - INFO - 
Learning Rate: 0.0010
2022-01-11 11:12:49,591 - INFO - [Step=67000]	Loss=1.9278	221.1 examples/second
2022-01-11 11:15:01,380 - INFO - [Step=67250]	Loss=1.9343	242.8 examples/second
2022-01-11 11:17:13,584 - INFO - [Step=67500]	Loss=1.9085	242.1 examples/second
2022-01-11 11:18:34,153 - INFO - Test Loss=1.7355, Test top-1 acc=0.2908
2022-01-11 11:18:34,154 - INFO - Group Accuracy:

2022-01-11 11:18:34,154 - INFO - [0.9621687  0.9737349  0.9628916  0.9821687  0.96457833 0.98746985
 0.9626506  0.9701205  0.98289156 0.9674699  0.97975904 0.97108436
 0.94915664 0.9539759  0.9587952  0.9727711  0.9879518 ]
2022-01-11 11:18:34,155 - INFO - Epoch time: 454.26749086380005
2022-01-11 11:18:34,155 - INFO - 
Epoch: 81
2022-01-11 11:18:34,155 - INFO - 
Learning Rate: 0.0010
2022-01-11 11:19:37,969 - INFO - [Step=67750]	Loss=1.8989	221.6 examples/second
2022-01-11 11:21:49,805 - INFO - [Step=68000]	Loss=1.9231	242.7 examples/second
2022-01-11 11:24:01,743 - INFO - [Step=68250]	Loss=1.9084	242.5 examples/second
2022-01-11 11:26:07,253 - INFO - Test Loss=1.7294, Test top-1 acc=0.2973
2022-01-11 11:26:07,254 - INFO - Group Accuracy:

2022-01-11 11:26:07,254 - INFO - [0.9626506  0.97566265 0.9621687  0.98313254 0.96433735 0.9879518
 0.9633735  0.97156626 0.98313254 0.96771085 0.9785542  0.9693976
 0.9506024  0.95325303 0.96096385 0.973494   0.9881928 ]
2022-01-11 11:26:07,255 - INFO - Epoch time: 453.1001431941986
2022-01-11 11:26:07,255 - INFO - 
Epoch: 82
2022-01-11 11:26:07,255 - INFO - 
Learning Rate: 0.0010
2022-01-11 11:26:25,345 - INFO - [Step=68500]	Loss=1.9136	222.8 examples/second
2022-01-11 11:28:35,537 - INFO - [Step=68750]	Loss=1.8983	245.8 examples/second
2022-01-11 11:30:45,177 - INFO - [Step=69000]	Loss=1.9126	246.8 examples/second
2022-01-11 11:32:55,576 - INFO - [Step=69250]	Loss=1.9127	245.4 examples/second
2022-01-11 11:33:32,321 - INFO - Test Loss=1.7299, Test top-1 acc=0.2942
2022-01-11 11:33:32,322 - INFO - Group Accuracy:

2022-01-11 11:33:32,331 - INFO - [0.9621687  0.97493976 0.9624096  0.98289156 0.9653012  0.98771083
 0.9612048  0.9713253  0.9833735  0.9674699  0.9778313  0.9698795
 0.94963855 0.95301205 0.95975906 0.97204816 0.9889157 ]
2022-01-11 11:33:32,332 - INFO - Epoch time: 445.07681584358215
2022-01-11 11:33:32,332 - INFO - 
Epoch: 83
2022-01-11 11:33:32,332 - INFO - 
Learning Rate: 0.0010
2022-01-11 11:35:16,048 - INFO - [Step=69500]	Loss=1.9068	227.8 examples/second
2022-01-11 11:37:26,091 - INFO - [Step=69750]	Loss=1.9147	246.1 examples/second
2022-01-11 11:39:36,977 - INFO - [Step=70000]	Loss=1.9171	244.5 examples/second
2022-01-11 11:40:58,414 - INFO - Test Loss=1.7250, Test top-1 acc=0.2976
2022-01-11 11:40:58,415 - INFO - Group Accuracy:

2022-01-11 11:40:58,415 - INFO - [0.9619277  0.97542167 0.9626506  0.9826506  0.96481925 0.9879518
 0.9631325  0.9701205  0.9826506  0.9674699  0.9780723  0.9703615
 0.94891566 0.95373493 0.9595181  0.97180724 0.9886747 ]
2022-01-11 11:40:58,415 - INFO - Epoch time: 446.0836658477783
2022-01-11 11:40:58,416 - INFO - 
Epoch: 84
2022-01-11 11:40:58,416 - INFO - 
Learning Rate: 0.0010
2022-01-11 11:41:57,659 - INFO - [Step=70250]	Loss=1.9056	227.5 examples/second
2022-01-11 11:44:07,657 - INFO - [Step=70500]	Loss=1.9092	246.2 examples/second
2022-01-11 11:46:19,061 - INFO - [Step=70750]	Loss=1.9258	243.5 examples/second
2022-01-11 11:48:23,333 - INFO - Test Loss=1.7150, Test top-1 acc=0.2937
2022-01-11 11:48:23,334 - INFO - Group Accuracy:

2022-01-11 11:48:23,334 - INFO - [0.9612048  0.97542167 0.9624096  0.98240966 0.96409637 0.9893976
 0.9633735  0.9727711  0.98361444 0.966506   0.9778313  0.9701205
 0.9498795  0.9549398  0.96024096 0.97180724 0.9884337 ]
2022-01-11 11:48:23,334 - INFO - Epoch time: 444.9188597202301
2022-01-11 11:48:23,334 - INFO - 
Epoch: 85
2022-01-11 11:48:23,334 - INFO - 
Learning Rate: 0.0010
2022-01-11 11:48:38,469 - INFO - [Step=71000]	Loss=1.9159	229.5 examples/second
2022-01-11 11:50:47,388 - INFO - [Step=71250]	Loss=1.9001	248.2 examples/second
2022-01-11 11:52:56,240 - INFO - [Step=71500]	Loss=1.8919	248.3 examples/second
2022-01-11 11:55:04,917 - INFO - [Step=71750]	Loss=1.9108	248.7 examples/second
2022-01-11 11:55:43,653 - INFO - Test Loss=1.7167, Test top-1 acc=0.2925
2022-01-11 11:55:43,653 - INFO - Group Accuracy:

2022-01-11 11:55:43,653 - INFO - [0.9619277  0.9739759  0.9633735  0.98289156 0.9633735  0.9889157
 0.9626506  0.9706024  0.98313254 0.9660241  0.9766265  0.9698795
 0.9501205  0.9539759  0.96072286 0.97301203 0.9886747 ]
2022-01-11 11:55:43,654 - INFO - Epoch time: 440.3197944164276
2022-01-11 11:55:43,654 - INFO - 
Epoch: 86
2022-01-11 11:55:43,654 - INFO - 
Learning Rate: 0.0010
2022-01-11 11:57:23,266 - INFO - [Step=72000]	Loss=1.9330	231.3 examples/second
2022-01-11 11:59:32,529 - INFO - [Step=72250]	Loss=1.9103	247.6 examples/second
2022-01-11 12:01:42,723 - INFO - [Step=72500]	Loss=1.8911	245.8 examples/second
2022-01-11 12:03:07,123 - INFO - Test Loss=1.7240, Test top-1 acc=0.2945
2022-01-11 12:03:07,124 - INFO - Group Accuracy:

2022-01-11 12:03:07,124 - INFO - [0.96168673 0.973253   0.96168673 0.98313254 0.96481925 0.9879518
 0.9626506  0.9713253  0.9833735  0.9674699  0.9773494  0.9708434
 0.94963855 0.95421684 0.96       0.97228914 0.98771083]
2022-01-11 12:03:07,125 - INFO - Epoch time: 443.47078037261963
2022-01-11 12:03:07,125 - INFO - 
Epoch: 87
2022-01-11 12:03:07,125 - INFO - 
Learning Rate: 0.0010
2022-01-11 12:04:03,788 - INFO - [Step=72750]	Loss=1.9085	226.8 examples/second
2022-01-11 12:06:13,638 - INFO - [Step=73000]	Loss=1.9132	246.4 examples/second
2022-01-11 12:08:22,407 - INFO - [Step=73250]	Loss=1.8975	248.5 examples/second
2022-01-11 12:10:29,807 - INFO - Test Loss=1.7123, Test top-1 acc=0.2976
2022-01-11 12:10:29,807 - INFO - Group Accuracy:

2022-01-11 12:10:29,807 - INFO - [0.9624096  0.9746988  0.96433735 0.98289156 0.9633735  0.9881928
 0.9624096  0.97108436 0.98289156 0.96795183 0.9773494  0.97108436
 0.95036143 0.9544578  0.96024096 0.97301203 0.9889157 ]
2022-01-11 12:10:29,808 - INFO - Epoch time: 442.6830298900604
2022-01-11 12:10:29,808 - INFO - 
Epoch: 88
2022-01-11 12:10:29,808 - INFO - 
Learning Rate: 0.0010
2022-01-11 12:10:42,256 - INFO - [Step=73500]	Loss=1.8955	228.8 examples/second
2022-01-11 12:12:50,843 - INFO - [Step=73750]	Loss=1.9098	248.9 examples/second
2022-01-11 12:14:59,391 - INFO - [Step=74000]	Loss=1.9062	248.9 examples/second
2022-01-11 12:17:08,505 - INFO - [Step=74250]	Loss=1.8938	247.8 examples/second
2022-01-11 12:17:49,765 - INFO - Test Loss=1.7095, Test top-1 acc=0.2942
2022-01-11 12:17:49,765 - INFO - Group Accuracy:

2022-01-11 12:17:49,765 - INFO - [0.9626506  0.97518075 0.96433735 0.98313254 0.96506023 0.9884337
 0.9631325  0.97156626 0.98361444 0.966747   0.9775904  0.9708434
 0.9506024  0.95421684 0.96024096 0.9737349  0.9881928 ]
2022-01-11 12:17:49,766 - INFO - Epoch time: 439.95734667778015
2022-01-11 12:17:49,766 - INFO - 
Epoch: 89
2022-01-11 12:17:49,766 - INFO - 
Learning Rate: 0.0010
2022-01-11 12:19:26,869 - INFO - [Step=74500]	Loss=1.9207	231.3 examples/second
2022-01-11 12:21:34,958 - INFO - [Step=74750]	Loss=1.9111	249.8 examples/second
2022-01-11 12:23:43,593 - INFO - [Step=75000]	Loss=1.8987	248.8 examples/second
2022-01-11 12:25:09,025 - INFO - Test Loss=1.7180, Test top-1 acc=0.2918
2022-01-11 12:25:09,025 - INFO - Group Accuracy:

2022-01-11 12:25:09,025 - INFO - [0.9612048  0.97445786 0.9626506  0.9826506  0.96506023 0.9881928
 0.9628916  0.9696385  0.98361444 0.9662651  0.9778313  0.9691566
 0.9501205  0.9539759  0.9612048  0.97301203 0.9881928 ]
2022-01-11 12:25:09,027 - INFO - Epoch time: 439.26080083847046
2022-01-11 12:25:19,670 - INFO - Computing OOD Statistics...
2022-01-11 12:25:19,682 - INFO - 	Baseline.          AUROC: 0.8352. TNR@95TPR: 0.3600. AUPR OUT: 0.4971
2022-01-11 12:25:19,687 - INFO - 	ODIN (T=1000).     AUROC: 0.8806. TNR@95TPR: 0.3929. AUPR OUT: 0.5558
2022-01-11 12:25:19,687 - INFO - Printing Final Accuracy + OOD Detection stats
2022-01-11 12:25:19,688 - INFO - Top 1 Accuracy:  Min: 0.4060; Max: 0.4060; Avg: 0.4060; Std: 0.0000; Len: 1
2022-01-11 12:25:19,688 - INFO - Top 5 Accuracy:  Min: 0.9550; Max: 0.9550; Avg: 0.9550; Std: 0.0000; Len: 1
2022-01-11 12:25:19,688 - INFO - **********************************************************************
2022-01-11 12:25:19,688 - INFO - 	MSP (auroc): [0.8351909284195607] Min: 0.8352; Max: 0.8352; Avg: 0.8352; Std: 0.0000; Len: 1
2022-01-11 12:25:19,688 - INFO - 	MSP (tnr): [0.36] Min: 0.3600; Max: 0.3600; Avg: 0.3600; Std: 0.0000; Len: 1
2022-01-11 12:25:19,688 - INFO - 	MSP (aupr): [0.49714169512926365] Min: 0.4971; Max: 0.4971; Avg: 0.4971; Std: 0.0000; Len: 1
2022-01-11 12:25:19,688 - INFO - 	ODIN (auroc): [0.8806296243798725] Min: 0.8806; Max: 0.8806; Avg: 0.8806; Std: 0.0000; Len: 1
2022-01-11 12:25:19,688 - INFO - 	ODIN (tnr): [0.39294117647058824] Min: 0.3929; Max: 0.3929; Avg: 0.3929; Std: 0.0000; Len: 1
2022-01-11 12:25:19,688 - INFO - 	ODIN (aupr): [0.5557886145743215] Min: 0.5558; Max: 0.5558; Avg: 0.5558; Std: 0.0000; Len: 1
