2022-01-11 14:05:57,529 - INFO - ==> Preparing data..
2022-01-11 14:05:57,889 - INFO - checkpoint filename: experiments/coarse/mos/FC3_LRp1_R0/checkpoint.pt
2022-01-11 14:05:57,889 - INFO - log filename: experiments/coarse/mos/FC3_LRp1_R0/train.log
2022-01-11 14:05:57,889 - INFO - ********************************************************
2022-01-11 14:05:57,889 - INFO - Starting Iter: 0 / 1
2022-01-11 14:05:57,889 - INFO - ********************************************************
2022-01-11 14:06:01,048 - INFO - 
Epoch: 0
2022-01-11 14:06:01,048 - INFO - 
Learning Rate: 0.0100
2022-01-11 14:07:59,709 - INFO - [Step=250]	Loss=7.3026	269.7 examples/second
2022-01-11 14:09:56,343 - INFO - [Step=500]	Loss=5.4696	274.4 examples/second
2022-01-11 14:11:53,286 - INFO - [Step=750]	Loss=5.3071	273.6 examples/second
2022-01-11 14:12:41,183 - INFO - Test Loss=5.1259, Test top-1 acc=0.0451
2022-01-11 14:12:41,183 - INFO - Group Accuracy:

2022-01-11 14:12:41,183 - INFO - [0.939759  0.939759  0.939759  0.939759  0.939759  0.939759  0.939759
 0.94      0.939759  0.939759  0.9518072 0.939759  0.939759  0.939759
 0.939759  0.939759  0.9518072]
2022-01-11 14:12:41,184 - INFO - Saving...
2022-01-11 14:12:41,331 - INFO - Epoch time: 400.2829444408417
2022-01-11 14:12:41,331 - INFO - 
Epoch: 1
2022-01-11 14:12:41,331 - INFO - 
Learning Rate: 0.0280
2022-01-11 14:14:00,718 - INFO - [Step=1000]	Loss=5.1963	251.1 examples/second
2022-01-11 14:15:57,437 - INFO - [Step=1250]	Loss=4.9560	274.2 examples/second
2022-01-11 14:17:54,082 - INFO - [Step=1500]	Loss=4.7513	274.3 examples/second
2022-01-11 14:19:21,548 - INFO - Test Loss=4.4741, Test top-1 acc=0.0651
2022-01-11 14:19:21,548 - INFO - Group Accuracy:

2022-01-11 14:19:21,548 - INFO - [0.939759   0.9395181  0.939759   0.94216865 0.939759   0.94240963
 0.93903613 0.939759   0.939759   0.939759   0.9518072  0.939759
 0.939759   0.939759   0.939759   0.939759   0.9518072 ]
2022-01-11 14:19:21,549 - INFO - Saving...
2022-01-11 14:19:21,815 - INFO - Epoch time: 400.48395919799805
2022-01-11 14:19:21,815 - INFO - 
Epoch: 2
2022-01-11 14:19:21,815 - INFO - 
Learning Rate: 0.0460
2022-01-11 14:20:01,288 - INFO - [Step=1750]	Loss=4.6030	251.6 examples/second
2022-01-11 14:21:58,065 - INFO - [Step=2000]	Loss=4.4768	274.0 examples/second
2022-01-11 14:23:54,556 - INFO - [Step=2250]	Loss=4.2878	274.7 examples/second
2022-01-11 14:25:51,377 - INFO - [Step=2500]	Loss=4.0892	273.9 examples/second
2022-01-11 14:26:01,568 - INFO - Test Loss=3.9365, Test top-1 acc=0.1439
2022-01-11 14:26:01,569 - INFO - Group Accuracy:

2022-01-11 14:26:01,569 - INFO - [0.93903613 0.94096386 0.94       0.94939756 0.940241   0.946506
 0.9433735  0.9433735  0.9407229  0.939759   0.95277107 0.9404819
 0.9395181  0.939759   0.9404819  0.94192773 0.9510843 ]
2022-01-11 14:26:01,570 - INFO - Saving...
2022-01-11 14:26:01,824 - INFO - Epoch time: 400.0089898109436
2022-01-11 14:26:01,824 - INFO - 
Epoch: 3
2022-01-11 14:26:01,824 - INFO - 
Learning Rate: 0.0640
2022-01-11 14:27:58,282 - INFO - [Step=2750]	Loss=4.0151	252.2 examples/second
2022-01-11 14:29:55,166 - INFO - [Step=3000]	Loss=3.8667	273.8 examples/second
2022-01-11 14:31:52,087 - INFO - [Step=3250]	Loss=3.6880	273.7 examples/second
2022-01-11 14:32:41,851 - INFO - Test Loss=6.4102, Test top-1 acc=0.1814
2022-01-11 14:32:41,851 - INFO - Group Accuracy:

2022-01-11 14:32:41,851 - INFO - [0.94433737 0.9477109  0.940241   0.9580723  0.9448193  0.9573494
 0.9440964  0.9327711  0.95421684 0.9426506  0.95228916 0.9431325
 0.93783134 0.9392771  0.9387952  0.94433737 0.9626506 ]
2022-01-11 14:32:41,852 - INFO - Saving...
2022-01-11 14:32:42,065 - INFO - Epoch time: 400.24120116233826
2022-01-11 14:32:42,065 - INFO - 
Epoch: 4
2022-01-11 14:32:42,066 - INFO - 
Learning Rate: 0.1000
2022-01-11 14:33:59,412 - INFO - [Step=3500]	Loss=3.6454	251.3 examples/second
2022-01-11 14:35:56,148 - INFO - [Step=3750]	Loss=3.5305	274.1 examples/second
2022-01-11 14:37:52,984 - INFO - [Step=4000]	Loss=3.4761	273.9 examples/second
2022-01-11 14:39:22,371 - INFO - Test Loss=3.4709, Test top-1 acc=0.2436
2022-01-11 14:39:22,372 - INFO - Group Accuracy:

2022-01-11 14:39:22,372 - INFO - [0.94506025 0.9554217  0.94120485 0.95710844 0.9436145  0.9626506
 0.9313253  0.9479518  0.9501205  0.9407229  0.9520482  0.93783134
 0.9392771  0.94120485 0.94120485 0.9498795  0.94939756]
2022-01-11 14:39:22,373 - INFO - Saving...
2022-01-11 14:39:22,643 - INFO - Epoch time: 400.5773479938507
2022-01-11 14:39:22,643 - INFO - 
Epoch: 5
2022-01-11 14:39:22,643 - INFO - 
Learning Rate: 0.1000
2022-01-11 14:40:00,002 - INFO - [Step=4250]	Loss=3.2949	251.9 examples/second
2022-01-11 14:41:57,071 - INFO - [Step=4500]	Loss=3.1318	273.3 examples/second
2022-01-11 14:43:53,758 - INFO - [Step=4750]	Loss=3.0505	274.2 examples/second
2022-01-11 14:45:50,562 - INFO - [Step=5000]	Loss=2.9510	274.0 examples/second
2022-01-11 14:46:02,864 - INFO - Test Loss=2.9335, Test top-1 acc=0.3308
2022-01-11 14:46:02,864 - INFO - Group Accuracy:

2022-01-11 14:46:02,864 - INFO - [0.95325303 0.95301205 0.9433735  0.9662651  0.9469879  0.97108436
 0.9279518  0.94433737 0.96361446 0.9486747  0.9612048  0.94626504
 0.94240963 0.9433735  0.94578314 0.9518072  0.973253  ]
2022-01-11 14:46:02,865 - INFO - Saving...
2022-01-11 14:46:03,127 - INFO - Epoch time: 400.48385524749756
2022-01-11 14:46:03,127 - INFO - 
Epoch: 6
2022-01-11 14:46:03,127 - INFO - 
Learning Rate: 0.1000
2022-01-11 14:47:57,906 - INFO - [Step=5250]	Loss=2.8724	251.3 examples/second
2022-01-11 14:49:54,606 - INFO - [Step=5500]	Loss=2.7790	274.2 examples/second
2022-01-11 14:51:51,623 - INFO - [Step=5750]	Loss=2.7317	273.5 examples/second
2022-01-11 14:52:43,818 - INFO - Test Loss=4.3531, Test top-1 acc=0.3393
2022-01-11 14:52:43,818 - INFO - Group Accuracy:

2022-01-11 14:52:43,818 - INFO - [0.94192773 0.96       0.946747   0.966506   0.95228916 0.973494
 0.94240963 0.9515663  0.96072286 0.9433735  0.95975906 0.9498795
 0.9431325  0.93831325 0.9479518  0.9559036  0.9746988 ]
2022-01-11 14:52:43,819 - INFO - Saving...
2022-01-11 14:52:44,089 - INFO - Epoch time: 400.96155166625977
2022-01-11 14:52:44,089 - INFO - 
Epoch: 7
2022-01-11 14:52:44,089 - INFO - 
Learning Rate: 0.1000
2022-01-11 14:53:58,949 - INFO - [Step=6000]	Loss=2.6328	251.3 examples/second
2022-01-11 14:55:55,847 - INFO - [Step=6250]	Loss=2.5743	273.7 examples/second
2022-01-11 14:57:52,772 - INFO - [Step=6500]	Loss=2.5610	273.7 examples/second
2022-01-11 14:59:25,129 - INFO - Test Loss=2.6665, Test top-1 acc=0.3566
2022-01-11 14:59:25,130 - INFO - Group Accuracy:

2022-01-11 14:59:25,130 - INFO - [0.9513253  0.9590362  0.9445783  0.96698797 0.95036143 0.97445786
 0.94554216 0.9578313  0.966747   0.95228916 0.9595181  0.9546988
 0.9453012  0.9440964  0.94843376 0.95421684 0.97927713]
2022-01-11 14:59:25,130 - INFO - Saving...
2022-01-11 14:59:25,395 - INFO - Epoch time: 401.3058876991272
2022-01-11 14:59:25,395 - INFO - 
Epoch: 8
2022-01-11 14:59:25,395 - INFO - 
Learning Rate: 0.1000
2022-01-11 15:00:00,533 - INFO - [Step=6750]	Loss=2.4688	250.5 examples/second
2022-01-11 15:01:57,273 - INFO - [Step=7000]	Loss=2.4163	274.1 examples/second
2022-01-11 15:03:54,223 - INFO - [Step=7250]	Loss=2.3705	273.6 examples/second
2022-01-11 15:05:51,074 - INFO - [Step=7500]	Loss=2.3137	273.9 examples/second
2022-01-11 15:06:06,204 - INFO - Test Loss=2.3552, Test top-1 acc=0.4318
2022-01-11 15:06:06,205 - INFO - Group Accuracy:

2022-01-11 15:06:06,205 - INFO - [0.9539759  0.9592771  0.94289154 0.9703615  0.9583132  0.9773494
 0.95301205 0.9626506  0.9696385  0.95373493 0.966747   0.9561446
 0.9472289  0.946747   0.95301205 0.9624096  0.9775904 ]
2022-01-11 15:06:06,205 - INFO - Saving...
2022-01-11 15:06:06,440 - INFO - Epoch time: 401.04518246650696
2022-01-11 15:06:06,440 - INFO - 
Epoch: 9
2022-01-11 15:06:06,441 - INFO - 
Learning Rate: 0.1000
2022-01-11 15:07:58,749 - INFO - [Step=7750]	Loss=2.2426	250.6 examples/second
2022-01-11 15:09:55,622 - INFO - [Step=8000]	Loss=2.2207	273.8 examples/second
2022-01-11 15:11:52,432 - INFO - [Step=8250]	Loss=2.1939	273.9 examples/second
2022-01-11 15:12:46,979 - INFO - Test Loss=2.0662, Test top-1 acc=0.4598
2022-01-11 15:12:46,979 - INFO - Group Accuracy:

2022-01-11 15:12:46,979 - INFO - [0.9573494  0.96771085 0.95662653 0.97614455 0.9590362  0.9840964
 0.9539759  0.9653012  0.97614455 0.96       0.96433735 0.9585542
 0.94963855 0.9506024  0.94963855 0.9672289  0.98361444]
2022-01-11 15:12:46,980 - INFO - Saving...
2022-01-11 15:12:47,241 - INFO - Epoch time: 400.8008620738983
2022-01-11 15:12:47,242 - INFO - 
Epoch: 10
2022-01-11 15:12:47,242 - INFO - 
Learning Rate: 0.1000
2022-01-11 15:13:59,616 - INFO - [Step=8500]	Loss=2.1262	251.6 examples/second
2022-01-11 15:15:56,847 - INFO - [Step=8750]	Loss=2.0989	273.0 examples/second
2022-01-11 15:17:53,824 - INFO - [Step=9000]	Loss=2.0795	273.6 examples/second
2022-01-11 15:19:28,226 - INFO - Test Loss=2.3595, Test top-1 acc=0.4735
2022-01-11 15:19:28,226 - INFO - Group Accuracy:

2022-01-11 15:19:28,226 - INFO - [0.95686746 0.9662651  0.95638555 0.98024094 0.96096385 0.98
 0.9587952  0.9621687  0.96771085 0.9619277  0.96795183 0.95566267
 0.94626504 0.9515663  0.95277107 0.96385545 0.9833735 ]
2022-01-11 15:19:28,227 - INFO - Saving...
2022-01-11 15:19:28,485 - INFO - Epoch time: 401.24349999427795
2022-01-11 15:19:28,485 - INFO - 
Epoch: 11
2022-01-11 15:19:28,485 - INFO - 
Learning Rate: 0.1000
2022-01-11 15:20:01,215 - INFO - [Step=9250]	Loss=2.0524	251.2 examples/second
2022-01-11 15:21:58,282 - INFO - [Step=9500]	Loss=2.0147	273.3 examples/second
2022-01-11 15:23:54,914 - INFO - [Step=9750]	Loss=1.9956	274.4 examples/second
2022-01-11 15:25:51,484 - INFO - [Step=10000]	Loss=1.9776	274.5 examples/second
2022-01-11 15:26:08,637 - INFO - Test Loss=1.9562, Test top-1 acc=0.4829
2022-01-11 15:26:08,637 - INFO - Group Accuracy:

2022-01-11 15:26:08,637 - INFO - [0.9660241  0.96891564 0.94506025 0.9821687  0.9633735  0.9812048
 0.95975906 0.96361446 0.9766265  0.9657831  0.97204816 0.9672289
 0.9481928  0.95228916 0.9561446  0.96891564 0.9812048 ]
2022-01-11 15:26:08,638 - INFO - Saving...
2022-01-11 15:26:08,877 - INFO - Epoch time: 400.3918082714081
2022-01-11 15:26:08,877 - INFO - 
Epoch: 12
2022-01-11 15:26:08,877 - INFO - 
Learning Rate: 0.1000
2022-01-11 15:27:58,469 - INFO - [Step=10250]	Loss=1.9199	252.0 examples/second
2022-01-11 15:29:55,226 - INFO - [Step=10500]	Loss=1.9335	274.1 examples/second
2022-01-11 15:31:51,860 - INFO - [Step=10750]	Loss=1.9036	274.4 examples/second
2022-01-11 15:32:48,527 - INFO - Test Loss=1.9712, Test top-1 acc=0.5065
2022-01-11 15:32:48,527 - INFO - Group Accuracy:

2022-01-11 15:32:48,527 - INFO - [0.96481925 0.973253   0.9624096  0.97927713 0.9592771  0.9845783
 0.9614458  0.9592771  0.9787952  0.96361446 0.9701205  0.96361446
 0.94554216 0.9508434  0.95710844 0.96843374 0.97903615]
2022-01-11 15:32:48,528 - INFO - Saving...
2022-01-11 15:32:48,792 - INFO - Epoch time: 399.91466879844666
2022-01-11 15:32:48,792 - INFO - 
Epoch: 13
2022-01-11 15:32:48,792 - INFO - 
Learning Rate: 0.1000
2022-01-11 15:33:58,654 - INFO - [Step=11000]	Loss=1.8613	252.4 examples/second
2022-01-11 15:35:55,327 - INFO - [Step=11250]	Loss=1.8657	274.3 examples/second
2022-01-11 15:37:52,488 - INFO - [Step=11500]	Loss=1.8370	273.1 examples/second
2022-01-11 15:39:29,253 - INFO - Test Loss=1.7542, Test top-1 acc=0.5306
2022-01-11 15:39:29,254 - INFO - Group Accuracy:

2022-01-11 15:39:29,254 - INFO - [0.96385545 0.9773494  0.96168673 0.97927713 0.9624096  0.98722893
 0.9518072  0.9746988  0.97903615 0.9660241  0.97975904 0.9698795
 0.9539759  0.95253015 0.96048194 0.96795183 0.98626506]
2022-01-11 15:39:29,254 - INFO - Saving...
2022-01-11 15:39:29,488 - INFO - Epoch time: 400.6956901550293
2022-01-11 15:39:29,488 - INFO - 
Epoch: 14
2022-01-11 15:39:29,488 - INFO - 
Learning Rate: 0.1000
2022-01-11 15:39:59,711 - INFO - [Step=11750]	Loss=1.8357	251.5 examples/second
2022-01-11 15:41:56,283 - INFO - [Step=12000]	Loss=1.7988	274.5 examples/second
2022-01-11 15:43:53,486 - INFO - [Step=12250]	Loss=1.7816	273.0 examples/second
2022-01-11 15:45:50,508 - INFO - [Step=12500]	Loss=1.7961	273.5 examples/second
2022-01-11 15:46:09,694 - INFO - Test Loss=1.8428, Test top-1 acc=0.5424
2022-01-11 15:46:09,694 - INFO - Group Accuracy:

2022-01-11 15:46:09,694 - INFO - [0.96048194 0.97518075 0.96457833 0.98240966 0.96433735 0.98578316
 0.96385545 0.96843374 0.9775904  0.9561446  0.9727711  0.9612048
 0.9551807  0.9544578  0.9626506  0.9686747  0.9848193 ]
2022-01-11 15:46:09,695 - INFO - Saving...
2022-01-11 15:46:09,957 - INFO - Epoch time: 400.46928453445435
2022-01-11 15:46:09,958 - INFO - 
Epoch: 15
2022-01-11 15:46:09,958 - INFO - 
Learning Rate: 0.1000
2022-01-11 15:47:57,345 - INFO - [Step=12750]	Loss=1.7351	252.3 examples/second
2022-01-11 15:49:54,218 - INFO - [Step=13000]	Loss=1.7238	273.8 examples/second
2022-01-11 15:51:51,194 - INFO - [Step=13250]	Loss=1.7347	273.6 examples/second
2022-01-11 15:52:50,435 - INFO - Test Loss=1.6636, Test top-1 acc=0.5759
2022-01-11 15:52:50,436 - INFO - Group Accuracy:

2022-01-11 15:52:50,436 - INFO - [0.96771085 0.97903615 0.96385545 0.9819277  0.9657831  0.98433733
 0.9657831  0.9633735  0.9775904  0.96698797 0.9780723  0.9725301
 0.9498795  0.96       0.96096385 0.9708434  0.9893976 ]
2022-01-11 15:52:50,436 - INFO - Saving...
2022-01-11 15:52:50,676 - INFO - Epoch time: 400.71850061416626
2022-01-11 15:52:50,676 - INFO - 
Epoch: 16
2022-01-11 15:52:50,676 - INFO - 
Learning Rate: 0.1000
2022-01-11 15:53:58,329 - INFO - [Step=13500]	Loss=1.7250	251.7 examples/second
2022-01-11 15:55:55,226 - INFO - [Step=13750]	Loss=1.6857	273.7 examples/second
2022-01-11 15:57:52,045 - INFO - [Step=14000]	Loss=1.6972	273.9 examples/second
2022-01-11 15:59:31,240 - INFO - Test Loss=2.2327, Test top-1 acc=0.5381
2022-01-11 15:59:31,240 - INFO - Group Accuracy:

2022-01-11 15:59:31,240 - INFO - [0.9592771  0.9706024  0.9590362  0.9807229  0.9621687  0.98626506
 0.9595181  0.96771085 0.9766265  0.96457833 0.9739759  0.96698797
 0.94963855 0.95710844 0.95759034 0.97156626 0.98578316]
2022-01-11 15:59:31,241 - INFO - Epoch time: 400.56484961509705
2022-01-11 15:59:31,241 - INFO - 
Epoch: 17
2022-01-11 15:59:31,241 - INFO - 
Learning Rate: 0.1000
2022-01-11 15:59:59,214 - INFO - [Step=14250]	Loss=1.6805	251.6 examples/second
2022-01-11 16:01:55,964 - INFO - [Step=14500]	Loss=1.6314	274.1 examples/second
2022-01-11 16:03:52,650 - INFO - [Step=14750]	Loss=1.6520	274.2 examples/second
2022-01-11 16:05:49,560 - INFO - [Step=15000]	Loss=1.6587	273.7 examples/second
2022-01-11 16:06:11,362 - INFO - Test Loss=1.9557, Test top-1 acc=0.5388
2022-01-11 16:06:11,362 - INFO - Group Accuracy:

2022-01-11 16:06:11,362 - INFO - [0.95975906 0.97301203 0.96698797 0.9821687  0.9551807  0.9845783
 0.9614458  0.9655422  0.9771084  0.9674699  0.973494   0.96891564
 0.95253015 0.9551807  0.95325303 0.9703615  0.9860241 ]
2022-01-11 16:06:11,363 - INFO - Epoch time: 400.1216707229614
2022-01-11 16:06:11,363 - INFO - 
Epoch: 18
2022-01-11 16:06:11,363 - INFO - 
Learning Rate: 0.1000
2022-01-11 16:07:56,135 - INFO - [Step=15250]	Loss=1.5955	252.8 examples/second
2022-01-11 16:09:52,661 - INFO - [Step=15500]	Loss=1.6020	274.6 examples/second
2022-01-11 16:11:49,586 - INFO - [Step=15750]	Loss=1.6160	273.7 examples/second
2022-01-11 16:12:50,865 - INFO - Test Loss=1.8052, Test top-1 acc=0.5578
2022-01-11 16:12:50,865 - INFO - Group Accuracy:

2022-01-11 16:12:50,865 - INFO - [0.9614458  0.9785542  0.9657831  0.9826506  0.9703615  0.98289156
 0.9660241  0.96506023 0.98024094 0.96891564 0.97228914 0.9592771
 0.9592771  0.96048194 0.9587952  0.9686747  0.98578316]
2022-01-11 16:12:50,866 - INFO - Epoch time: 399.5028076171875
2022-01-11 16:12:50,866 - INFO - 
Epoch: 19
2022-01-11 16:12:50,866 - INFO - 
Learning Rate: 0.1000
2022-01-11 16:13:56,014 - INFO - [Step=16000]	Loss=1.5793	253.1 examples/second
2022-01-11 16:15:52,698 - INFO - [Step=16250]	Loss=1.5830	274.2 examples/second
2022-01-11 16:17:49,647 - INFO - [Step=16500]	Loss=1.5659	273.6 examples/second
2022-01-11 16:19:30,673 - INFO - Test Loss=1.6327, Test top-1 acc=0.5761
2022-01-11 16:19:30,673 - INFO - Group Accuracy:

2022-01-11 16:19:30,674 - INFO - [0.9696385  0.9780723  0.96891564 0.9807229  0.966506   0.98722893
 0.966506   0.9672289  0.97975904 0.966747   0.97301203 0.9693976
 0.9554217  0.95759034 0.9612048  0.97542167 0.9833735 ]
2022-01-11 16:19:30,674 - INFO - Saving...
2022-01-11 16:19:30,903 - INFO - Epoch time: 400.03735303878784
2022-01-11 16:19:30,904 - INFO - 
Epoch: 20
2022-01-11 16:19:30,904 - INFO - 
Learning Rate: 0.1000
2022-01-11 16:19:56,375 - INFO - [Step=16750]	Loss=1.5721	252.5 examples/second
2022-01-11 16:21:53,135 - INFO - [Step=17000]	Loss=1.5393	274.1 examples/second
2022-01-11 16:23:49,986 - INFO - [Step=17250]	Loss=1.5424	273.9 examples/second
2022-01-11 16:25:46,534 - INFO - [Step=17500]	Loss=1.5428	274.6 examples/second
2022-01-11 16:26:10,479 - INFO - Test Loss=1.7468, Test top-1 acc=0.6000
2022-01-11 16:26:10,479 - INFO - Group Accuracy:

2022-01-11 16:26:10,479 - INFO - [0.9672289  0.97903615 0.96891564 0.9848193  0.96819276 0.9884337
 0.9662651  0.96506023 0.9778313  0.9686747  0.9809638  0.9737349
 0.9498795  0.96096385 0.96385545 0.97590363 0.9898795 ]
2022-01-11 16:26:10,480 - INFO - Saving...
2022-01-11 16:26:10,715 - INFO - Epoch time: 399.81137442588806
2022-01-11 16:26:10,715 - INFO - 
Epoch: 21
2022-01-11 16:26:10,715 - INFO - 
Learning Rate: 0.1000
2022-01-11 16:27:53,411 - INFO - [Step=17750]	Loss=1.5021	252.2 examples/second
2022-01-11 16:29:50,366 - INFO - [Step=18000]	Loss=1.5017	273.6 examples/second
2022-01-11 16:31:46,958 - INFO - [Step=18250]	Loss=1.5292	274.5 examples/second
2022-01-11 16:32:50,813 - INFO - Test Loss=1.5782, Test top-1 acc=0.6080
2022-01-11 16:32:50,813 - INFO - Group Accuracy:

2022-01-11 16:32:50,813 - INFO - [0.96771085 0.9737349  0.95710844 0.9893976  0.96819276 0.9889157
 0.9674699  0.97108436 0.9814458  0.96409637 0.97518075 0.9727711
 0.9506024  0.9655422  0.96168673 0.9768675  0.9891566 ]
2022-01-11 16:32:50,814 - INFO - Saving...
2022-01-11 16:32:51,088 - INFO - Epoch time: 400.37267899513245
2022-01-11 16:32:51,088 - INFO - 
Epoch: 22
2022-01-11 16:32:51,088 - INFO - 
Learning Rate: 0.1000
2022-01-11 16:33:54,348 - INFO - [Step=18500]	Loss=1.4959	251.2 examples/second
2022-01-11 16:35:51,021 - INFO - [Step=18750]	Loss=1.4806	274.3 examples/second
2022-01-11 16:37:47,719 - INFO - [Step=19000]	Loss=1.4960	274.2 examples/second
2022-01-11 16:39:30,796 - INFO - Test Loss=1.8751, Test top-1 acc=0.5747
2022-01-11 16:39:30,796 - INFO - Group Accuracy:

2022-01-11 16:39:30,796 - INFO - [0.9626506  0.9474699  0.96409637 0.98240966 0.9696385  0.98698795
 0.96481925 0.9674699  0.9819277  0.9621687  0.9691566  0.96481925
 0.9549398  0.9592771  0.96       0.97518075 0.9901205 ]
2022-01-11 16:39:30,797 - INFO - Epoch time: 399.70926690101624
2022-01-11 16:39:30,797 - INFO - 
Epoch: 23
2022-01-11 16:39:30,797 - INFO - 
Learning Rate: 0.1000
2022-01-11 16:39:53,870 - INFO - [Step=19250]	Loss=1.5007	253.7 examples/second
2022-01-11 16:41:50,372 - INFO - [Step=19500]	Loss=1.4695	274.7 examples/second
2022-01-11 16:43:47,163 - INFO - [Step=19750]	Loss=1.4808	274.0 examples/second
2022-01-11 16:45:43,823 - INFO - [Step=20000]	Loss=1.4855	274.3 examples/second
2022-01-11 16:46:10,190 - INFO - Test Loss=1.6868, Test top-1 acc=0.5863
2022-01-11 16:46:10,190 - INFO - Group Accuracy:

2022-01-11 16:46:10,190 - INFO - [0.9624096  0.97566265 0.9655422  0.9845783  0.9691566  0.9886747
 0.96385545 0.966506   0.9821687  0.97204816 0.9698795  0.973494
 0.9633735  0.9624096  0.9592771  0.97180724 0.98433733]
2022-01-11 16:46:10,191 - INFO - Epoch time: 399.39392924308777
2022-01-11 16:46:10,191 - INFO - 
Epoch: 24
2022-01-11 16:46:10,192 - INFO - 
Learning Rate: 0.1000
2022-01-11 16:47:50,286 - INFO - [Step=20250]	Loss=1.4399	253.0 examples/second
2022-01-11 16:49:46,731 - INFO - [Step=20500]	Loss=1.4475	274.8 examples/second
2022-01-11 16:51:43,383 - INFO - [Step=20750]	Loss=1.4640	274.3 examples/second
2022-01-11 16:52:49,250 - INFO - Test Loss=1.4507, Test top-1 acc=0.6082
2022-01-11 16:52:49,251 - INFO - Group Accuracy:

2022-01-11 16:52:49,251 - INFO - [0.9693976  0.9780723  0.973494   0.98578316 0.9693976  0.9879518
 0.9619277  0.9713253  0.98       0.9703615  0.97927713 0.97493976
 0.9592771  0.96361446 0.96481925 0.97108436 0.9881928 ]
2022-01-11 16:52:49,252 - INFO - Saving...
2022-01-11 16:52:49,501 - INFO - Epoch time: 399.30905199050903
2022-01-11 16:52:49,501 - INFO - 
Epoch: 25
2022-01-11 16:52:49,501 - INFO - 
Learning Rate: 0.1000
2022-01-11 16:53:49,967 - INFO - [Step=21000]	Loss=1.4483	252.8 examples/second
2022-01-11 16:55:46,590 - INFO - [Step=21250]	Loss=1.4207	274.4 examples/second
2022-01-11 16:57:43,137 - INFO - [Step=21500]	Loss=1.4366	274.6 examples/second
2022-01-11 16:59:28,854 - INFO - Test Loss=2.4699, Test top-1 acc=0.5381
2022-01-11 16:59:28,855 - INFO - Group Accuracy:

2022-01-11 16:59:28,855 - INFO - [0.96096385 0.966506   0.9633735  0.9819277  0.9696385  0.9809638
 0.9573494  0.93759036 0.9814458  0.96506023 0.97228914 0.9727711
 0.95710844 0.9506024  0.96072286 0.97301203 0.9889157 ]
2022-01-11 16:59:28,855 - INFO - Epoch time: 399.3545722961426
2022-01-11 16:59:28,855 - INFO - 
Epoch: 26
2022-01-11 16:59:28,855 - INFO - 
Learning Rate: 0.1000
2022-01-11 16:59:49,602 - INFO - [Step=21750]	Loss=1.4388	253.0 examples/second
2022-01-11 17:01:46,286 - INFO - [Step=22000]	Loss=1.4092	274.2 examples/second
2022-01-11 17:03:42,925 - INFO - [Step=22250]	Loss=1.4357	274.4 examples/second
2022-01-11 17:05:39,608 - INFO - [Step=22500]	Loss=1.4182	274.2 examples/second
2022-01-11 17:06:08,544 - INFO - Test Loss=1.4409, Test top-1 acc=0.6296
2022-01-11 17:06:08,544 - INFO - Group Accuracy:

2022-01-11 17:06:08,544 - INFO - [0.97180724 0.9816868  0.96506023 0.98650604 0.97180724 0.98963857
 0.966506   0.9742169  0.9819277  0.9691566  0.973253   0.9696385
 0.96361446 0.96843374 0.96771085 0.97638553 0.9893976 ]
2022-01-11 17:06:08,545 - INFO - Saving...
2022-01-11 17:06:08,777 - INFO - Epoch time: 399.92151737213135
2022-01-11 17:06:08,777 - INFO - 
Epoch: 27
2022-01-11 17:06:08,777 - INFO - 
Learning Rate: 0.1000
2022-01-11 17:07:46,599 - INFO - [Step=22750]	Loss=1.3798	252.0 examples/second
2022-01-11 17:09:43,106 - INFO - [Step=23000]	Loss=1.4076	274.7 examples/second
2022-01-11 17:11:39,466 - INFO - [Step=23250]	Loss=1.4013	275.0 examples/second
2022-01-11 17:12:47,575 - INFO - Test Loss=1.4943, Test top-1 acc=0.6113
2022-01-11 17:12:47,576 - INFO - Group Accuracy:

2022-01-11 17:12:47,576 - INFO - [0.96481925 0.9819277  0.97638553 0.9848193  0.97445786 0.9891566
 0.9614458  0.9655422  0.9814458  0.96698797 0.97903615 0.97156626
 0.9510843  0.966506   0.96481925 0.9768675  0.9884337 ]
2022-01-11 17:12:47,577 - INFO - Epoch time: 398.799542427063
2022-01-11 17:12:47,577 - INFO - 
Epoch: 28
2022-01-11 17:12:47,577 - INFO - 
Learning Rate: 0.1000
2022-01-11 17:13:45,762 - INFO - [Step=23500]	Loss=1.4045	253.4 examples/second
2022-01-11 17:15:42,149 - INFO - [Step=23750]	Loss=1.3735	274.9 examples/second
2022-01-11 17:17:38,563 - INFO - [Step=24000]	Loss=1.3723	274.9 examples/second
2022-01-11 17:19:26,232 - INFO - Test Loss=1.7518, Test top-1 acc=0.5841
2022-01-11 17:19:26,233 - INFO - Group Accuracy:

2022-01-11 17:19:26,233 - INFO - [0.9626506  0.9780723  0.9631325  0.9853012  0.96168673 0.9855422
 0.9614458  0.9737349  0.9845783  0.96843374 0.96891564 0.96072286
 0.9628916  0.9612048  0.96168673 0.9696385  0.98746985]
2022-01-11 17:19:26,234 - INFO - Epoch time: 398.6567327976227
2022-01-11 17:19:26,234 - INFO - 
Epoch: 29
2022-01-11 17:19:26,234 - INFO - 
Learning Rate: 0.0100
2022-01-11 17:19:44,681 - INFO - [Step=24250]	Loss=1.3889	253.7 examples/second
2022-01-11 17:21:41,283 - INFO - [Step=24500]	Loss=1.0618	274.4 examples/second
2022-01-11 17:23:37,826 - INFO - [Step=24750]	Loss=1.0158	274.6 examples/second
2022-01-11 17:25:34,256 - INFO - [Step=25000]	Loss=0.9918	274.8 examples/second
2022-01-11 17:26:05,030 - INFO - Test Loss=0.9071, Test top-1 acc=0.7540
2022-01-11 17:26:05,030 - INFO - Group Accuracy:

2022-01-11 17:26:05,030 - INFO - [0.9768675  0.9901205  0.9845783  0.9920482  0.9804819  0.993494
 0.9771084  0.9819277  0.9891566  0.9771084  0.9898795  0.9840964
 0.9746988  0.9785542  0.97542167 0.9881928  0.9939759 ]
2022-01-11 17:26:05,031 - INFO - Saving...
2022-01-11 17:26:05,280 - INFO - Epoch time: 399.04626846313477
2022-01-11 17:26:05,280 - INFO - 
Epoch: 30
2022-01-11 17:26:05,280 - INFO - 
Learning Rate: 0.0100
2022-01-11 17:27:40,981 - INFO - [Step=25250]	Loss=0.9563	252.5 examples/second
2022-01-11 17:29:37,593 - INFO - [Step=25500]	Loss=0.9480	274.4 examples/second
2022-01-11 17:31:34,316 - INFO - [Step=25750]	Loss=0.9308	274.2 examples/second
2022-01-11 17:32:45,012 - INFO - Test Loss=0.8773, Test top-1 acc=0.7545
2022-01-11 17:32:45,012 - INFO - Group Accuracy:

2022-01-11 17:32:45,012 - INFO - [0.97638553 0.98963857 0.9838554  0.9920482  0.9814458  0.99421686
 0.9785542  0.98313254 0.9901205  0.9773494  0.9886747  0.98433733
 0.97566265 0.97903615 0.97566265 0.9884337  0.99373496]
2022-01-11 17:32:45,013 - INFO - Saving...
2022-01-11 17:32:45,269 - INFO - Epoch time: 399.9885301589966
2022-01-11 17:32:45,269 - INFO - 
Epoch: 31
2022-01-11 17:32:45,269 - INFO - 
Learning Rate: 0.0100
2022-01-11 17:33:41,574 - INFO - [Step=26000]	Loss=0.9199	251.5 examples/second
2022-01-11 17:35:38,084 - INFO - [Step=26250]	Loss=0.8926	274.7 examples/second
2022-01-11 17:37:34,530 - INFO - [Step=26500]	Loss=0.9031	274.8 examples/second
2022-01-11 17:39:24,749 - INFO - Test Loss=0.8628, Test top-1 acc=0.7542
2022-01-11 17:39:24,749 - INFO - Group Accuracy:

2022-01-11 17:39:24,749 - INFO - [0.9778313  0.9898795  0.98361444 0.9918072  0.9816868  0.993494
 0.9775904  0.9833735  0.9893976  0.97638553 0.9898795  0.98433733
 0.9773494  0.9807229  0.97614455 0.9881928  0.993253  ]
2022-01-11 17:39:24,750 - INFO - Epoch time: 399.4813392162323
2022-01-11 17:39:24,750 - INFO - 
Epoch: 32
2022-01-11 17:39:24,750 - INFO - 
Learning Rate: 0.0100
2022-01-11 17:39:41,171 - INFO - [Step=26750]	Loss=0.9057	252.7 examples/second
2022-01-11 17:41:38,566 - INFO - [Step=27000]	Loss=0.8844	272.6 examples/second
2022-01-11 17:43:36,270 - INFO - [Step=27250]	Loss=0.8748	271.9 examples/second
2022-01-11 17:45:33,345 - INFO - [Step=27500]	Loss=0.8637	273.3 examples/second
2022-01-11 17:46:07,000 - INFO - Test Loss=0.8565, Test top-1 acc=0.7581
2022-01-11 17:46:07,000 - INFO - Group Accuracy:

2022-01-11 17:46:07,000 - INFO - [0.97831327 0.9901205  0.9850602  0.9920482  0.98313254 0.99373496
 0.97927713 0.9819277  0.99036145 0.9766265  0.99036145 0.9833735
 0.97566265 0.9819277  0.97638553 0.98746985 0.99373496]
2022-01-11 17:46:07,001 - INFO - Saving...
2022-01-11 17:46:07,258 - INFO - Epoch time: 402.5080246925354
2022-01-11 17:46:07,259 - INFO - 
Epoch: 33
2022-01-11 17:46:07,259 - INFO - 
Learning Rate: 0.0100
2022-01-11 17:47:40,400 - INFO - [Step=27750]	Loss=0.8497	251.9 examples/second
2022-01-11 17:49:37,310 - INFO - [Step=28000]	Loss=0.8490	273.7 examples/second
2022-01-11 17:51:33,987 - INFO - [Step=28250]	Loss=0.8757	274.3 examples/second
2022-01-11 17:52:47,460 - INFO - Test Loss=0.8440, Test top-1 acc=0.7651
2022-01-11 17:52:47,461 - INFO - Group Accuracy:

2022-01-11 17:52:47,461 - INFO - [0.9775904  0.99084336 0.9850602  0.9918072  0.98289156 0.99421686
 0.98024094 0.9826506  0.9898795  0.9778313  0.9898795  0.9845783
 0.9775904  0.97975904 0.97590363 0.9879518  0.99421686]
2022-01-11 17:52:47,462 - INFO - Saving...
2022-01-11 17:52:47,696 - INFO - Epoch time: 400.4373781681061
2022-01-11 17:52:47,696 - INFO - 
Epoch: 34
2022-01-11 17:52:47,696 - INFO - 
Learning Rate: 0.0100
2022-01-11 17:53:41,635 - INFO - [Step=28500]	Loss=0.8546	250.7 examples/second
2022-01-11 17:55:38,071 - INFO - [Step=28750]	Loss=0.8500	274.8 examples/second
2022-01-11 17:57:34,656 - INFO - [Step=29000]	Loss=0.8441	274.5 examples/second
2022-01-11 17:59:28,415 - INFO - Test Loss=0.8498, Test top-1 acc=0.7600
2022-01-11 17:59:28,415 - INFO - Group Accuracy:

2022-01-11 17:59:28,415 - INFO - [0.97638553 0.99060243 0.9845783  0.9922892  0.98289156 0.99445784
 0.9804819  0.9816868  0.99108434 0.9766265  0.9889157  0.9860241
 0.9768675  0.97927713 0.97566265 0.9884337  0.9927711 ]
2022-01-11 17:59:28,416 - INFO - Epoch time: 400.7194867134094
2022-01-11 17:59:28,416 - INFO - 
Epoch: 35
2022-01-11 17:59:28,416 - INFO - 
Learning Rate: 0.0100
2022-01-11 17:59:42,715 - INFO - [Step=29250]	Loss=0.8363	249.9 examples/second
2022-01-11 18:01:39,250 - INFO - [Step=29500]	Loss=0.8124	274.6 examples/second
2022-01-11 18:03:35,835 - INFO - [Step=29750]	Loss=0.8130	274.5 examples/second
2022-01-11 18:05:32,494 - INFO - [Step=30000]	Loss=0.8300	274.3 examples/second
2022-01-11 18:06:07,917 - INFO - Test Loss=0.8355, Test top-1 acc=0.7605
2022-01-11 18:06:07,918 - INFO - Group Accuracy:

2022-01-11 18:06:07,918 - INFO - [0.9780723  0.9898795  0.9850602  0.993494   0.9819277  0.99445784
 0.9780723  0.9826506  0.9898795  0.9771084  0.9889157  0.9845783
 0.9768675  0.9814458  0.9766265  0.9891566  0.9946988 ]
2022-01-11 18:06:07,919 - INFO - Epoch time: 399.5030221939087
2022-01-11 18:06:07,919 - INFO - 
Epoch: 36
2022-01-11 18:06:07,919 - INFO - 
Learning Rate: 0.0100
2022-01-11 18:07:39,185 - INFO - [Step=30250]	Loss=0.8196	252.6 examples/second
2022-01-11 18:09:35,433 - INFO - [Step=30500]	Loss=0.8077	275.3 examples/second
2022-01-11 18:11:31,934 - INFO - [Step=30750]	Loss=0.8232	274.7 examples/second
2022-01-11 18:12:47,472 - INFO - Test Loss=0.8261, Test top-1 acc=0.7692
2022-01-11 18:12:47,472 - INFO - Group Accuracy:

2022-01-11 18:12:47,472 - INFO - [0.9780723  0.99108434 0.9848193  0.99421686 0.9821687  0.99421686
 0.97927713 0.9821687  0.9898795  0.97638553 0.99084336 0.9850602
 0.97614455 0.9814458  0.9780723  0.9881928  0.99493974]
2022-01-11 18:12:47,473 - INFO - Saving...
2022-01-11 18:12:47,681 - INFO - Epoch time: 399.76232051849365
2022-01-11 18:12:47,682 - INFO - 
Epoch: 37
2022-01-11 18:12:47,682 - INFO - 
Learning Rate: 0.0100
2022-01-11 18:13:38,939 - INFO - [Step=31000]	Loss=0.8028	252.0 examples/second
2022-01-11 18:15:35,398 - INFO - [Step=31250]	Loss=0.7859	274.8 examples/second
2022-01-11 18:17:31,843 - INFO - [Step=31500]	Loss=0.8139	274.8 examples/second
2022-01-11 18:19:27,084 - INFO - Test Loss=0.8377, Test top-1 acc=0.7667
2022-01-11 18:19:27,084 - INFO - Group Accuracy:

2022-01-11 18:19:27,084 - INFO - [0.9773494  0.9920482  0.9845783  0.99373496 0.98289156 0.9939759
 0.9814458  0.9826506  0.99084336 0.9771084  0.9884337  0.98650604
 0.9768675  0.9804819  0.97566265 0.9879518  0.99421686]
2022-01-11 18:19:27,085 - INFO - Epoch time: 399.40323781967163
2022-01-11 18:19:27,085 - INFO - 
Epoch: 38
2022-01-11 18:19:27,085 - INFO - 
Learning Rate: 0.0100
2022-01-11 18:19:38,605 - INFO - [Step=31750]	Loss=0.7920	252.4 examples/second
2022-01-11 18:21:35,072 - INFO - [Step=32000]	Loss=0.7869	274.8 examples/second
2022-01-11 18:23:31,664 - INFO - [Step=32250]	Loss=0.7929	274.5 examples/second
2022-01-11 18:25:28,138 - INFO - [Step=32500]	Loss=0.7841	274.7 examples/second
2022-01-11 18:26:06,851 - INFO - Test Loss=0.8358, Test top-1 acc=0.7675
2022-01-11 18:26:06,852 - INFO - Group Accuracy:

2022-01-11 18:26:06,852 - INFO - [0.9785542  0.99084336 0.9853012  0.9925301  0.9819277  0.9939759
 0.9812048  0.9833735  0.9898795  0.97638553 0.9879518  0.9845783
 0.97638553 0.9809638  0.9766265  0.9881928  0.9946988 ]
2022-01-11 18:26:06,853 - INFO - Epoch time: 399.767950296402
2022-01-11 18:26:06,853 - INFO - 
Epoch: 39
2022-01-11 18:26:06,853 - INFO - 
Learning Rate: 0.0100
2022-01-11 18:27:35,512 - INFO - [Step=32750]	Loss=0.7813	251.2 examples/second
2022-01-11 18:29:31,987 - INFO - [Step=33000]	Loss=0.7791	274.7 examples/second
2022-01-11 18:31:28,492 - INFO - [Step=33250]	Loss=0.7912	274.7 examples/second
2022-01-11 18:32:46,879 - INFO - Test Loss=0.8143, Test top-1 acc=0.7663
2022-01-11 18:32:46,880 - INFO - Group Accuracy:

2022-01-11 18:32:46,880 - INFO - [0.97951806 0.9918072  0.98698795 0.993253   0.98361444 0.9939759
 0.9816868  0.9826506  0.9898795  0.9775904  0.9884337  0.9840964
 0.97566265 0.9816868  0.9766265  0.9884337  0.99493974]
2022-01-11 18:32:46,880 - INFO - Epoch time: 400.0272104740143
2022-01-11 18:32:46,880 - INFO - 
Epoch: 40
2022-01-11 18:32:46,880 - INFO - 
Learning Rate: 0.0100
2022-01-11 18:33:35,704 - INFO - [Step=33500]	Loss=0.7669	251.6 examples/second
2022-01-11 18:35:32,450 - INFO - [Step=33750]	Loss=0.7672	274.1 examples/second
2022-01-11 18:37:28,784 - INFO - [Step=34000]	Loss=0.7716	275.1 examples/second
2022-01-11 18:39:26,540 - INFO - Test Loss=0.8439, Test top-1 acc=0.7636
2022-01-11 18:39:26,540 - INFO - Group Accuracy:

2022-01-11 18:39:26,546 - INFO - [0.9780723  0.98963857 0.9845783  0.9930121  0.9821687  0.9939759
 0.97951806 0.9819277  0.9893976  0.97638553 0.99036145 0.9840964
 0.9768675  0.9807229  0.97493976 0.9889157  0.99445784]
2022-01-11 18:39:26,547 - INFO - Epoch time: 399.66631531715393
2022-01-11 18:39:26,547 - INFO - 
Epoch: 41
2022-01-11 18:39:26,547 - INFO - 
Learning Rate: 0.0100
2022-01-11 18:39:35,859 - INFO - [Step=34250]	Loss=0.7748	251.8 examples/second
2022-01-11 18:41:32,555 - INFO - [Step=34500]	Loss=0.7621	274.2 examples/second
2022-01-11 18:43:29,107 - INFO - [Step=34750]	Loss=0.7622	274.6 examples/second
2022-01-11 18:45:25,844 - INFO - [Step=35000]	Loss=0.7566	274.1 examples/second
2022-01-11 18:46:06,731 - INFO - Test Loss=0.8344, Test top-1 acc=0.7706
2022-01-11 18:46:06,731 - INFO - Group Accuracy:

2022-01-11 18:46:06,731 - INFO - [0.98024094 0.9920482  0.98578316 0.9939759  0.9826506  0.99421686
 0.9809638  0.98313254 0.9893976  0.9768675  0.98771083 0.9853012
 0.97638553 0.98240966 0.97542167 0.9891566  0.9954217 ]
2022-01-11 18:46:06,732 - INFO - Saving...
2022-01-11 18:46:07,029 - INFO - Epoch time: 400.48260712623596
2022-01-11 18:46:07,029 - INFO - 
Epoch: 42
2022-01-11 18:46:07,030 - INFO - 
Learning Rate: 0.0100
2022-01-11 18:47:33,389 - INFO - [Step=35250]	Loss=0.7472	250.9 examples/second
2022-01-11 18:49:30,108 - INFO - [Step=35500]	Loss=0.7511	274.2 examples/second
2022-01-11 18:51:27,128 - INFO - [Step=35750]	Loss=0.7537	273.5 examples/second
2022-01-11 18:52:47,367 - INFO - Test Loss=0.8307, Test top-1 acc=0.7689
2022-01-11 18:52:47,367 - INFO - Group Accuracy:

2022-01-11 18:52:47,368 - INFO - [0.97927713 0.99108434 0.9845783  0.9925301  0.9840964  0.9946988
 0.9819277  0.98361444 0.9901205  0.97566265 0.9886747  0.98433733
 0.9778313  0.9807229  0.97445786 0.9881928  0.9954217 ]
2022-01-11 18:52:47,369 - INFO - Epoch time: 400.3396465778351
2022-01-11 18:52:47,369 - INFO - 
Epoch: 43
2022-01-11 18:52:47,369 - INFO - 
Learning Rate: 0.0100
2022-01-11 18:53:34,248 - INFO - [Step=36000]	Loss=0.7419	251.7 examples/second
2022-01-11 18:55:30,903 - INFO - [Step=36250]	Loss=0.7405	274.3 examples/second
2022-01-11 18:57:27,745 - INFO - [Step=36500]	Loss=0.7386	273.9 examples/second
2022-01-11 18:59:28,212 - INFO - Test Loss=0.8239, Test top-1 acc=0.7708
2022-01-11 18:59:28,212 - INFO - Group Accuracy:

2022-01-11 18:59:28,220 - INFO - [0.97927713 0.9922892  0.9860241  0.993494   0.98313254 0.99421686
 0.9807229  0.9826506  0.98963857 0.97614455 0.99084336 0.98289156
 0.97831327 0.9807229  0.97638553 0.98746985 0.9946988 ]
2022-01-11 18:59:28,221 - INFO - Saving...
2022-01-11 18:59:28,486 - INFO - Epoch time: 401.11696910858154
2022-01-11 18:59:28,486 - INFO - 
Epoch: 44
2022-01-11 18:59:28,486 - INFO - 
Learning Rate: 0.0100
2022-01-11 18:59:35,521 - INFO - [Step=36750]	Loss=0.7470	250.4 examples/second
2022-01-11 19:01:32,373 - INFO - [Step=37000]	Loss=0.7218	273.9 examples/second
2022-01-11 19:03:29,005 - INFO - [Step=37250]	Loss=0.7265	274.4 examples/second
2022-01-11 19:05:25,563 - INFO - [Step=37500]	Loss=0.7522	274.5 examples/second
2022-01-11 19:06:08,592 - INFO - Test Loss=0.8350, Test top-1 acc=0.7740
2022-01-11 19:06:08,593 - INFO - Group Accuracy:

2022-01-11 19:06:08,593 - INFO - [0.97951806 0.99108434 0.9855422  0.993494   0.9821687  0.99421686
 0.9804819  0.9845783  0.9893976  0.9768675  0.9884337  0.9838554
 0.97542167 0.9812048  0.97614455 0.9889157  0.99445784]
2022-01-11 19:06:08,594 - INFO - Saving...
2022-01-11 19:06:08,843 - INFO - Epoch time: 400.35613322257996
2022-01-11 19:06:08,843 - INFO - 
Epoch: 45
2022-01-11 19:06:08,843 - INFO - 
Learning Rate: 0.0100
2022-01-11 19:07:32,671 - INFO - [Step=37750]	Loss=0.7266	251.8 examples/second
2022-01-11 19:09:29,243 - INFO - [Step=38000]	Loss=0.7291	274.5 examples/second
2022-01-11 19:11:25,836 - INFO - [Step=38250]	Loss=0.7258	274.5 examples/second
2022-01-11 19:12:48,487 - INFO - Test Loss=0.8342, Test top-1 acc=0.7757
2022-01-11 19:12:48,487 - INFO - Group Accuracy:

2022-01-11 19:12:48,487 - INFO - [0.97927713 0.99108434 0.9860241  0.99421686 0.98433733 0.99421686
 0.9807229  0.98313254 0.99036145 0.9771084  0.9886747  0.98433733
 0.9780723  0.98024094 0.97566265 0.98771083 0.9954217 ]
2022-01-11 19:12:48,488 - INFO - Saving...
2022-01-11 19:12:48,740 - INFO - Epoch time: 399.8975579738617
2022-01-11 19:12:48,741 - INFO - 
Epoch: 46
2022-01-11 19:12:48,741 - INFO - 
Learning Rate: 0.0100
2022-01-11 19:13:32,878 - INFO - [Step=38500]	Loss=0.7209	251.9 examples/second
2022-01-11 19:15:29,347 - INFO - [Step=38750]	Loss=0.7024	274.8 examples/second
2022-01-11 19:17:25,918 - INFO - [Step=39000]	Loss=0.7353	274.5 examples/second
2022-01-11 19:19:28,204 - INFO - Test Loss=0.8288, Test top-1 acc=0.7706
2022-01-11 19:19:28,204 - INFO - Group Accuracy:

2022-01-11 19:19:28,204 - INFO - [0.97903615 0.9922892  0.9853012  0.9927711  0.9833735  0.99421686
 0.97927713 0.98240966 0.98722893 0.97542167 0.9879518  0.9845783
 0.9768675  0.9816868  0.9768675  0.9881928  0.99493974]
2022-01-11 19:19:28,205 - INFO - Epoch time: 399.4645552635193
2022-01-11 19:19:28,205 - INFO - 
Epoch: 47
2022-01-11 19:19:28,205 - INFO - 
Learning Rate: 0.0100
2022-01-11 19:19:32,603 - INFO - [Step=39250]	Loss=0.7427	252.6 examples/second
2022-01-11 19:21:29,013 - INFO - [Step=39500]	Loss=0.6893	274.9 examples/second
2022-01-11 19:23:25,403 - INFO - [Step=39750]	Loss=0.7226	274.9 examples/second
2022-01-11 19:25:21,878 - INFO - [Step=40000]	Loss=0.7214	274.7 examples/second
2022-01-11 19:26:07,451 - INFO - Test Loss=0.8201, Test top-1 acc=0.7737
2022-01-11 19:26:07,452 - INFO - Group Accuracy:

2022-01-11 19:26:07,452 - INFO - [0.9785542  0.99156624 0.9853012  0.99421686 0.9838554  0.99445784
 0.9816868  0.9816868  0.9901205  0.97638553 0.9881928  0.9850602
 0.9778313  0.9819277  0.97542167 0.9893976  0.9954217 ]
2022-01-11 19:26:07,453 - INFO - Epoch time: 399.24742460250854
2022-01-11 19:26:07,453 - INFO - 
Epoch: 48
2022-01-11 19:26:07,453 - INFO - 
Learning Rate: 0.0100
2022-01-11 19:27:28,944 - INFO - [Step=40250]	Loss=0.7055	251.8 examples/second
2022-01-11 19:29:25,413 - INFO - [Step=40500]	Loss=0.7164	274.8 examples/second
2022-01-11 19:31:22,089 - INFO - [Step=40750]	Loss=0.7142	274.3 examples/second
2022-01-11 19:32:46,812 - INFO - Test Loss=0.8158, Test top-1 acc=0.7752
2022-01-11 19:32:46,813 - INFO - Group Accuracy:

2022-01-11 19:32:46,813 - INFO - [0.97975904 0.9927711  0.9860241  0.993494   0.98433733 0.9946988
 0.98024094 0.98433733 0.9881928  0.97614455 0.99108434 0.9860241
 0.9768675  0.98289156 0.97614455 0.98746985 0.99493974]
2022-01-11 19:32:46,814 - INFO - Epoch time: 399.3608191013336
2022-01-11 19:32:46,814 - INFO - 
Epoch: 49
2022-01-11 19:32:46,814 - INFO - 
Learning Rate: 0.0100
2022-01-11 19:33:28,387 - INFO - [Step=41000]	Loss=0.7191	253.4 examples/second
2022-01-11 19:35:24,718 - INFO - [Step=41250]	Loss=0.6915	275.1 examples/second
2022-01-11 19:37:21,223 - INFO - [Step=41500]	Loss=0.6967	274.7 examples/second
2022-01-11 19:39:18,022 - INFO - [Step=41750]	Loss=0.7204	274.0 examples/second
2022-01-11 19:39:25,899 - INFO - Test Loss=0.8275, Test top-1 acc=0.7723
2022-01-11 19:39:25,900 - INFO - Group Accuracy:

2022-01-11 19:39:25,900 - INFO - [0.9804819  0.9927711  0.9855422  0.99421686 0.9845783  0.99445784
 0.98024094 0.98240966 0.98963857 0.9773494  0.98963857 0.9840964
 0.9775904  0.9819277  0.9771084  0.9891566  0.99493974]
2022-01-11 19:39:25,901 - INFO - Epoch time: 399.08724641799927
2022-01-11 19:39:25,901 - INFO - 
Epoch: 50
2022-01-11 19:39:25,901 - INFO - 
Learning Rate: 0.0100
2022-01-11 19:41:24,744 - INFO - [Step=42000]	Loss=0.6885	252.5 examples/second
2022-01-11 19:43:21,489 - INFO - [Step=42250]	Loss=0.6937	274.1 examples/second
2022-01-11 19:45:18,127 - INFO - [Step=42500]	Loss=0.7065	274.4 examples/second
2022-01-11 19:46:05,919 - INFO - Test Loss=0.8313, Test top-1 acc=0.7730
2022-01-11 19:46:05,919 - INFO - Group Accuracy:

2022-01-11 19:46:05,919 - INFO - [0.97831327 0.9918072  0.9848193  0.9927711  0.9840964  0.99445784
 0.98289156 0.98433733 0.9891566  0.9780723  0.9884337  0.9853012
 0.97638553 0.98024094 0.9768675  0.9884337  0.9946988 ]
2022-01-11 19:46:05,921 - INFO - Epoch time: 400.01950907707214
2022-01-11 19:46:05,921 - INFO - 
Epoch: 51
2022-01-11 19:46:05,921 - INFO - 
Learning Rate: 0.0100
2022-01-11 19:47:24,907 - INFO - [Step=42750]	Loss=0.6869	252.4 examples/second
2022-01-11 19:49:21,493 - INFO - [Step=43000]	Loss=0.6830	274.5 examples/second
2022-01-11 19:51:18,099 - INFO - [Step=43250]	Loss=0.7139	274.4 examples/second
2022-01-11 19:52:45,251 - INFO - Test Loss=0.8243, Test top-1 acc=0.7778
2022-01-11 19:52:45,251 - INFO - Group Accuracy:

2022-01-11 19:52:45,251 - INFO - [0.97975904 0.99156624 0.9833735  0.9939759  0.98240966 0.9959036
 0.9816868  0.9812048  0.9893976  0.9771084  0.9920482  0.9848193
 0.9768675  0.9819277  0.9766265  0.9884337  0.9959036 ]
2022-01-11 19:52:45,252 - INFO - Saving...
2022-01-11 19:52:45,505 - INFO - Epoch time: 399.58471298217773
2022-01-11 19:52:45,506 - INFO - 
Epoch: 52
2022-01-11 19:52:45,506 - INFO - 
Learning Rate: 0.0100
2022-01-11 19:53:25,211 - INFO - [Step=43500]	Loss=0.7042	251.7 examples/second
2022-01-11 19:55:21,994 - INFO - [Step=43750]	Loss=0.6773	274.0 examples/second
2022-01-11 19:57:18,644 - INFO - [Step=44000]	Loss=0.6885	274.3 examples/second
2022-01-11 19:59:15,581 - INFO - [Step=44250]	Loss=0.6945	273.7 examples/second
2022-01-11 19:59:25,617 - INFO - Test Loss=0.8490, Test top-1 acc=0.7824
2022-01-11 19:59:25,618 - INFO - Group Accuracy:

2022-01-11 19:59:25,618 - INFO - [0.97831327 0.9922892  0.98240966 0.9927711  0.98313254 0.99493974
 0.9807229  0.9840964  0.9898795  0.9785542  0.99036145 0.98361444
 0.9768675  0.98024094 0.97518075 0.9884337  0.9954217 ]
2022-01-11 19:59:25,619 - INFO - Saving...
2022-01-11 19:59:25,849 - INFO - Epoch time: 400.34338665008545
2022-01-11 19:59:25,849 - INFO - 
Epoch: 53
2022-01-11 19:59:25,849 - INFO - 
Learning Rate: 0.0100
2022-01-11 20:01:22,021 - INFO - [Step=44500]	Loss=0.6788	253.1 examples/second
2022-01-11 20:03:18,619 - INFO - [Step=44750]	Loss=0.6849	274.4 examples/second
2022-01-11 20:05:15,076 - INFO - [Step=45000]	Loss=0.6914	274.8 examples/second
2022-01-11 20:06:04,831 - INFO - Test Loss=0.8415, Test top-1 acc=0.7764
2022-01-11 20:06:04,831 - INFO - Group Accuracy:

2022-01-11 20:06:04,831 - INFO - [0.9807229  0.99060243 0.9853012  0.993253   0.9833735  0.9939759
 0.9768675  0.9814458  0.99036145 0.9771084  0.99060243 0.98698795
 0.9766265  0.98240966 0.9778313  0.9881928  0.9956626 ]
2022-01-11 20:06:04,832 - INFO - Epoch time: 398.9829361438751
2022-01-11 20:06:04,832 - INFO - 
Epoch: 54
2022-01-11 20:06:04,832 - INFO - 
Learning Rate: 0.0100
2022-01-11 20:07:21,681 - INFO - [Step=45250]	Loss=0.6805	252.8 examples/second
2022-01-11 20:09:18,275 - INFO - [Step=45500]	Loss=0.6602	274.5 examples/second
2022-01-11 20:11:15,070 - INFO - [Step=45750]	Loss=0.6812	274.0 examples/second
2022-01-11 20:12:44,471 - INFO - Test Loss=0.8416, Test top-1 acc=0.7812
2022-01-11 20:12:44,471 - INFO - Group Accuracy:

2022-01-11 20:12:44,471 - INFO - [0.9809638  0.9920482  0.9826506  0.9939759  0.9819277  0.99445784
 0.98       0.9804819  0.9901205  0.9773494  0.99036145 0.9850602
 0.9771084  0.98313254 0.9780723  0.9886747  0.9956626 ]
2022-01-11 20:12:44,472 - INFO - Epoch time: 399.6397294998169
2022-01-11 20:12:44,472 - INFO - 
Epoch: 55
2022-01-11 20:12:44,472 - INFO - 
Learning Rate: 0.0100
2022-01-11 20:13:21,537 - INFO - [Step=46000]	Loss=0.6902	253.0 examples/second
2022-01-11 20:15:18,286 - INFO - [Step=46250]	Loss=0.6554	274.1 examples/second
2022-01-11 20:17:15,061 - INFO - [Step=46500]	Loss=0.6636	274.0 examples/second
2022-01-11 20:19:11,831 - INFO - [Step=46750]	Loss=0.6845	274.0 examples/second
2022-01-11 20:19:24,722 - INFO - Test Loss=0.8574, Test top-1 acc=0.7749
2022-01-11 20:19:24,723 - INFO - Group Accuracy:

2022-01-11 20:19:24,723 - INFO - [0.9780723  0.99084336 0.9848193  0.9925301  0.9821687  0.9951807
 0.98024094 0.9821687  0.9881928  0.9766265  0.99036145 0.98578316
 0.9766265  0.9804819  0.97638553 0.9901205  0.9954217 ]
2022-01-11 20:19:24,725 - INFO - Epoch time: 400.2524755001068
2022-01-11 20:19:24,725 - INFO - 
Epoch: 56
2022-01-11 20:19:24,725 - INFO - 
Learning Rate: 0.0100
2022-01-11 20:21:19,558 - INFO - [Step=47000]	Loss=0.6532	250.5 examples/second
2022-01-11 20:23:16,958 - INFO - [Step=47250]	Loss=0.6671	272.6 examples/second
2022-01-11 20:25:13,259 - INFO - [Step=47500]	Loss=0.6632	275.1 examples/second
2022-01-11 20:26:04,995 - INFO - Test Loss=0.8548, Test top-1 acc=0.7846
2022-01-11 20:26:04,995 - INFO - Group Accuracy:

2022-01-11 20:26:04,996 - INFO - [0.9768675 0.9913253 0.9848193 0.9925301 0.9840964 0.9951807 0.9804819
 0.9819277 0.9891566 0.9778313 0.9901205 0.9840964 0.9766265 0.9809638
 0.9742169 0.9889157 0.9939759]
2022-01-11 20:26:04,997 - INFO - Saving...
2022-01-11 20:26:05,273 - INFO - Epoch time: 400.54818868637085
2022-01-11 20:26:05,273 - INFO - 
Epoch: 57
2022-01-11 20:26:05,273 - INFO - 
Learning Rate: 0.0100
2022-01-11 20:27:19,432 - INFO - [Step=47750]	Loss=0.6582	253.6 examples/second
2022-01-11 20:29:15,615 - INFO - [Step=48000]	Loss=0.6680	275.4 examples/second
2022-01-11 20:31:11,585 - INFO - [Step=48250]	Loss=0.6592	275.9 examples/second
2022-01-11 20:32:43,142 - INFO - Test Loss=0.8402, Test top-1 acc=0.7807
2022-01-11 20:32:43,143 - INFO - Group Accuracy:

2022-01-11 20:32:43,143 - INFO - [0.98       0.9920482  0.9881928  0.9927711  0.98361444 0.9946988
 0.98024094 0.9814458  0.9898795  0.9771084  0.9898795  0.9838554
 0.9766265  0.98240966 0.97903615 0.98771083 0.9954217 ]
2022-01-11 20:32:43,143 - INFO - Epoch time: 397.8702540397644
2022-01-11 20:32:43,144 - INFO - 
Epoch: 58
2022-01-11 20:32:43,144 - INFO - 
Learning Rate: 0.0100
2022-01-11 20:33:17,904 - INFO - [Step=48500]	Loss=0.6677	253.3 examples/second
2022-01-11 20:35:13,176 - INFO - [Step=48750]	Loss=0.6388	277.6 examples/second
2022-01-11 20:37:08,477 - INFO - [Step=49000]	Loss=0.6534	277.5 examples/second
2022-01-11 20:39:03,774 - INFO - [Step=49250]	Loss=0.6680	277.5 examples/second
2022-01-11 20:39:18,506 - INFO - Test Loss=0.8471, Test top-1 acc=0.7786
2022-01-11 20:39:18,507 - INFO - Group Accuracy:

2022-01-11 20:39:18,507 - INFO - [0.9804819  0.98963857 0.98626506 0.99421686 0.9838554  0.99445784
 0.97951806 0.9816868  0.9886747  0.9771084  0.9891566  0.9850602
 0.9780723  0.9814458  0.9785542  0.98771083 0.99445784]
2022-01-11 20:39:18,508 - INFO - Epoch time: 395.3644030094147
2022-01-11 20:39:18,508 - INFO - 
Epoch: 59
2022-01-11 20:39:18,508 - INFO - 
Learning Rate: 0.0010
2022-01-11 20:41:11,032 - INFO - [Step=49500]	Loss=0.6217	251.5 examples/second
2022-01-11 20:43:08,294 - INFO - [Step=49750]	Loss=0.5885	272.9 examples/second
2022-01-11 20:45:05,582 - INFO - [Step=50000]	Loss=0.5660	272.8 examples/second
2022-01-11 20:46:00,179 - INFO - Test Loss=0.7862, Test top-1 acc=0.7889
2022-01-11 20:46:00,180 - INFO - Group Accuracy:

2022-01-11 20:46:00,180 - INFO - [0.98024094 0.9920482  0.9860241  0.99445784 0.98650604 0.99445784
 0.9816868  0.98289156 0.9893976  0.97927713 0.99036145 0.9853012
 0.9778313  0.9812048  0.97831327 0.9891566  0.9946988 ]
2022-01-11 20:46:00,181 - INFO - Saving...
2022-01-11 20:46:00,429 - INFO - Epoch time: 401.92049407958984
2022-01-11 20:46:00,429 - INFO - 
Epoch: 60
2022-01-11 20:46:00,429 - INFO - 
Learning Rate: 0.0010
2022-01-11 20:47:12,023 - INFO - [Step=50250]	Loss=0.5710	253.1 examples/second
2022-01-11 20:49:07,497 - INFO - [Step=50500]	Loss=0.5689	277.1 examples/second
2022-01-11 20:51:02,693 - INFO - [Step=50750]	Loss=0.5675	277.8 examples/second
2022-01-11 20:52:35,903 - INFO - Test Loss=0.7804, Test top-1 acc=0.7899
2022-01-11 20:52:35,904 - INFO - Group Accuracy:

2022-01-11 20:52:35,904 - INFO - [0.9804819  0.9922892  0.98722893 0.99493974 0.9848193  0.99493974
 0.9812048  0.9838554  0.9901205  0.9785542  0.99036145 0.98433733
 0.9785542  0.98289156 0.9773494  0.98963857 0.99445784]
2022-01-11 20:52:35,904 - INFO - Saving...
2022-01-11 20:52:36,153 - INFO - Epoch time: 395.72394466400146
2022-01-11 20:52:36,153 - INFO - 
Epoch: 61
2022-01-11 20:52:36,153 - INFO - 
Learning Rate: 0.0010
2022-01-11 20:53:08,483 - INFO - [Step=51000]	Loss=0.5593	254.4 examples/second
2022-01-11 20:55:04,767 - INFO - [Step=51250]	Loss=0.5457	275.2 examples/second
2022-01-11 20:57:01,154 - INFO - [Step=51500]	Loss=0.5520	274.9 examples/second
2022-01-11 20:58:57,919 - INFO - [Step=51750]	Loss=0.5599	274.1 examples/second
2022-01-11 20:59:15,068 - INFO - Test Loss=0.7767, Test top-1 acc=0.7880
2022-01-11 20:59:15,069 - INFO - Group Accuracy:

2022-01-11 20:59:15,069 - INFO - [0.9807229  0.9922892  0.98626506 0.99445784 0.9853012  0.9951807
 0.9812048  0.98361444 0.99060243 0.97951806 0.99084336 0.9855422
 0.9780723  0.9833735  0.9771084  0.9901205  0.9946988 ]
2022-01-11 20:59:15,070 - INFO - Epoch time: 398.9168977737427
2022-01-11 20:59:15,070 - INFO - 
Epoch: 62
2022-01-11 20:59:15,070 - INFO - 
Learning Rate: 0.0010
2022-01-11 21:01:03,783 - INFO - [Step=52000]	Loss=0.5625	254.2 examples/second
2022-01-11 21:02:59,174 - INFO - [Step=52250]	Loss=0.5546	277.3 examples/second
2022-01-11 21:04:54,580 - INFO - [Step=52500]	Loss=0.5428	277.3 examples/second
2022-01-11 21:05:51,091 - INFO - Test Loss=0.7804, Test top-1 acc=0.7911
2022-01-11 21:05:51,092 - INFO - Group Accuracy:

2022-01-11 21:05:51,092 - INFO - [0.9819277  0.9920482  0.98674697 0.99445784 0.9860241  0.99493974
 0.9816868  0.98240966 0.99036145 0.97975904 0.98963857 0.9845783
 0.9775904  0.98289156 0.97831327 0.98963857 0.9951807 ]
2022-01-11 21:05:51,092 - INFO - Saving...
2022-01-11 21:05:51,357 - INFO - Epoch time: 396.2867822647095
2022-01-11 21:05:51,357 - INFO - 
Epoch: 63
2022-01-11 21:05:51,357 - INFO - 
Learning Rate: 0.0010
2022-01-11 21:07:01,331 - INFO - [Step=52750]	Loss=0.5519	252.5 examples/second
2022-01-11 21:08:58,441 - INFO - [Step=53000]	Loss=0.5469	273.2 examples/second
2022-01-11 21:10:55,575 - INFO - [Step=53250]	Loss=0.5411	273.2 examples/second
2022-01-11 21:12:32,586 - INFO - Test Loss=0.7746, Test top-1 acc=0.7913
2022-01-11 21:12:32,587 - INFO - Group Accuracy:

2022-01-11 21:12:32,587 - INFO - [0.98240966 0.9922892  0.98578316 0.99493974 0.9850602  0.9951807
 0.9804819  0.9826506  0.9898795  0.97927713 0.9901205  0.9853012
 0.9787952  0.9840964  0.9775904  0.9898795  0.99493974]
2022-01-11 21:12:32,588 - INFO - Saving...
2022-01-11 21:12:32,894 - INFO - Epoch time: 401.5372371673584
2022-01-11 21:12:32,895 - INFO - 
Epoch: 64
2022-01-11 21:12:32,895 - INFO - 
Learning Rate: 0.0010
2022-01-11 21:13:03,004 - INFO - [Step=53500]	Loss=0.5390	251.1 examples/second
2022-01-11 21:14:58,346 - INFO - [Step=53750]	Loss=0.5404	277.4 examples/second
2022-01-11 21:16:53,640 - INFO - [Step=54000]	Loss=0.5315	277.6 examples/second
2022-01-11 21:18:48,962 - INFO - [Step=54250]	Loss=0.5435	277.5 examples/second
2022-01-11 21:19:08,554 - INFO - Test Loss=0.7757, Test top-1 acc=0.7947
2022-01-11 21:19:08,554 - INFO - Group Accuracy:

2022-01-11 21:19:08,554 - INFO - [0.9814458  0.9930121  0.98650604 0.99421686 0.98578316 0.9951807
 0.9819277  0.9826506  0.9898795  0.97831327 0.99108434 0.9848193
 0.9771084  0.9840964  0.97831327 0.9901205  0.9956626 ]
2022-01-11 21:19:08,555 - INFO - Saving...
2022-01-11 21:19:08,834 - INFO - Epoch time: 395.9391474723816
2022-01-11 21:19:08,834 - INFO - 
Epoch: 65
2022-01-11 21:19:08,834 - INFO - 
Learning Rate: 0.0010
2022-01-11 21:20:56,695 - INFO - [Step=54500]	Loss=0.5311	250.5 examples/second
2022-01-11 21:22:53,708 - INFO - [Step=54750]	Loss=0.5303	273.5 examples/second
2022-01-11 21:24:50,684 - INFO - [Step=55000]	Loss=0.5426	273.6 examples/second
2022-01-11 21:25:50,180 - INFO - Test Loss=0.7773, Test top-1 acc=0.7896
2022-01-11 21:25:50,181 - INFO - Group Accuracy:

2022-01-11 21:25:50,181 - INFO - [0.9814458  0.9922892  0.98626506 0.9951807  0.9845783  0.99493974
 0.98240966 0.9826506  0.9901205  0.9787952  0.9913253  0.9853012
 0.9778313  0.98361444 0.9780723  0.9891566  0.99493974]
2022-01-11 21:25:50,182 - INFO - Epoch time: 401.34785056114197
2022-01-11 21:25:50,182 - INFO - 
Epoch: 66
2022-01-11 21:25:50,182 - INFO - 
Learning Rate: 0.0010
2022-01-11 21:26:58,764 - INFO - [Step=55250]	Loss=0.5308	249.8 examples/second
2022-01-11 21:28:56,276 - INFO - [Step=55500]	Loss=0.5374	272.3 examples/second
2022-01-11 21:30:53,633 - INFO - [Step=55750]	Loss=0.5380	272.7 examples/second
2022-01-11 21:32:33,307 - INFO - Test Loss=0.7772, Test top-1 acc=0.7971
2022-01-11 21:32:33,308 - INFO - Group Accuracy:

2022-01-11 21:32:33,308 - INFO - [0.9816868  0.9930121  0.98674697 0.99421686 0.9860241  0.9954217
 0.9809638  0.9838554  0.9898795  0.9785542  0.99060243 0.98626506
 0.9775904  0.98313254 0.9778313  0.9893976  0.9954217 ]
2022-01-11 21:32:33,309 - INFO - Saving...
2022-01-11 21:32:33,565 - INFO - Epoch time: 403.3829507827759
2022-01-11 21:32:33,565 - INFO - 
Epoch: 67
2022-01-11 21:32:33,565 - INFO - 
Learning Rate: 0.0010
2022-01-11 21:33:01,537 - INFO - [Step=56000]	Loss=0.5305	250.2 examples/second
2022-01-11 21:34:58,763 - INFO - [Step=56250]	Loss=0.5179	273.0 examples/second
2022-01-11 21:36:55,955 - INFO - [Step=56500]	Loss=0.5364	273.1 examples/second
2022-01-11 21:38:52,694 - INFO - [Step=56750]	Loss=0.5245	274.1 examples/second
2022-01-11 21:39:14,455 - INFO - Test Loss=0.7779, Test top-1 acc=0.7952
2022-01-11 21:39:14,455 - INFO - Group Accuracy:

2022-01-11 21:39:14,455 - INFO - [0.9819277  0.9927711  0.98698795 0.99421686 0.9848193  0.9951807
 0.98313254 0.98361444 0.9901205  0.9780723  0.99060243 0.9848193
 0.9787952  0.9833735  0.9780723  0.9891566  0.9951807 ]
2022-01-11 21:39:14,456 - INFO - Epoch time: 400.89092683792114
2022-01-11 21:39:14,456 - INFO - 
Epoch: 68
2022-01-11 21:39:14,456 - INFO - 
Learning Rate: 0.0010
2022-01-11 21:40:58,204 - INFO - [Step=57000]	Loss=0.5330	255.0 examples/second
2022-01-11 21:42:53,357 - INFO - [Step=57250]	Loss=0.5285	277.9 examples/second
2022-01-11 21:44:48,511 - INFO - [Step=57500]	Loss=0.5255	277.9 examples/second
2022-01-11 21:45:49,701 - INFO - Test Loss=0.7792, Test top-1 acc=0.7959
2022-01-11 21:45:49,701 - INFO - Group Accuracy:

2022-01-11 21:45:49,701 - INFO - [0.9816868  0.9925301  0.98698795 0.99445784 0.9855422  0.99493974
 0.9819277  0.9833735  0.9898795  0.9785542  0.9901205  0.9850602
 0.97831327 0.98313254 0.9775904  0.9891566  0.99493974]
2022-01-11 21:45:49,702 - INFO - Epoch time: 395.24598574638367
2022-01-11 21:45:49,702 - INFO - 
Epoch: 69
2022-01-11 21:45:49,702 - INFO - 
Learning Rate: 0.0010
2022-01-11 21:46:54,159 - INFO - [Step=57750]	Loss=0.5373	254.7 examples/second
2022-01-11 21:48:49,434 - INFO - [Step=58000]	Loss=0.5250	277.6 examples/second
2022-01-11 21:50:44,676 - INFO - [Step=58250]	Loss=0.5265	277.7 examples/second
2022-01-11 21:52:24,715 - INFO - Test Loss=0.7725, Test top-1 acc=0.7935
2022-01-11 21:52:24,716 - INFO - Group Accuracy:

2022-01-11 21:52:24,716 - INFO - [0.9821687  0.9922892  0.98674697 0.99493974 0.9853012  0.9956626
 0.9816868  0.9833735  0.9898795  0.9787952  0.99036145 0.9855422
 0.9785542  0.98361444 0.9785542  0.9898795  0.9954217 ]
2022-01-11 21:52:24,717 - INFO - Epoch time: 395.01491355895996
2022-01-11 21:52:24,717 - INFO - 
Epoch: 70
2022-01-11 21:52:24,717 - INFO - 
Learning Rate: 0.0010
2022-01-11 21:52:50,537 - INFO - [Step=58500]	Loss=0.5127	254.3 examples/second
2022-01-11 21:54:47,720 - INFO - [Step=58750]	Loss=0.5153	273.1 examples/second
2022-01-11 21:56:45,030 - INFO - [Step=59000]	Loss=0.5187	272.8 examples/second
2022-01-11 21:58:42,196 - INFO - [Step=59250]	Loss=0.5222	273.1 examples/second
2022-01-11 21:59:06,530 - INFO - Test Loss=0.7895, Test top-1 acc=0.7916
2022-01-11 21:59:06,530 - INFO - Group Accuracy:

2022-01-11 21:59:06,530 - INFO - [0.9816868  0.9930121  0.9850602  0.99493974 0.9860241  0.9954217
 0.9809638  0.98289156 0.98963857 0.9785542  0.9889157  0.9848193
 0.9785542  0.9833735  0.9775904  0.9901205  0.99493974]
2022-01-11 21:59:06,531 - INFO - Epoch time: 401.8136248588562
2022-01-11 21:59:06,531 - INFO - 
Epoch: 71
2022-01-11 21:59:06,531 - INFO - 
Learning Rate: 0.0010
2022-01-11 22:00:47,818 - INFO - [Step=59500]	Loss=0.5206	254.7 examples/second
2022-01-11 22:02:42,688 - INFO - [Step=59750]	Loss=0.5155	278.6 examples/second
2022-01-11 22:04:37,685 - INFO - [Step=60000]	Loss=0.5114	278.3 examples/second
2022-01-11 22:05:41,041 - INFO - Test Loss=0.7778, Test top-1 acc=0.7959
2022-01-11 22:05:41,042 - INFO - Group Accuracy:

2022-01-11 22:05:41,042 - INFO - [0.9804819  0.993494   0.98626506 0.99445784 0.9853012  0.9951807
 0.98289156 0.98289156 0.99036145 0.9787952  0.99036145 0.9860241
 0.9785542  0.9826506  0.9778313  0.9893976  0.9954217 ]
2022-01-11 22:05:41,043 - INFO - Epoch time: 394.5119571685791
2022-01-11 22:05:41,043 - INFO - 
Epoch: 72
2022-01-11 22:05:41,043 - INFO - 
Learning Rate: 0.0010
2022-01-11 22:06:43,620 - INFO - [Step=60250]	Loss=0.5181	254.1 examples/second
2022-01-11 22:08:38,756 - INFO - [Step=60500]	Loss=0.5206	277.9 examples/second
2022-01-11 22:10:34,079 - INFO - [Step=60750]	Loss=0.5223	277.5 examples/second
2022-01-11 22:12:16,369 - INFO - Test Loss=0.7815, Test top-1 acc=0.7954
2022-01-11 22:12:16,369 - INFO - Group Accuracy:

2022-01-11 22:12:16,370 - INFO - [0.9812048  0.9925301  0.98722893 0.99421686 0.98674697 0.99493974
 0.9821687  0.98313254 0.99036145 0.9785542  0.99060243 0.9855422
 0.9778313  0.98289156 0.9778313  0.9889157  0.9954217 ]
2022-01-11 22:12:16,370 - INFO - Epoch time: 395.3274292945862
2022-01-11 22:12:16,370 - INFO - 
Epoch: 73
2022-01-11 22:12:16,370 - INFO - 
Learning Rate: 0.0010
2022-01-11 22:12:40,247 - INFO - [Step=61000]	Loss=0.5215	253.6 examples/second
2022-01-11 22:14:37,358 - INFO - [Step=61250]	Loss=0.5079	273.2 examples/second
2022-01-11 22:16:34,581 - INFO - [Step=61500]	Loss=0.5219	273.0 examples/second
2022-01-11 22:18:31,692 - INFO - [Step=61750]	Loss=0.5165	273.2 examples/second
2022-01-11 22:18:58,549 - INFO - Test Loss=0.7802, Test top-1 acc=0.7925
2022-01-11 22:18:58,549 - INFO - Group Accuracy:

2022-01-11 22:18:58,549 - INFO - [0.9814458  0.9930121  0.98722893 0.9946988  0.9855422  0.9954217
 0.9819277  0.9826506  0.9898795  0.9785542  0.99108434 0.9853012
 0.9785542  0.9833735  0.9780723  0.9898795  0.9951807 ]
2022-01-11 22:18:58,550 - INFO - Epoch time: 402.17991280555725
2022-01-11 22:18:58,550 - INFO - 
Epoch: 74
2022-01-11 22:18:58,550 - INFO - 
Learning Rate: 0.0010
2022-01-11 22:20:37,995 - INFO - [Step=62000]	Loss=0.5208	253.4 examples/second
2022-01-11 22:22:33,006 - INFO - [Step=62250]	Loss=0.5026	278.2 examples/second
2022-01-11 22:24:28,027 - INFO - [Step=62500]	Loss=0.5093	278.2 examples/second
2022-01-11 22:25:33,392 - INFO - Test Loss=0.7849, Test top-1 acc=0.7947
2022-01-11 22:25:33,393 - INFO - Group Accuracy:

2022-01-11 22:25:33,393 - INFO - [0.9816868  0.9927711  0.98650604 0.9946988  0.98578316 0.9951807
 0.9816868  0.98240966 0.99036145 0.97831327 0.99084336 0.9850602
 0.9787952  0.9821687  0.9787952  0.9889157  0.9951807 ]
2022-01-11 22:25:33,394 - INFO - Epoch time: 394.84334444999695
2022-01-11 22:25:33,394 - INFO - 
Epoch: 75
2022-01-11 22:25:33,394 - INFO - 
Learning Rate: 0.0010
2022-01-11 22:26:34,371 - INFO - [Step=62750]	Loss=0.5179	253.3 examples/second
2022-01-11 22:28:31,459 - INFO - [Step=63000]	Loss=0.5033	273.3 examples/second
2022-01-11 22:30:28,725 - INFO - [Step=63250]	Loss=0.5262	272.9 examples/second
2022-01-11 22:32:14,977 - INFO - Test Loss=0.7827, Test top-1 acc=0.7949
2022-01-11 22:32:14,977 - INFO - Group Accuracy:

2022-01-11 22:32:14,977 - INFO - [0.9814458  0.993253   0.98722893 0.99493974 0.9853012  0.9951807
 0.98240966 0.9819277  0.99084336 0.9780723  0.99108434 0.9845783
 0.97903615 0.9819277  0.9771084  0.98963857 0.99493974]
2022-01-11 22:32:14,979 - INFO - Epoch time: 401.58474588394165
2022-01-11 22:32:14,979 - INFO - 
Epoch: 76
2022-01-11 22:32:14,979 - INFO - 
Learning Rate: 0.0010
2022-01-11 22:32:35,986 - INFO - [Step=63500]	Loss=0.5154	251.5 examples/second
2022-01-11 22:34:33,210 - INFO - [Step=63750]	Loss=0.5049	273.0 examples/second
2022-01-11 22:36:30,143 - INFO - [Step=64000]	Loss=0.5144	273.7 examples/second
2022-01-11 22:38:27,205 - INFO - [Step=64250]	Loss=0.4950	273.4 examples/second
2022-01-11 22:38:55,949 - INFO - Test Loss=0.7848, Test top-1 acc=0.7978
2022-01-11 22:38:55,950 - INFO - Group Accuracy:

2022-01-11 22:38:55,950 - INFO - [0.9809638  0.9930121  0.98722893 0.99493974 0.98674697 0.9954217
 0.9819277  0.98361444 0.99060243 0.97951806 0.99156624 0.98433733
 0.97831327 0.9821687  0.9778313  0.98963857 0.9951807 ]
2022-01-11 22:38:55,951 - INFO - Saving...
2022-01-11 22:38:56,222 - INFO - Epoch time: 401.24313282966614
2022-01-11 22:38:56,222 - INFO - 
Epoch: 77
2022-01-11 22:38:56,222 - INFO - 
Learning Rate: 0.0010
2022-01-11 22:40:34,481 - INFO - [Step=64500]	Loss=0.5002	251.4 examples/second
2022-01-11 22:42:31,341 - INFO - [Step=64750]	Loss=0.5136	273.8 examples/second
2022-01-11 22:44:28,311 - INFO - [Step=65000]	Loss=0.4906	273.6 examples/second
2022-01-11 22:45:37,474 - INFO - Test Loss=0.7884, Test top-1 acc=0.7945
2022-01-11 22:45:37,474 - INFO - Group Accuracy:

2022-01-11 22:45:37,474 - INFO - [0.9819277  0.9927711  0.98722893 0.9951807  0.9860241  0.9954217
 0.9809638  0.9833735  0.99084336 0.97951806 0.99060243 0.9848193
 0.97831327 0.98313254 0.97638553 0.9891566  0.9951807 ]
2022-01-11 22:45:37,475 - INFO - Epoch time: 401.2527232170105
2022-01-11 22:45:37,475 - INFO - 
Epoch: 78
2022-01-11 22:45:37,475 - INFO - 
Learning Rate: 0.0010
2022-01-11 22:46:35,330 - INFO - [Step=65250]	Loss=0.4992	251.9 examples/second
2022-01-11 22:48:30,367 - INFO - [Step=65500]	Loss=0.5182	278.2 examples/second
2022-01-11 22:50:25,401 - INFO - [Step=65750]	Loss=0.4965	278.2 examples/second
2022-01-11 22:52:12,177 - INFO - Test Loss=0.7967, Test top-1 acc=0.7937
2022-01-11 22:52:12,177 - INFO - Group Accuracy:

2022-01-11 22:52:12,178 - INFO - [0.9809638  0.9920482  0.98722893 0.9951807  0.9853012  0.9951807
 0.9804819  0.98240966 0.9901205  0.97903615 0.99060243 0.9850602
 0.97831327 0.98289156 0.9768675  0.9891566  0.99493974]
2022-01-11 22:52:12,179 - INFO - Epoch time: 394.7042303085327
2022-01-11 22:52:12,179 - INFO - 
Epoch: 79
2022-01-11 22:52:12,179 - INFO - 
Learning Rate: 0.0010
2022-01-11 22:52:30,674 - INFO - [Step=66000]	Loss=0.5083	255.4 examples/second
2022-01-11 22:54:27,038 - INFO - [Step=66250]	Loss=0.4878	275.0 examples/second
2022-01-11 22:56:23,451 - INFO - [Step=66500]	Loss=0.5051	274.9 examples/second
2022-01-11 22:58:19,669 - INFO - [Step=66750]	Loss=0.5122	275.3 examples/second
2022-01-11 22:58:51,041 - INFO - Test Loss=0.7929, Test top-1 acc=0.7925
2022-01-11 22:58:51,041 - INFO - Group Accuracy:

2022-01-11 22:58:51,041 - INFO - [0.9814458  0.9922892  0.98674697 0.99493974 0.98578316 0.9946988
 0.98240966 0.9816868  0.9901205  0.97903615 0.99084336 0.9850602
 0.97831327 0.9826506  0.97831327 0.9889157  0.9951807 ]
2022-01-11 22:58:51,042 - INFO - Epoch time: 398.86311769485474
2022-01-11 22:58:51,042 - INFO - 
Epoch: 80
2022-01-11 22:58:51,042 - INFO - 
Learning Rate: 0.0010
2022-01-11 23:00:25,732 - INFO - [Step=67000]	Loss=0.5005	253.8 examples/second
2022-01-11 23:02:21,216 - INFO - [Step=67250]	Loss=0.4944	277.1 examples/second
2022-01-11 23:04:16,825 - INFO - [Step=67500]	Loss=0.4916	276.8 examples/second
2022-01-11 23:05:27,642 - INFO - Test Loss=0.7912, Test top-1 acc=0.7959
2022-01-11 23:05:27,642 - INFO - Group Accuracy:

2022-01-11 23:05:27,642 - INFO - [0.9807229  0.9930121  0.98722893 0.99493974 0.98626506 0.99493974
 0.9809638  0.9826506  0.99060243 0.97951806 0.99060243 0.98433733
 0.97927713 0.9819277  0.97951806 0.9893976  0.99493974]
2022-01-11 23:05:27,643 - INFO - Epoch time: 396.600533246994
2022-01-11 23:05:27,643 - INFO - 
Epoch: 81
2022-01-11 23:05:27,643 - INFO - 
Learning Rate: 0.0010
2022-01-11 23:06:23,352 - INFO - [Step=67750]	Loss=0.4989	252.9 examples/second
2022-01-11 23:08:18,119 - INFO - [Step=68000]	Loss=0.4953	278.8 examples/second
2022-01-11 23:10:12,949 - INFO - [Step=68250]	Loss=0.4917	278.7 examples/second
2022-01-11 23:12:01,967 - INFO - Test Loss=0.7962, Test top-1 acc=0.7918
2022-01-11 23:12:01,967 - INFO - Group Accuracy:

2022-01-11 23:12:01,967 - INFO - [0.98       0.9922892  0.98722893 0.99445784 0.9853012  0.9954217
 0.9816868  0.9816868  0.99060243 0.97903615 0.9920482  0.9848193
 0.9773494  0.9816868  0.97831327 0.9889157  0.9946988 ]
2022-01-11 23:12:01,968 - INFO - Epoch time: 394.32479310035706
2022-01-11 23:12:01,968 - INFO - 
Epoch: 82
2022-01-11 23:12:01,968 - INFO - 
Learning Rate: 0.0010
2022-01-11 23:12:18,140 - INFO - [Step=68500]	Loss=0.4921	255.6 examples/second
2022-01-11 23:14:14,196 - INFO - [Step=68750]	Loss=0.4914	275.7 examples/second
2022-01-11 23:16:10,019 - INFO - [Step=69000]	Loss=0.4895	276.3 examples/second
2022-01-11 23:18:05,831 - INFO - [Step=69250]	Loss=0.4894	276.3 examples/second
2022-01-11 23:18:39,173 - INFO - Test Loss=0.7942, Test top-1 acc=0.7942
2022-01-11 23:18:39,174 - INFO - Group Accuracy:

2022-01-11 23:18:39,174 - INFO - [0.98024094 0.9930121  0.98698795 0.99421686 0.98650604 0.9954217
 0.9814458  0.9819277  0.9898795  0.97903615 0.99084336 0.9845783
 0.97831327 0.9821687  0.9785542  0.98963857 0.9954217 ]
2022-01-11 23:18:39,175 - INFO - Epoch time: 397.2070138454437
2022-01-11 23:18:39,175 - INFO - 
Epoch: 83
2022-01-11 23:18:39,175 - INFO - 
Learning Rate: 0.0010
2022-01-11 23:20:11,200 - INFO - [Step=69500]	Loss=0.4814	255.2 examples/second
2022-01-11 23:22:06,005 - INFO - [Step=69750]	Loss=0.4927	278.7 examples/second
2022-01-11 23:24:01,197 - INFO - [Step=70000]	Loss=0.4951	277.8 examples/second
2022-01-11 23:25:13,702 - INFO - Test Loss=0.7873, Test top-1 acc=0.7925
2022-01-11 23:25:13,703 - INFO - Group Accuracy:

2022-01-11 23:25:13,703 - INFO - [0.9812048  0.993253   0.98771083 0.9951807  0.98578316 0.99493974
 0.9814458  0.9816868  0.99036145 0.97903615 0.99060243 0.9853012
 0.97903615 0.98240966 0.97831327 0.9893976  0.9946988 ]
2022-01-11 23:25:13,704 - INFO - Epoch time: 394.52888798713684
2022-01-11 23:25:13,704 - INFO - 
Epoch: 84
2022-01-11 23:25:13,704 - INFO - 
Learning Rate: 0.0010
2022-01-11 23:26:08,069 - INFO - [Step=70250]	Loss=0.4968	252.2 examples/second
2022-01-11 23:28:05,537 - INFO - [Step=70500]	Loss=0.4929	272.4 examples/second
2022-01-11 23:30:02,877 - INFO - [Step=70750]	Loss=0.4962	272.7 examples/second
2022-01-11 23:31:56,213 - INFO - Test Loss=0.7920, Test top-1 acc=0.7969
2022-01-11 23:31:56,214 - INFO - Group Accuracy:

2022-01-11 23:31:56,214 - INFO - [0.9816868  0.9930121  0.98771083 0.9946988  0.9860241  0.99493974
 0.9821687  0.98313254 0.99036145 0.9778313  0.9925301  0.98578316
 0.97975904 0.98313254 0.9780723  0.98963857 0.9954217 ]
2022-01-11 23:31:56,215 - INFO - Epoch time: 402.511470079422
2022-01-11 23:31:56,216 - INFO - 
Epoch: 85
2022-01-11 23:31:56,216 - INFO - 
Learning Rate: 0.0010
2022-01-11 23:32:10,488 - INFO - [Step=71000]	Loss=0.4976	250.8 examples/second
2022-01-11 23:34:05,464 - INFO - [Step=71250]	Loss=0.4824	278.3 examples/second
2022-01-11 23:36:00,322 - INFO - [Step=71500]	Loss=0.4894	278.6 examples/second
2022-01-11 23:37:55,208 - INFO - [Step=71750]	Loss=0.4878	278.5 examples/second
2022-01-11 23:38:30,437 - INFO - Test Loss=0.8007, Test top-1 acc=0.7966
2022-01-11 23:38:30,438 - INFO - Group Accuracy:

2022-01-11 23:38:30,438 - INFO - [0.98240966 0.9925301  0.98722893 0.99493974 0.9855422  0.99493974
 0.98240966 0.98240966 0.9898795  0.97831327 0.99036145 0.9845783
 0.9785542  0.9821687  0.9778313  0.9889157  0.9954217 ]
2022-01-11 23:38:30,439 - INFO - Epoch time: 394.2232081890106
2022-01-11 23:38:30,439 - INFO - 
Epoch: 86
2022-01-11 23:38:30,439 - INFO - 
Learning Rate: 0.0010
2022-01-11 23:40:01,640 - INFO - [Step=72000]	Loss=0.5052	253.1 examples/second
2022-01-11 23:41:58,928 - INFO - [Step=72250]	Loss=0.4869	272.8 examples/second
2022-01-11 23:43:56,119 - INFO - [Step=72500]	Loss=0.4863	273.1 examples/second
2022-01-11 23:45:11,793 - INFO - Test Loss=0.7958, Test top-1 acc=0.7940
2022-01-11 23:45:11,794 - INFO - Group Accuracy:

2022-01-11 23:45:11,794 - INFO - [0.9809638  0.9930121  0.98722893 0.9954217  0.9850602  0.9951807
 0.9821687  0.9809638  0.9901205  0.97927713 0.99156624 0.9853012
 0.9780723  0.9816868  0.9773494  0.9889157  0.9946988 ]
2022-01-11 23:45:11,795 - INFO - Epoch time: 401.35594630241394
2022-01-11 23:45:11,795 - INFO - 
Epoch: 87
2022-01-11 23:45:11,795 - INFO - 
Learning Rate: 0.0010
2022-01-11 23:46:03,518 - INFO - [Step=72750]	Loss=0.4822	251.2 examples/second
2022-01-11 23:48:01,277 - INFO - [Step=73000]	Loss=0.4908	271.7 examples/second
2022-01-11 23:49:58,951 - INFO - [Step=73250]	Loss=0.4814	271.9 examples/second
2022-01-11 23:51:54,858 - INFO - Test Loss=0.7954, Test top-1 acc=0.7918
2022-01-11 23:51:54,859 - INFO - Group Accuracy:

2022-01-11 23:51:54,859 - INFO - [0.9816868  0.9930121  0.98626506 0.9946988  0.98626506 0.99445784
 0.9809638  0.9816868  0.9901205  0.97903615 0.9918072  0.9860241
 0.9785542  0.98240966 0.9771084  0.9891566  0.9954217 ]
2022-01-11 23:51:54,860 - INFO - Epoch time: 403.06480956077576
2022-01-11 23:51:54,860 - INFO - 
Epoch: 88
2022-01-11 23:51:54,860 - INFO - 
Learning Rate: 0.0010
2022-01-11 23:52:06,687 - INFO - [Step=73500]	Loss=0.4942	250.5 examples/second
2022-01-11 23:54:01,803 - INFO - [Step=73750]	Loss=0.4916	278.0 examples/second
2022-01-11 23:55:57,184 - INFO - [Step=74000]	Loss=0.4906	277.3 examples/second
2022-01-11 23:57:52,283 - INFO - [Step=74250]	Loss=0.4858	278.0 examples/second
2022-01-11 23:58:30,150 - INFO - Test Loss=0.7999, Test top-1 acc=0.7942
2022-01-11 23:58:30,150 - INFO - Group Accuracy:

2022-01-11 23:58:30,150 - INFO - [0.9821687  0.9922892  0.98626506 0.9946988  0.9845783  0.9946988
 0.9814458  0.9826506  0.99084336 0.9778313  0.9913253  0.9845783
 0.9785542  0.98289156 0.9766265  0.9891566  0.9951807 ]
2022-01-11 23:58:30,151 - INFO - Epoch time: 395.2911367416382
2022-01-11 23:58:30,151 - INFO - 
Epoch: 89
2022-01-11 23:58:30,151 - INFO - 
Learning Rate: 0.0010
2022-01-11 23:59:59,471 - INFO - [Step=74500]	Loss=0.4885	251.6 examples/second
2022-01-12 00:01:56,829 - INFO - [Step=74750]	Loss=0.4847	272.7 examples/second
2022-01-12 00:03:54,121 - INFO - [Step=75000]	Loss=0.4754	272.8 examples/second
2022-01-12 00:05:12,521 - INFO - Test Loss=0.8061, Test top-1 acc=0.7930
2022-01-12 00:05:12,521 - INFO - Group Accuracy:

2022-01-12 00:05:12,521 - INFO - [0.9812048  0.993253   0.98746985 0.9946988  0.9853012  0.9951807
 0.9819277  0.98240966 0.9901205  0.9771084  0.99036145 0.9850602
 0.9785542  0.9819277  0.9766265  0.9891566  0.9951807 ]
2022-01-12 00:05:12,522 - INFO - Epoch time: 402.37143540382385
2022-01-12 00:05:24,081 - INFO - Computing OOD Statistics...
2022-01-12 00:05:24,089 - INFO - 	Baseline.          AUROC: 0.4228. TNR@95TPR: 0.0318. AUPR OUT: 0.1440
2022-01-12 00:05:24,095 - INFO - 	ODIN (T=1000).     AUROC: 0.9103. TNR@95TPR: 0.5682. AUPR OUT: 0.6829
2022-01-12 00:05:24,095 - INFO - Printing Final Accuracy + OOD Detection stats
2022-01-12 00:05:24,095 - INFO - Top 1 Accuracy:  Min: 0.7978; Max: 0.7978; Avg: 0.7978; Std: 0.0000; Len: 1
2022-01-12 00:05:24,095 - INFO - Top 5 Accuracy:  Min: 0.9866; Max: 0.9866; Avg: 0.9866; Std: 0.0000; Len: 1
2022-01-12 00:05:24,096 - INFO - **********************************************************************
2022-01-12 00:05:24,096 - INFO - 	MSP (auroc): [0.42275719347980156] Min: 0.4228; Max: 0.4228; Avg: 0.4228; Std: 0.0000; Len: 1
2022-01-12 00:05:24,096 - INFO - 	MSP (tnr): [0.03176470588235292] Min: 0.0318; Max: 0.0318; Avg: 0.0318; Std: 0.0000; Len: 1
2022-01-12 00:05:24,096 - INFO - 	MSP (aupr): [0.14402401612930982] Min: 0.1440; Max: 0.1440; Avg: 0.1440; Std: 0.0000; Len: 1
2022-01-12 00:05:24,096 - INFO - 	ODIN (auroc): [0.9102883061658399] Min: 0.9103; Max: 0.9103; Avg: 0.9103; Std: 0.0000; Len: 1
2022-01-12 00:05:24,096 - INFO - 	ODIN (tnr): [0.5682352941176471] Min: 0.5682; Max: 0.5682; Avg: 0.5682; Std: 0.0000; Len: 1
2022-01-12 00:05:24,096 - INFO - 	ODIN (aupr): [0.682901846333647] Min: 0.6829; Max: 0.6829; Avg: 0.6829; Std: 0.0000; Len: 1
