2022-01-11 01:28:15,391 - INFO - ==> Preparing data..
2022-01-11 01:28:15,795 - INFO - checkpoint filename: experiments/coarse/mos/adam_LRp1_R1/checkpoint.pt
2022-01-11 01:28:15,795 - INFO - log filename: experiments/coarse/mos/adam_LRp1_R1/train.log
2022-01-11 01:28:15,795 - INFO - ********************************************************
2022-01-11 01:28:15,795 - INFO - Starting Iter: 0 / 1
2022-01-11 01:28:15,795 - INFO - ********************************************************
2022-01-11 01:28:19,072 - INFO - 
Epoch: 0
2022-01-11 01:28:19,073 - INFO - 
Learning Rate: 0.0100
2022-01-11 01:30:18,042 - INFO - [Step=250]	Loss=6.6803	269.0 examples/second
2022-01-11 01:32:15,121 - INFO - [Step=500]	Loss=5.2950	273.3 examples/second
2022-01-11 01:34:12,215 - INFO - [Step=750]	Loss=5.1731	273.3 examples/second
2022-01-11 01:34:59,488 - INFO - Test Loss=4.9771, Test top-1 acc=0.0400
2022-01-11 01:34:59,488 - INFO - Group Accuracy:

2022-01-11 01:34:59,489 - INFO - [0.939759  0.939759  0.939759  0.939759  0.939759  0.9395181 0.939759
 0.939759  0.939759  0.939759  0.9518072 0.939759  0.939759  0.9395181
 0.939759  0.939759  0.9518072]
2022-01-11 01:34:59,490 - INFO - Saving...
2022-01-11 01:34:59,752 - INFO - Epoch time: 400.6799030303955
2022-01-11 01:34:59,752 - INFO - 
Epoch: 1
2022-01-11 01:34:59,752 - INFO - 
Learning Rate: 0.0280
2022-01-11 01:36:18,644 - INFO - [Step=1000]	Loss=5.0819	253.1 examples/second
2022-01-11 01:38:15,578 - INFO - [Step=1250]	Loss=4.9141	273.7 examples/second
2022-01-11 01:40:12,816 - INFO - [Step=1500]	Loss=4.7043	272.9 examples/second
2022-01-11 01:41:40,233 - INFO - Test Loss=4.5422, Test top-1 acc=0.1142
2022-01-11 01:41:40,233 - INFO - Group Accuracy:

2022-01-11 01:41:40,233 - INFO - [0.9392771  0.9395181  0.93421686 0.94506025 0.9359036  0.91036147
 0.9407229  0.9395181  0.9404819  0.939759   0.9518072  0.939759
 0.93903613 0.939759   0.9351807  0.94       0.9520482 ]
2022-01-11 01:41:40,234 - INFO - Saving...
2022-01-11 01:41:40,499 - INFO - Epoch time: 400.746356010437
2022-01-11 01:41:40,499 - INFO - 
Epoch: 2
2022-01-11 01:41:40,499 - INFO - 
Learning Rate: 0.0460
2022-01-11 01:42:19,985 - INFO - [Step=1750]	Loss=4.5435	251.6 examples/second
2022-01-11 01:44:17,096 - INFO - [Step=2000]	Loss=4.4186	273.2 examples/second
2022-01-11 01:46:14,593 - INFO - [Step=2250]	Loss=4.2469	272.3 examples/second
2022-01-11 01:48:11,772 - INFO - [Step=2500]	Loss=4.1349	273.1 examples/second
2022-01-11 01:48:21,594 - INFO - Test Loss=4.4956, Test top-1 acc=0.2024
2022-01-11 01:48:21,594 - INFO - Group Accuracy:

2022-01-11 01:48:21,594 - INFO - [0.940241   0.9279518  0.940241   0.9474699  0.9407229  0.9513253
 0.9318072  0.93373495 0.946747   0.93108433 0.9361446  0.9351807
 0.939759   0.9279518  0.93421686 0.94216865 0.95325303]
2022-01-11 01:48:21,595 - INFO - Saving...
2022-01-11 01:48:21,871 - INFO - Epoch time: 401.37150168418884
2022-01-11 01:48:21,871 - INFO - 
Epoch: 3
2022-01-11 01:48:21,871 - INFO - 
Learning Rate: 0.0640
2022-01-11 01:50:18,468 - INFO - [Step=2750]	Loss=4.1069	252.6 examples/second
2022-01-11 01:52:15,620 - INFO - [Step=3000]	Loss=4.0080	273.2 examples/second
2022-01-11 01:54:13,006 - INFO - [Step=3250]	Loss=3.8978	272.6 examples/second
2022-01-11 01:55:02,960 - INFO - Test Loss=4.5192, Test top-1 acc=0.2089
2022-01-11 01:55:02,960 - INFO - Group Accuracy:

2022-01-11 01:55:02,960 - INFO - [0.93831325 0.9436145  0.9366265  0.94915664 0.93421686 0.93421686
 0.94216865 0.94168675 0.95421684 0.9359036  0.94506025 0.9392771
 0.9385542  0.9346988  0.9426506  0.9426506  0.9559036 ]
2022-01-11 01:55:02,961 - INFO - Saving...
2022-01-11 01:55:03,243 - INFO - Epoch time: 401.3723773956299
2022-01-11 01:55:03,244 - INFO - 
Epoch: 4
2022-01-11 01:55:03,244 - INFO - 
Learning Rate: 0.1000
2022-01-11 01:56:20,226 - INFO - [Step=3500]	Loss=4.0056	251.5 examples/second
2022-01-11 01:58:17,087 - INFO - [Step=3750]	Loss=3.9337	273.8 examples/second
2022-01-11 02:00:14,062 - INFO - [Step=4000]	Loss=3.8558	273.6 examples/second
2022-01-11 02:01:44,164 - INFO - Test Loss=3.7129, Test top-1 acc=0.2501
2022-01-11 02:01:44,165 - INFO - Group Accuracy:

2022-01-11 02:01:44,165 - INFO - [0.94192773 0.94168675 0.9387952  0.9559036  0.9395181  0.95975906
 0.9436145  0.94192773 0.9544578  0.933494   0.95277107 0.940241
 0.9392771  0.94168675 0.94144577 0.94554216 0.96385545]
2022-01-11 02:01:44,166 - INFO - Saving...
2022-01-11 02:01:44,425 - INFO - Epoch time: 401.18111872673035
2022-01-11 02:01:44,425 - INFO - 
Epoch: 5
2022-01-11 02:01:44,425 - INFO - 
Learning Rate: 0.1000
2022-01-11 02:02:21,471 - INFO - [Step=4250]	Loss=3.7892	251.2 examples/second
2022-01-11 02:04:18,252 - INFO - [Step=4500]	Loss=3.7383	274.0 examples/second
2022-01-11 02:06:15,046 - INFO - [Step=4750]	Loss=3.6738	274.0 examples/second
2022-01-11 02:08:12,183 - INFO - [Step=5000]	Loss=3.6175	273.2 examples/second
2022-01-11 02:08:24,231 - INFO - Test Loss=3.6329, Test top-1 acc=0.2855
2022-01-11 02:08:24,232 - INFO - Group Accuracy:

2022-01-11 02:08:24,232 - INFO - [0.9404819  0.9472289  0.9407229  0.9585542  0.93421686 0.95975906
 0.9440964  0.9513253  0.95686746 0.94168675 0.94144577 0.9387952
 0.940241   0.94216865 0.91542166 0.9431325  0.96168673]
2022-01-11 02:08:24,233 - INFO - Saving...
2022-01-11 02:08:24,475 - INFO - Epoch time: 400.0500898361206
2022-01-11 02:08:24,475 - INFO - 
Epoch: 6
2022-01-11 02:08:24,475 - INFO - 
Learning Rate: 0.1000
2022-01-11 02:10:18,804 - INFO - [Step=5250]	Loss=3.5938	252.7 examples/second
2022-01-11 02:12:15,811 - INFO - [Step=5500]	Loss=3.5431	273.5 examples/second
2022-01-11 02:14:12,971 - INFO - [Step=5750]	Loss=3.5128	273.1 examples/second
2022-01-11 02:15:05,249 - INFO - Test Loss=4.7586, Test top-1 acc=0.2280
2022-01-11 02:15:05,250 - INFO - Group Accuracy:

2022-01-11 02:15:05,250 - INFO - [0.9385542  0.94120485 0.9356626  0.94506025 0.94289154 0.94
 0.94144577 0.94626504 0.91566265 0.94144577 0.93301207 0.92578316
 0.93831325 0.9007229  0.9426506  0.9387952  0.96795183]
2022-01-11 02:15:05,250 - INFO - Epoch time: 400.7751274108887
2022-01-11 02:15:05,251 - INFO - 
Epoch: 7
2022-01-11 02:15:05,251 - INFO - 
Learning Rate: 0.1000
2022-01-11 02:16:19,603 - INFO - [Step=6000]	Loss=3.4894	252.7 examples/second
2022-01-11 02:18:16,315 - INFO - [Step=6250]	Loss=3.4838	274.2 examples/second
2022-01-11 02:20:13,204 - INFO - [Step=6500]	Loss=3.4210	273.8 examples/second
2022-01-11 02:21:44,990 - INFO - Test Loss=3.2603, Test top-1 acc=0.3306
2022-01-11 02:21:44,990 - INFO - Group Accuracy:

2022-01-11 02:21:44,990 - INFO - [0.9438554  0.9513253  0.9385542  0.96       0.9426506  0.9633735
 0.9426506  0.9501205  0.9559036  0.9438554  0.95662653 0.94506025
 0.93783134 0.94192773 0.9426506  0.9501205  0.9708434 ]
2022-01-11 02:21:44,991 - INFO - Saving...
2022-01-11 02:21:45,238 - INFO - Epoch time: 399.98789143562317
2022-01-11 02:21:45,239 - INFO - 
Epoch: 8
2022-01-11 02:21:45,239 - INFO - 
Learning Rate: 0.1000
2022-01-11 02:22:20,113 - INFO - [Step=6750]	Loss=3.3683	252.1 examples/second
2022-01-11 02:24:17,120 - INFO - [Step=7000]	Loss=3.3673	273.5 examples/second
2022-01-11 02:26:13,943 - INFO - [Step=7250]	Loss=3.3746	273.9 examples/second
2022-01-11 02:28:10,786 - INFO - [Step=7500]	Loss=3.3044	273.9 examples/second
2022-01-11 02:28:25,579 - INFO - Test Loss=3.1703, Test top-1 acc=0.3255
2022-01-11 02:28:25,579 - INFO - Group Accuracy:

2022-01-11 02:28:25,579 - INFO - [0.94168675 0.9508434  0.94843376 0.9614458  0.9426506  0.96481925
 0.94192773 0.9520482  0.9539759  0.9445783  0.95686746 0.946506
 0.94144577 0.9445783  0.9448193  0.9518072  0.97204816]
2022-01-11 02:28:25,580 - INFO - Epoch time: 400.34177470207214
2022-01-11 02:28:25,581 - INFO - 
Epoch: 9
2022-01-11 02:28:25,581 - INFO - 
Learning Rate: 0.1000
2022-01-11 02:30:17,526 - INFO - [Step=7750]	Loss=3.2630	252.5 examples/second
2022-01-11 02:32:14,384 - INFO - [Step=8000]	Loss=3.2480	273.8 examples/second
2022-01-11 02:34:11,191 - INFO - [Step=8250]	Loss=3.2120	274.0 examples/second
2022-01-11 02:35:05,671 - INFO - Test Loss=3.1902, Test top-1 acc=0.3378
2022-01-11 02:35:05,671 - INFO - Group Accuracy:

2022-01-11 02:35:05,671 - INFO - [0.94289154 0.9508434  0.9448193  0.9624096  0.946506   0.9698795
 0.946506   0.95325303 0.96433735 0.9407229  0.95036143 0.9440964
 0.939759   0.9448193  0.9438554  0.94963855 0.9703615 ]
2022-01-11 02:35:05,672 - INFO - Saving...
2022-01-11 02:35:05,933 - INFO - Epoch time: 400.3522050380707
2022-01-11 02:35:05,933 - INFO - 
Epoch: 10
2022-01-11 02:35:05,933 - INFO - 
Learning Rate: 0.1000
2022-01-11 02:36:17,791 - INFO - [Step=8500]	Loss=3.2130	252.8 examples/second
2022-01-11 02:38:14,326 - INFO - [Step=8750]	Loss=3.1628	274.6 examples/second
2022-01-11 02:40:11,022 - INFO - [Step=9000]	Loss=3.1542	274.2 examples/second
2022-01-11 02:41:45,209 - INFO - Test Loss=3.3080, Test top-1 acc=0.3333
2022-01-11 02:41:45,209 - INFO - Group Accuracy:

2022-01-11 02:41:45,209 - INFO - [0.94192773 0.95036143 0.94289154 0.96168673 0.946747   0.9624096
 0.9469879  0.9518072  0.9612048  0.946747   0.9578313  0.9453012
 0.9387952  0.94289154 0.94578314 0.9498795  0.9766265 ]
2022-01-11 02:41:45,210 - INFO - Epoch time: 399.27704191207886
2022-01-11 02:41:45,210 - INFO - 
Epoch: 11
2022-01-11 02:41:45,210 - INFO - 
Learning Rate: 0.1000
2022-01-11 02:42:17,432 - INFO - [Step=9250]	Loss=3.1364	253.1 examples/second
2022-01-11 02:44:13,936 - INFO - [Step=9500]	Loss=3.1063	274.7 examples/second
2022-01-11 02:46:10,449 - INFO - [Step=9750]	Loss=3.0937	274.6 examples/second
2022-01-11 02:48:07,626 - INFO - [Step=10000]	Loss=3.0299	273.1 examples/second
2022-01-11 02:48:24,760 - INFO - Test Loss=3.0158, Test top-1 acc=0.3641
2022-01-11 02:48:24,760 - INFO - Group Accuracy:

2022-01-11 02:48:24,760 - INFO - [0.9469879  0.9506024  0.94963855 0.9660241  0.9477109  0.97301203
 0.93759036 0.95349395 0.966506   0.94289154 0.95759034 0.94433737
 0.9407229  0.94240963 0.946506   0.9498795  0.973253  ]
2022-01-11 02:48:24,761 - INFO - Saving...
2022-01-11 02:48:25,010 - INFO - Epoch time: 399.79988956451416
2022-01-11 02:48:25,010 - INFO - 
Epoch: 12
2022-01-11 02:48:25,010 - INFO - 
Learning Rate: 0.1000
2022-01-11 02:50:14,168 - INFO - [Step=10250]	Loss=3.0020	252.9 examples/second
2022-01-11 02:52:10,978 - INFO - [Step=10500]	Loss=3.0059	274.0 examples/second
2022-01-11 02:54:07,914 - INFO - [Step=10750]	Loss=2.9917	273.7 examples/second
2022-01-11 02:55:04,841 - INFO - Test Loss=2.8607, Test top-1 acc=0.3713
2022-01-11 02:55:04,841 - INFO - Group Accuracy:

2022-01-11 02:55:04,841 - INFO - [0.94554216 0.9592771  0.94506025 0.96457833 0.94626504 0.9706024
 0.94891566 0.9573494  0.9624096  0.9448193  0.9580723  0.9498795
 0.94120485 0.93759036 0.9460241  0.95349395 0.9768675 ]
2022-01-11 02:55:04,842 - INFO - Saving...
2022-01-11 02:55:05,080 - INFO - Epoch time: 400.0695867538452
2022-01-11 02:55:05,080 - INFO - 
Epoch: 13
2022-01-11 02:55:05,080 - INFO - 
Learning Rate: 0.1000
2022-01-11 02:56:14,825 - INFO - [Step=11000]	Loss=2.9635	252.1 examples/second
2022-01-11 02:58:11,292 - INFO - [Step=11250]	Loss=2.9070	274.8 examples/second
2022-01-11 03:00:07,668 - INFO - [Step=11500]	Loss=2.9327	275.0 examples/second
2022-01-11 03:01:44,574 - INFO - Test Loss=2.9960, Test top-1 acc=0.3812
2022-01-11 03:01:44,575 - INFO - Group Accuracy:

2022-01-11 03:01:44,575 - INFO - [0.9498795  0.9590362  0.946506   0.96433735 0.9479518  0.97204816
 0.9479518  0.9508434  0.9660241  0.93759036 0.9619277  0.9508434
 0.94120485 0.94289154 0.946506   0.95277107 0.97445786]
2022-01-11 03:01:44,576 - INFO - Saving...
2022-01-11 03:01:44,836 - INFO - Epoch time: 399.7557957172394
2022-01-11 03:01:44,836 - INFO - 
Epoch: 14
2022-01-11 03:01:44,836 - INFO - 
Learning Rate: 0.1000
2022-01-11 03:02:15,088 - INFO - [Step=11750]	Loss=2.9007	251.1 examples/second
2022-01-11 03:04:11,553 - INFO - [Step=12000]	Loss=2.8357	274.8 examples/second
2022-01-11 03:06:07,905 - INFO - [Step=12250]	Loss=2.8423	275.0 examples/second
2022-01-11 03:08:04,397 - INFO - [Step=12500]	Loss=2.7998	274.7 examples/second
2022-01-11 03:08:23,950 - INFO - Test Loss=2.6746, Test top-1 acc=0.4395
2022-01-11 03:08:23,950 - INFO - Group Accuracy:

2022-01-11 03:08:23,950 - INFO - [0.94506025 0.96072286 0.95301205 0.9691566  0.9520482  0.97566265
 0.9508434  0.95686746 0.96698797 0.9469879  0.9592771  0.95421684
 0.94216865 0.9453012  0.940241   0.95638555 0.9804819 ]
2022-01-11 03:08:23,951 - INFO - Saving...
2022-01-11 03:08:24,178 - INFO - Epoch time: 399.3413996696472
2022-01-11 03:08:24,178 - INFO - 
Epoch: 15
2022-01-11 03:08:24,178 - INFO - 
Learning Rate: 0.1000
2022-01-11 03:10:11,500 - INFO - [Step=12750]	Loss=2.7686	251.8 examples/second
2022-01-11 03:12:07,895 - INFO - [Step=13000]	Loss=2.7665	274.9 examples/second
2022-01-11 03:14:04,438 - INFO - [Step=13250]	Loss=2.7383	274.6 examples/second
2022-01-11 03:15:03,311 - INFO - Test Loss=2.5130, Test top-1 acc=0.4482
2022-01-11 03:15:03,311 - INFO - Group Accuracy:

2022-01-11 03:15:03,311 - INFO - [0.95373493 0.9578313  0.9546988  0.9725301  0.95325303 0.9821687
 0.9513253  0.9592771  0.96819276 0.95253015 0.96433735 0.95638555
 0.9431325  0.9481928  0.9481928  0.95710844 0.97903615]
2022-01-11 03:15:03,312 - INFO - Saving...
2022-01-11 03:15:03,563 - INFO - Epoch time: 399.385605096817
2022-01-11 03:15:03,564 - INFO - 
Epoch: 16
2022-01-11 03:15:03,564 - INFO - 
Learning Rate: 0.1000
2022-01-11 03:16:10,644 - INFO - [Step=13500]	Loss=2.7224	253.6 examples/second
2022-01-11 03:18:07,120 - INFO - [Step=13750]	Loss=2.6755	274.7 examples/second
2022-01-11 03:20:03,594 - INFO - [Step=14000]	Loss=2.6824	274.7 examples/second
2022-01-11 03:21:42,130 - INFO - Test Loss=2.7645, Test top-1 acc=0.4128
2022-01-11 03:21:42,130 - INFO - Group Accuracy:

2022-01-11 03:21:42,130 - INFO - [0.9510843  0.9590362  0.95349395 0.9701205  0.946747   0.97542167
 0.94963855 0.9520482  0.96457833 0.93759036 0.9614458  0.9515663
 0.94216865 0.94289154 0.9481928  0.9561446  0.97927713]
2022-01-11 03:21:42,131 - INFO - Epoch time: 398.56774377822876
2022-01-11 03:21:42,132 - INFO - 
Epoch: 17
2022-01-11 03:21:42,132 - INFO - 
Learning Rate: 0.1000
2022-01-11 03:22:09,556 - INFO - [Step=14250]	Loss=2.7162	254.0 examples/second
2022-01-11 03:24:06,014 - INFO - [Step=14500]	Loss=2.7401	274.8 examples/second
2022-01-11 03:26:02,888 - INFO - [Step=14750]	Loss=2.6096	273.8 examples/second
2022-01-11 03:27:59,495 - INFO - [Step=15000]	Loss=2.5872	274.4 examples/second
2022-01-11 03:28:21,291 - INFO - Test Loss=2.6900, Test top-1 acc=0.4318
2022-01-11 03:28:21,291 - INFO - Group Accuracy:

2022-01-11 03:28:21,291 - INFO - [0.95638555 0.9539759  0.9477109  0.96072286 0.95228916 0.97831327
 0.94939756 0.9573494  0.973494   0.9551807  0.96048194 0.9510843
 0.9431325  0.946506   0.94289154 0.9592771  0.98      ]
2022-01-11 03:28:21,292 - INFO - Epoch time: 399.16025042533875
2022-01-11 03:28:21,292 - INFO - 
Epoch: 18
2022-01-11 03:28:21,292 - INFO - 
Learning Rate: 0.1000
2022-01-11 03:30:05,883 - INFO - [Step=15250]	Loss=2.5882	253.2 examples/second
2022-01-11 03:32:02,443 - INFO - [Step=15500]	Loss=2.5453	274.5 examples/second
2022-01-11 03:33:58,586 - INFO - [Step=15750]	Loss=2.5381	275.5 examples/second
2022-01-11 03:35:00,090 - INFO - Test Loss=2.2856, Test top-1 acc=0.4812
2022-01-11 03:35:00,090 - INFO - Group Accuracy:

2022-01-11 03:35:00,090 - INFO - [0.9539759  0.96168673 0.9513253  0.98       0.95325303 0.97951806
 0.94939756 0.9624096  0.97156626 0.95662653 0.96771085 0.95662653
 0.9460241  0.94843376 0.95421684 0.9612048  0.98313254]
2022-01-11 03:35:00,092 - INFO - Saving...
2022-01-11 03:35:00,340 - INFO - Epoch time: 399.0481424331665
2022-01-11 03:35:00,340 - INFO - 
Epoch: 19
2022-01-11 03:35:00,340 - INFO - 
Learning Rate: 0.1000
2022-01-11 03:36:05,341 - INFO - [Step=16000]	Loss=2.5203	252.5 examples/second
2022-01-11 03:38:01,518 - INFO - [Step=16250]	Loss=2.5310	275.4 examples/second
2022-01-11 03:39:57,897 - INFO - [Step=16500]	Loss=2.4942	275.0 examples/second
2022-01-11 03:41:38,879 - INFO - Test Loss=2.3834, Test top-1 acc=0.4713
2022-01-11 03:41:38,880 - INFO - Group Accuracy:

2022-01-11 03:41:38,880 - INFO - [0.9498795  0.96385545 0.9544578  0.97108436 0.9549398  0.9737349
 0.9549398  0.9590362  0.97518075 0.9554217  0.9696385  0.96072286
 0.9472289  0.94963855 0.9448193  0.9628916  0.9838554 ]
2022-01-11 03:41:38,881 - INFO - Epoch time: 398.54034948349
2022-01-11 03:41:38,881 - INFO - 
Epoch: 20
2022-01-11 03:41:38,881 - INFO - 
Learning Rate: 0.1000
2022-01-11 03:42:04,074 - INFO - [Step=16750]	Loss=2.4884	253.6 examples/second
2022-01-11 03:44:00,499 - INFO - [Step=17000]	Loss=2.4460	274.9 examples/second
2022-01-11 03:45:56,675 - INFO - [Step=17250]	Loss=2.4210	275.4 examples/second
2022-01-11 03:47:53,347 - INFO - [Step=17500]	Loss=2.4285	274.3 examples/second
2022-01-11 03:48:17,425 - INFO - Test Loss=2.1595, Test top-1 acc=0.5041
2022-01-11 03:48:17,426 - INFO - Group Accuracy:

2022-01-11 03:48:17,426 - INFO - [0.9578313  0.96433735 0.95638555 0.9778313  0.95710844 0.9814458
 0.95421684 0.9590362  0.97301203 0.9595181  0.96819276 0.96168673
 0.9498795  0.94843376 0.9539759  0.9657831  0.98433733]
2022-01-11 03:48:17,427 - INFO - Saving...
2022-01-11 03:48:17,669 - INFO - Epoch time: 398.7884578704834
2022-01-11 03:48:17,669 - INFO - 
Epoch: 21
2022-01-11 03:48:17,669 - INFO - 
Learning Rate: 0.1000
2022-01-11 03:50:00,195 - INFO - [Step=17750]	Loss=2.4044	252.3 examples/second
2022-01-11 03:51:56,374 - INFO - [Step=18000]	Loss=2.3711	275.4 examples/second
2022-01-11 03:53:52,551 - INFO - [Step=18250]	Loss=2.3702	275.4 examples/second
2022-01-11 03:54:56,391 - INFO - Test Loss=2.1516, Test top-1 acc=0.5236
2022-01-11 03:54:56,391 - INFO - Group Accuracy:

2022-01-11 03:54:56,391 - INFO - [0.9561446  0.96843374 0.95638555 0.9785542  0.9561446  0.98289156
 0.9544578  0.96361446 0.973253   0.9592771  0.9655422  0.96048194
 0.9513253  0.9513253  0.95253015 0.9653012  0.9848193 ]
2022-01-11 03:54:56,392 - INFO - Saving...
2022-01-11 03:54:56,637 - INFO - Epoch time: 398.96732997894287
2022-01-11 03:54:56,637 - INFO - 
Epoch: 22
2022-01-11 03:54:56,637 - INFO - 
Learning Rate: 0.1000
2022-01-11 03:55:59,406 - INFO - [Step=18500]	Loss=2.3591	252.3 examples/second
2022-01-11 03:57:55,960 - INFO - [Step=18750]	Loss=2.3632	274.6 examples/second
2022-01-11 03:59:52,177 - INFO - [Step=19000]	Loss=2.3724	275.3 examples/second
2022-01-11 04:01:35,549 - INFO - Test Loss=2.1381, Test top-1 acc=0.5135
2022-01-11 04:01:35,549 - INFO - Group Accuracy:

2022-01-11 04:01:35,549 - INFO - [0.9592771  0.9612048  0.95325303 0.9737349  0.9539759  0.9855422
 0.9546988  0.96361446 0.97542167 0.9612048  0.96891564 0.966506
 0.94506025 0.9518072  0.95277107 0.966747   0.9860241 ]
2022-01-11 04:01:35,550 - INFO - Epoch time: 398.9130344390869
2022-01-11 04:01:35,550 - INFO - 
Epoch: 23
2022-01-11 04:01:35,550 - INFO - 
Learning Rate: 0.1000
2022-01-11 04:01:58,660 - INFO - [Step=19250]	Loss=2.3089	253.0 examples/second
2022-01-11 04:03:55,138 - INFO - [Step=19500]	Loss=2.2907	274.7 examples/second
2022-01-11 04:05:51,306 - INFO - [Step=19750]	Loss=2.2829	275.5 examples/second
2022-01-11 04:07:47,710 - INFO - [Step=20000]	Loss=2.2870	274.9 examples/second
2022-01-11 04:08:14,158 - INFO - Test Loss=2.1810, Test top-1 acc=0.5154
2022-01-11 04:08:14,158 - INFO - Group Accuracy:

2022-01-11 04:08:14,159 - INFO - [0.9580723  0.9626506  0.96       0.9775904  0.9580723  0.9833735
 0.9546988  0.96457833 0.9739759  0.9551807  0.9698795  0.96409637
 0.95253015 0.9520482  0.9539759  0.96385545 0.9848193 ]
2022-01-11 04:08:14,160 - INFO - Epoch time: 398.6096386909485
2022-01-11 04:08:14,160 - INFO - 
Epoch: 24
2022-01-11 04:08:14,160 - INFO - 
Learning Rate: 0.1000
2022-01-11 04:09:54,078 - INFO - [Step=20250]	Loss=2.2345	253.2 examples/second
2022-01-11 04:11:50,203 - INFO - [Step=20500]	Loss=2.2140	275.6 examples/second
2022-01-11 04:13:46,207 - INFO - [Step=20750]	Loss=2.2383	275.9 examples/second
2022-01-11 04:14:52,109 - INFO - Test Loss=2.0789, Test top-1 acc=0.5316
2022-01-11 04:14:52,109 - INFO - Group Accuracy:

2022-01-11 04:14:52,109 - INFO - [0.9583132  0.9662651  0.9554217  0.9807229  0.9583132  0.9850602
 0.95253015 0.966747   0.97542167 0.9621687  0.97108436 0.96072286
 0.9513253  0.95349395 0.9506024  0.96481925 0.9853012 ]
2022-01-11 04:14:52,110 - INFO - Saving...
2022-01-11 04:14:52,387 - INFO - Epoch time: 398.22755217552185
2022-01-11 04:14:52,388 - INFO - 
Epoch: 25
2022-01-11 04:14:52,388 - INFO - 
Learning Rate: 0.1000
2022-01-11 04:15:52,457 - INFO - [Step=21000]	Loss=2.2086	253.5 examples/second
2022-01-11 04:17:48,783 - INFO - [Step=21250]	Loss=2.2277	275.1 examples/second
2022-01-11 04:19:44,925 - INFO - [Step=21500]	Loss=2.2018	275.5 examples/second
2022-01-11 04:21:30,424 - INFO - Test Loss=2.1332, Test top-1 acc=0.5330
2022-01-11 04:21:30,424 - INFO - Group Accuracy:

2022-01-11 04:21:30,424 - INFO - [0.96048194 0.96698797 0.9549398  0.97518075 0.95710844 0.98313254
 0.94626504 0.9633735  0.973253   0.96361446 0.97180724 0.9633735
 0.9546988  0.95349395 0.9539759  0.9686747  0.9840964 ]
2022-01-11 04:21:30,425 - INFO - Saving...
2022-01-11 04:21:30,662 - INFO - Epoch time: 398.2749009132385
2022-01-11 04:21:30,663 - INFO - 
Epoch: 26
2022-01-11 04:21:30,663 - INFO - 
Learning Rate: 0.1000
2022-01-11 04:21:51,177 - INFO - [Step=21750]	Loss=2.1721	253.5 examples/second
2022-01-11 04:23:47,301 - INFO - [Step=22000]	Loss=2.1746	275.6 examples/second
2022-01-11 04:25:43,663 - INFO - [Step=22250]	Loss=2.1719	275.0 examples/second
2022-01-11 04:27:39,967 - INFO - [Step=22500]	Loss=2.1646	275.1 examples/second
2022-01-11 04:28:08,546 - INFO - Test Loss=1.9514, Test top-1 acc=0.5453
2022-01-11 04:28:08,547 - INFO - Group Accuracy:

2022-01-11 04:28:08,547 - INFO - [0.96024096 0.9693976  0.96433735 0.9771084  0.95686746 0.98240966
 0.9551807  0.96795183 0.9768675  0.9624096  0.9691566  0.9653012
 0.9518072  0.95759034 0.9580723  0.9653012  0.9855422 ]
2022-01-11 04:28:08,548 - INFO - Saving...
2022-01-11 04:28:08,781 - INFO - Epoch time: 398.1179986000061
2022-01-11 04:28:08,781 - INFO - 
Epoch: 27
2022-01-11 04:28:08,781 - INFO - 
Learning Rate: 0.1000
2022-01-11 04:29:45,955 - INFO - [Step=22750]	Loss=2.2170	254.0 examples/second
2022-01-11 04:31:42,274 - INFO - [Step=23000]	Loss=2.1349	275.1 examples/second
2022-01-11 04:33:38,591 - INFO - [Step=23250]	Loss=2.1243	275.1 examples/second
2022-01-11 04:34:47,173 - INFO - Test Loss=1.8597, Test top-1 acc=0.5655
2022-01-11 04:34:47,173 - INFO - Group Accuracy:

2022-01-11 04:34:47,173 - INFO - [0.9633735  0.96819276 0.9655422  0.98024094 0.95662653 0.9848193
 0.95710844 0.9693976  0.97445786 0.96433735 0.97156626 0.96771085
 0.9544578  0.9544578  0.95686746 0.9703615  0.9889157 ]
2022-01-11 04:34:47,174 - INFO - Saving...
2022-01-11 04:34:47,441 - INFO - Epoch time: 398.6603033542633
2022-01-11 04:34:47,442 - INFO - 
Epoch: 28
2022-01-11 04:34:47,442 - INFO - 
Learning Rate: 0.1000
2022-01-11 04:35:45,544 - INFO - [Step=23500]	Loss=2.1124	252.1 examples/second
2022-01-11 04:37:41,718 - INFO - [Step=23750]	Loss=2.0853	275.4 examples/second
2022-01-11 04:39:38,044 - INFO - [Step=24000]	Loss=2.1000	275.1 examples/second
2022-01-11 04:41:26,175 - INFO - Test Loss=2.0812, Test top-1 acc=0.5383
2022-01-11 04:41:26,175 - INFO - Group Accuracy:

2022-01-11 04:41:26,175 - INFO - [0.9612048  0.9708434  0.95975906 0.9708434  0.9626506  0.9848193
 0.9619277  0.9655422  0.973253   0.96409637 0.96891564 0.96433735
 0.9544578  0.95277107 0.9549398  0.96481925 0.9833735 ]
2022-01-11 04:41:26,177 - INFO - Epoch time: 398.7350661754608
2022-01-11 04:41:26,177 - INFO - 
Epoch: 29
2022-01-11 04:41:26,177 - INFO - 
Learning Rate: 0.0100
2022-01-11 04:41:44,563 - INFO - [Step=24250]	Loss=2.0484	252.9 examples/second
2022-01-11 04:43:40,916 - INFO - [Step=24500]	Loss=1.7955	275.0 examples/second
2022-01-11 04:45:37,239 - INFO - [Step=24750]	Loss=1.7294	275.1 examples/second
2022-01-11 04:47:33,657 - INFO - [Step=25000]	Loss=1.7234	274.9 examples/second
2022-01-11 04:48:04,592 - INFO - Test Loss=1.4977, Test top-1 acc=0.6422
2022-01-11 04:48:04,593 - INFO - Group Accuracy:

2022-01-11 04:48:04,593 - INFO - [0.9655422  0.9768675  0.9693976  0.98698795 0.9701205  0.9893976
 0.96481925 0.9725301  0.98289156 0.9713253  0.97903615 0.9713253
 0.9626506  0.96481925 0.9614458  0.97493976 0.9913253 ]
2022-01-11 04:48:04,594 - INFO - Saving...
2022-01-11 04:48:04,830 - INFO - Epoch time: 398.6527819633484
2022-01-11 04:48:04,830 - INFO - 
Epoch: 30
2022-01-11 04:48:04,830 - INFO - 
Learning Rate: 0.0100
2022-01-11 04:49:40,063 - INFO - [Step=25250]	Loss=1.6827	253.2 examples/second
2022-01-11 04:51:36,391 - INFO - [Step=25500]	Loss=1.6537	275.1 examples/second
2022-01-11 04:53:32,513 - INFO - [Step=25750]	Loss=1.6425	275.6 examples/second
2022-01-11 04:54:43,097 - INFO - Test Loss=1.4558, Test top-1 acc=0.6511
2022-01-11 04:54:43,097 - INFO - Group Accuracy:

2022-01-11 04:54:43,097 - INFO - [0.9655422  0.9780723  0.97108436 0.98746985 0.9706024  0.9893976
 0.96771085 0.9746988  0.9826506  0.9713253  0.9778313  0.97156626
 0.9633735  0.9653012  0.9633735  0.97518075 0.9913253 ]
2022-01-11 04:54:43,098 - INFO - Saving...
2022-01-11 04:54:43,338 - INFO - Epoch time: 398.50853991508484
2022-01-11 04:54:43,338 - INFO - 
Epoch: 31
2022-01-11 04:54:43,339 - INFO - 
Learning Rate: 0.0100
2022-01-11 04:55:38,928 - INFO - [Step=26000]	Loss=1.6563	253.1 examples/second
2022-01-11 04:57:35,117 - INFO - [Step=26250]	Loss=1.6193	275.4 examples/second
2022-01-11 04:59:31,195 - INFO - [Step=26500]	Loss=1.6334	275.7 examples/second
2022-01-11 05:01:21,489 - INFO - Test Loss=1.4391, Test top-1 acc=0.6504
2022-01-11 05:01:21,490 - INFO - Group Accuracy:

2022-01-11 05:01:21,490 - INFO - [0.9662651  0.9773494  0.97228914 0.98722893 0.9713253  0.98963857
 0.9662651  0.9737349  0.98361444 0.97108436 0.97831327 0.97180724
 0.9624096  0.9653012  0.9612048  0.97614455 0.99108434]
2022-01-11 05:01:21,490 - INFO - Epoch time: 398.15194964408875
2022-01-11 05:01:21,491 - INFO - 
Epoch: 32
2022-01-11 05:01:21,491 - INFO - 
Learning Rate: 0.0100
2022-01-11 05:01:37,618 - INFO - [Step=26750]	Loss=1.6147	253.1 examples/second
2022-01-11 05:03:33,950 - INFO - [Step=27000]	Loss=1.6078	275.1 examples/second
2022-01-11 05:05:30,377 - INFO - [Step=27250]	Loss=1.5973	274.9 examples/second
2022-01-11 05:07:26,829 - INFO - [Step=27500]	Loss=1.5873	274.8 examples/second
2022-01-11 05:08:00,256 - INFO - Test Loss=1.4062, Test top-1 acc=0.6643
2022-01-11 05:08:00,257 - INFO - Group Accuracy:

2022-01-11 05:08:00,257 - INFO - [0.966506   0.97927713 0.973253   0.98698795 0.9713253  0.99108434
 0.9662651  0.97566265 0.9838554  0.9706024  0.9804819  0.973494
 0.96409637 0.966747   0.9621687  0.97614455 0.99156624]
2022-01-11 05:08:00,258 - INFO - Saving...
2022-01-11 05:08:00,498 - INFO - Epoch time: 399.0070927143097
2022-01-11 05:08:00,498 - INFO - 
Epoch: 33
2022-01-11 05:08:00,498 - INFO - 
Learning Rate: 0.0100
2022-01-11 05:09:33,066 - INFO - [Step=27750]	Loss=1.5738	253.5 examples/second
2022-01-11 05:11:29,725 - INFO - [Step=28000]	Loss=1.5652	274.3 examples/second
2022-01-11 05:13:25,738 - INFO - [Step=28250]	Loss=1.5790	275.8 examples/second
2022-01-11 05:14:38,798 - INFO - Test Loss=1.3960, Test top-1 acc=0.6636
2022-01-11 05:14:38,798 - INFO - Group Accuracy:

2022-01-11 05:14:38,798 - INFO - [0.96819276 0.97903615 0.9742169  0.98746985 0.9706024  0.98963857
 0.96771085 0.97542167 0.98361444 0.9706024  0.97951806 0.9739759
 0.96433735 0.9657831  0.9626506  0.97590363 0.9918072 ]
2022-01-11 05:14:38,799 - INFO - Epoch time: 398.30093693733215
2022-01-11 05:14:38,799 - INFO - 
Epoch: 34
2022-01-11 05:14:38,799 - INFO - 
Learning Rate: 0.0100
2022-01-11 05:15:31,968 - INFO - [Step=28500]	Loss=1.5604	253.5 examples/second
2022-01-11 05:17:28,031 - INFO - [Step=28750]	Loss=1.5522	275.7 examples/second
2022-01-11 05:19:24,488 - INFO - [Step=29000]	Loss=1.5656	274.8 examples/second
2022-01-11 05:21:16,932 - INFO - Test Loss=1.3690, Test top-1 acc=0.6684
2022-01-11 05:21:16,933 - INFO - Group Accuracy:

2022-01-11 05:21:16,933 - INFO - [0.9674699  0.97831327 0.9746988  0.9879518  0.97204816 0.9922892
 0.9701205  0.97566265 0.9838554  0.9708434  0.9814458  0.97445786
 0.96433735 0.96385545 0.9626506  0.97590363 0.99156624]
2022-01-11 05:21:16,934 - INFO - Saving...
2022-01-11 05:21:17,171 - INFO - Epoch time: 398.3717291355133
2022-01-11 05:21:17,171 - INFO - 
Epoch: 35
2022-01-11 05:21:17,171 - INFO - 
Learning Rate: 0.0100
2022-01-11 05:21:30,934 - INFO - [Step=29250]	Loss=1.5814	253.1 examples/second
2022-01-11 05:23:27,026 - INFO - [Step=29500]	Loss=1.5425	275.6 examples/second
2022-01-11 05:25:23,409 - INFO - [Step=29750]	Loss=1.5333	275.0 examples/second
2022-01-11 05:27:19,661 - INFO - [Step=30000]	Loss=1.5450	275.3 examples/second
2022-01-11 05:27:55,189 - INFO - Test Loss=1.3775, Test top-1 acc=0.6689
2022-01-11 05:27:55,189 - INFO - Group Accuracy:

2022-01-11 05:27:55,189 - INFO - [0.96771085 0.9785542  0.9727711  0.9879518  0.9708434  0.99084336
 0.9691566  0.97542167 0.98361444 0.97156626 0.97927713 0.973494
 0.96481925 0.96698797 0.9633735  0.97542167 0.99156624]
2022-01-11 05:27:55,190 - INFO - Saving...
2022-01-11 05:27:55,427 - INFO - Epoch time: 398.2557644844055
2022-01-11 05:27:55,427 - INFO - 
Epoch: 36
2022-01-11 05:27:55,427 - INFO - 
Learning Rate: 0.0100
2022-01-11 05:29:25,689 - INFO - [Step=30250]	Loss=1.5594	253.9 examples/second
2022-01-11 05:31:21,858 - INFO - [Step=30500]	Loss=1.5368	275.5 examples/second
2022-01-11 05:33:18,180 - INFO - [Step=30750]	Loss=1.5265	275.1 examples/second
2022-01-11 05:34:33,280 - INFO - Test Loss=1.3606, Test top-1 acc=0.6675
2022-01-11 05:34:33,280 - INFO - Group Accuracy:

2022-01-11 05:34:33,280 - INFO - [0.9672289  0.9780723  0.97228914 0.9879518  0.9703615  0.9913253
 0.96698797 0.97614455 0.9838554  0.9701205  0.9787952  0.97445786
 0.9655422  0.96771085 0.9612048  0.97927713 0.99156624]
2022-01-11 05:34:33,281 - INFO - Epoch time: 397.85431385040283
2022-01-11 05:34:33,281 - INFO - 
Epoch: 37
2022-01-11 05:34:33,281 - INFO - 
Learning Rate: 0.0100
2022-01-11 05:35:24,171 - INFO - [Step=31000]	Loss=1.5308	254.0 examples/second
2022-01-11 05:37:20,116 - INFO - [Step=31250]	Loss=1.5125	276.0 examples/second
2022-01-11 05:39:16,345 - INFO - [Step=31500]	Loss=1.5147	275.3 examples/second
2022-01-11 05:41:10,868 - INFO - Test Loss=1.3540, Test top-1 acc=0.6766
2022-01-11 05:41:10,868 - INFO - Group Accuracy:

2022-01-11 05:41:10,868 - INFO - [0.96698797 0.9804819  0.9739759  0.9884337  0.9727711  0.99108434
 0.9701205  0.9768675  0.9840964  0.97108436 0.9807229  0.97542167
 0.96433735 0.96433735 0.96506023 0.9771084  0.99156624]
2022-01-11 05:41:10,869 - INFO - Saving...
2022-01-11 05:41:11,121 - INFO - Epoch time: 397.8395712375641
2022-01-11 05:41:11,121 - INFO - 
Epoch: 38
2022-01-11 05:41:11,121 - INFO - 
Learning Rate: 0.0100
2022-01-11 05:41:22,513 - INFO - [Step=31750]	Loss=1.5288	253.6 examples/second
2022-01-11 05:43:18,638 - INFO - [Step=32000]	Loss=1.5074	275.6 examples/second
2022-01-11 05:45:14,948 - INFO - [Step=32250]	Loss=1.5074	275.1 examples/second
2022-01-11 05:47:11,184 - INFO - [Step=32500]	Loss=1.5255	275.3 examples/second
2022-01-11 05:47:49,555 - INFO - Test Loss=1.3414, Test top-1 acc=0.6802
2022-01-11 05:47:49,556 - INFO - Group Accuracy:

2022-01-11 05:47:49,556 - INFO - [0.96795183 0.9768675  0.9737349  0.9886747  0.9701205  0.99060243
 0.97108436 0.9768675  0.98433733 0.97156626 0.9816868  0.9768675
 0.96506023 0.96698797 0.96457833 0.97951806 0.99108434]
2022-01-11 05:47:49,557 - INFO - Saving...
2022-01-11 05:47:49,810 - INFO - Epoch time: 398.68912863731384
2022-01-11 05:47:49,810 - INFO - 
Epoch: 39
2022-01-11 05:47:49,811 - INFO - 
Learning Rate: 0.0100
2022-01-11 05:49:18,289 - INFO - [Step=32750]	Loss=1.5157	251.8 examples/second
2022-01-11 05:51:14,448 - INFO - [Step=33000]	Loss=1.4965	275.5 examples/second
2022-01-11 05:53:10,573 - INFO - [Step=33250]	Loss=1.4764	275.6 examples/second
2022-01-11 05:54:27,871 - INFO - Test Loss=1.3485, Test top-1 acc=0.6684
2022-01-11 05:54:27,871 - INFO - Group Accuracy:

2022-01-11 05:54:27,871 - INFO - [0.96771085 0.9785542  0.97445786 0.9881928  0.97204816 0.99156624
 0.9708434  0.97638553 0.9840964  0.9686747  0.97903615 0.9742169
 0.96361446 0.9657831  0.96457833 0.9778313  0.9920482 ]
2022-01-11 05:54:27,872 - INFO - Epoch time: 398.061794757843
2022-01-11 05:54:27,872 - INFO - 
Epoch: 40
2022-01-11 05:54:27,872 - INFO - 
Learning Rate: 0.0100
2022-01-11 05:55:16,309 - INFO - [Step=33500]	Loss=1.4845	254.5 examples/second
2022-01-11 05:57:12,402 - INFO - [Step=33750]	Loss=1.4777	275.6 examples/second
2022-01-11 05:59:08,374 - INFO - [Step=34000]	Loss=1.4759	275.9 examples/second
2022-01-11 06:01:05,358 - INFO - Test Loss=1.3255, Test top-1 acc=0.6771
2022-01-11 06:01:05,358 - INFO - Group Accuracy:

2022-01-11 06:01:05,358 - INFO - [0.9693976  0.97975904 0.97566265 0.9884337  0.9706024  0.99060243
 0.97108436 0.9766265  0.9833735  0.97204816 0.9809638  0.97590363
 0.96481925 0.9660241  0.9633735  0.9787952  0.9913253 ]
2022-01-11 06:01:05,359 - INFO - Epoch time: 397.48666882514954
2022-01-11 06:01:05,359 - INFO - 
Epoch: 41
2022-01-11 06:01:05,359 - INFO - 
Learning Rate: 0.0100
2022-01-11 06:01:14,375 - INFO - [Step=34250]	Loss=1.4816	254.0 examples/second
2022-01-11 06:03:10,496 - INFO - [Step=34500]	Loss=1.4564	275.6 examples/second
2022-01-11 06:05:06,532 - INFO - [Step=34750]	Loss=1.4663	275.8 examples/second
2022-01-11 06:07:02,819 - INFO - [Step=35000]	Loss=1.4728	275.2 examples/second
2022-01-11 06:07:42,854 - INFO - Test Loss=1.3422, Test top-1 acc=0.6798
2022-01-11 06:07:42,854 - INFO - Group Accuracy:

2022-01-11 06:07:42,854 - INFO - [0.9672289  0.97951806 0.97590363 0.9884337  0.973494   0.99060243
 0.9698795  0.97614455 0.9848193  0.97108436 0.97927713 0.97542167
 0.96433735 0.9662651  0.96433735 0.97831327 0.9920482 ]
2022-01-11 06:07:42,855 - INFO - Epoch time: 397.496294260025
2022-01-11 06:07:42,855 - INFO - 
Epoch: 42
2022-01-11 06:07:42,855 - INFO - 
Learning Rate: 0.0100
2022-01-11 06:09:08,235 - INFO - [Step=35250]	Loss=1.4668	255.2 examples/second
2022-01-11 06:11:04,365 - INFO - [Step=35500]	Loss=1.4659	275.6 examples/second
2022-01-11 06:13:00,685 - INFO - [Step=35750]	Loss=1.4788	275.1 examples/second
2022-01-11 06:14:20,273 - INFO - Test Loss=1.3274, Test top-1 acc=0.6805
2022-01-11 06:14:20,273 - INFO - Group Accuracy:

2022-01-11 06:14:20,273 - INFO - [0.9693976  0.9809638  0.97445786 0.9879518  0.9708434  0.99156624
 0.9698795  0.97566265 0.9848193  0.97108436 0.9812048  0.97542167
 0.9662651  0.9672289  0.9655422  0.9787952  0.9920482 ]
2022-01-11 06:14:20,274 - INFO - Saving...
2022-01-11 06:14:20,501 - INFO - Epoch time: 397.6456060409546
2022-01-11 06:14:20,501 - INFO - 
Epoch: 43
2022-01-11 06:14:20,501 - INFO - 
Learning Rate: 0.0100
2022-01-11 06:15:06,412 - INFO - [Step=36000]	Loss=1.4333	254.5 examples/second
2022-01-11 06:17:02,455 - INFO - [Step=36250]	Loss=1.4534	275.8 examples/second
2022-01-11 06:18:58,808 - INFO - [Step=36500]	Loss=1.4624	275.0 examples/second
2022-01-11 06:20:57,874 - INFO - Test Loss=1.3187, Test top-1 acc=0.6848
2022-01-11 06:20:57,875 - INFO - Group Accuracy:

2022-01-11 06:20:57,875 - INFO - [0.96843374 0.9807229  0.97638553 0.9886747  0.97180724 0.99156624
 0.9693976  0.9768675  0.98361444 0.9727711  0.9804819  0.9778313
 0.96361446 0.966747   0.9624096  0.97927713 0.9922892 ]
2022-01-11 06:20:57,875 - INFO - Saving...
2022-01-11 06:20:58,115 - INFO - Epoch time: 397.6138246059418
2022-01-11 06:20:58,115 - INFO - 
Epoch: 44
2022-01-11 06:20:58,115 - INFO - 
Learning Rate: 0.0100
2022-01-11 06:21:05,095 - INFO - [Step=36750]	Loss=1.4316	253.4 examples/second
2022-01-11 06:23:01,089 - INFO - [Step=37000]	Loss=1.4348	275.9 examples/second
2022-01-11 06:24:57,092 - INFO - [Step=37250]	Loss=1.4483	275.9 examples/second
2022-01-11 06:26:53,580 - INFO - [Step=37500]	Loss=1.4417	274.7 examples/second
2022-01-11 06:27:35,725 - INFO - Test Loss=1.3114, Test top-1 acc=0.6805
2022-01-11 06:27:35,726 - INFO - Group Accuracy:

2022-01-11 06:27:35,726 - INFO - [0.96843374 0.97951806 0.97542167 0.9893976  0.9706024  0.99108434
 0.9698795  0.97566265 0.98361444 0.9713253  0.9804819  0.97518075
 0.9662651  0.9657831  0.9653012  0.97831327 0.9922892 ]
2022-01-11 06:27:35,727 - INFO - Epoch time: 397.6117045879364
2022-01-11 06:27:35,727 - INFO - 
Epoch: 45
2022-01-11 06:27:35,727 - INFO - 
Learning Rate: 0.0100
2022-01-11 06:28:58,983 - INFO - [Step=37750]	Loss=1.4330	255.2 examples/second
2022-01-11 06:30:55,297 - INFO - [Step=38000]	Loss=1.4523	275.1 examples/second
2022-01-11 06:32:51,488 - INFO - [Step=38250]	Loss=1.4365	275.4 examples/second
2022-01-11 06:34:13,625 - INFO - Test Loss=1.2950, Test top-1 acc=0.6841
2022-01-11 06:34:13,626 - INFO - Group Accuracy:

2022-01-11 06:34:13,626 - INFO - [0.96698797 0.9807229  0.97614455 0.9891566  0.9708434  0.9913253
 0.97180724 0.97638553 0.9855422  0.9727711  0.9816868  0.97566265
 0.96506023 0.96819276 0.9657831  0.9804819  0.9913253 ]
2022-01-11 06:34:13,627 - INFO - Epoch time: 397.8995485305786
2022-01-11 06:34:13,627 - INFO - 
Epoch: 46
2022-01-11 06:34:13,627 - INFO - 
Learning Rate: 0.0100
2022-01-11 06:34:57,778 - INFO - [Step=38500]	Loss=1.4332	253.4 examples/second
2022-01-11 06:36:53,724 - INFO - [Step=38750]	Loss=1.4286	276.0 examples/second
2022-01-11 06:38:49,648 - INFO - [Step=39000]	Loss=1.4074	276.0 examples/second
2022-01-11 06:40:51,352 - INFO - Test Loss=1.3084, Test top-1 acc=0.6824
2022-01-11 06:40:51,353 - INFO - Group Accuracy:

2022-01-11 06:40:51,353 - INFO - [0.96795183 0.9804819  0.97590363 0.9889157  0.97108436 0.99060243
 0.9693976  0.97614455 0.9833735  0.9727711  0.9816868  0.9771084
 0.9653012  0.9693976  0.96361446 0.97951806 0.9922892 ]
2022-01-11 06:40:51,354 - INFO - Epoch time: 397.72708344459534
2022-01-11 06:40:51,354 - INFO - 
Epoch: 47
2022-01-11 06:40:51,354 - INFO - 
Learning Rate: 0.0100
2022-01-11 06:40:55,848 - INFO - [Step=39250]	Loss=1.4174	253.6 examples/second
2022-01-11 06:42:51,738 - INFO - [Step=39500]	Loss=1.4156	276.1 examples/second
2022-01-11 06:44:47,748 - INFO - [Step=39750]	Loss=1.4104	275.8 examples/second
2022-01-11 06:46:44,120 - INFO - [Step=40000]	Loss=1.4105	275.0 examples/second
2022-01-11 06:47:28,789 - INFO - Test Loss=1.2965, Test top-1 acc=0.6865
2022-01-11 06:47:28,789 - INFO - Group Accuracy:

2022-01-11 06:47:28,789 - INFO - [0.96698797 0.9804819  0.97445786 0.9889157  0.97156626 0.99060243
 0.9698795  0.97518075 0.9840964  0.97180724 0.9816868  0.9780723
 0.96457833 0.9674699  0.96409637 0.98024094 0.9925301 ]
2022-01-11 06:47:28,791 - INFO - Saving...
2022-01-11 06:47:29,029 - INFO - Epoch time: 397.67518949508667
2022-01-11 06:47:29,029 - INFO - 
Epoch: 48
2022-01-11 06:47:29,029 - INFO - 
Learning Rate: 0.0100
2022-01-11 06:48:50,097 - INFO - [Step=40250]	Loss=1.3829	254.0 examples/second
2022-01-11 06:50:46,069 - INFO - [Step=40500]	Loss=1.4139	275.9 examples/second
2022-01-11 06:52:42,011 - INFO - [Step=40750]	Loss=1.3951	276.0 examples/second
2022-01-11 06:54:06,251 - INFO - Test Loss=1.3072, Test top-1 acc=0.6884
2022-01-11 06:54:06,252 - INFO - Group Accuracy:

2022-01-11 06:54:06,252 - INFO - [0.96795183 0.98024094 0.9766265  0.9891566  0.973253   0.9918072
 0.9693976  0.97493976 0.9845783  0.97204816 0.9826506  0.97518075
 0.966506   0.9672289  0.96457833 0.9819277  0.9922892 ]
2022-01-11 06:54:06,253 - INFO - Saving...
2022-01-11 06:54:06,506 - INFO - Epoch time: 397.4769649505615
2022-01-11 06:54:06,507 - INFO - 
Epoch: 49
2022-01-11 06:54:06,507 - INFO - 
Learning Rate: 0.0100
2022-01-11 06:54:48,283 - INFO - [Step=41000]	Loss=1.3968	253.4 examples/second
2022-01-11 06:56:44,557 - INFO - [Step=41250]	Loss=1.3941	275.2 examples/second
2022-01-11 06:58:40,680 - INFO - [Step=41500]	Loss=1.3972	275.6 examples/second
2022-01-11 07:00:36,797 - INFO - [Step=41750]	Loss=1.4152	275.6 examples/second
2022-01-11 07:00:44,379 - INFO - Test Loss=1.2863, Test top-1 acc=0.6923
2022-01-11 07:00:44,379 - INFO - Group Accuracy:

2022-01-11 07:00:44,379 - INFO - [0.966747   0.9807229  0.97493976 0.9886747  0.9739759  0.99156624
 0.9706024  0.9768675  0.9833735  0.97156626 0.9812048  0.9785542
 0.96843374 0.9674699  0.9657831  0.98024094 0.9930121 ]
2022-01-11 07:00:44,380 - INFO - Saving...
2022-01-11 07:00:44,618 - INFO - Epoch time: 398.11129117012024
2022-01-11 07:00:44,618 - INFO - 
Epoch: 50
2022-01-11 07:00:44,618 - INFO - 
Learning Rate: 0.0100
2022-01-11 07:02:42,614 - INFO - [Step=42000]	Loss=1.3865	254.3 examples/second
2022-01-11 07:04:38,886 - INFO - [Step=42250]	Loss=1.3903	275.2 examples/second
2022-01-11 07:06:35,120 - INFO - [Step=42500]	Loss=1.4054	275.3 examples/second
2022-01-11 07:07:22,209 - INFO - Test Loss=1.2752, Test top-1 acc=0.6911
2022-01-11 07:07:22,209 - INFO - Group Accuracy:

2022-01-11 07:07:22,209 - INFO - [0.9698795  0.9812048  0.97614455 0.9893976  0.973494   0.99108434
 0.9706024  0.97590363 0.9838554  0.97180724 0.9821687  0.9773494
 0.966747   0.966747   0.9657831  0.9807229  0.9922892 ]
2022-01-11 07:07:22,210 - INFO - Epoch time: 397.59220695495605
2022-01-11 07:07:22,210 - INFO - 
Epoch: 51
2022-01-11 07:07:22,210 - INFO - 
Learning Rate: 0.0100
2022-01-11 07:08:40,869 - INFO - [Step=42750]	Loss=1.3729	254.5 examples/second
2022-01-11 07:10:37,003 - INFO - [Step=43000]	Loss=1.3733	275.5 examples/second
2022-01-11 07:12:33,041 - INFO - [Step=43250]	Loss=1.3777	275.8 examples/second
2022-01-11 07:13:59,760 - INFO - Test Loss=1.2913, Test top-1 acc=0.6875
2022-01-11 07:13:59,761 - INFO - Group Accuracy:

2022-01-11 07:13:59,761 - INFO - [0.9706024  0.98024094 0.9775904  0.9886747  0.97228914 0.99060243
 0.97108436 0.9739759  0.9850602  0.97156626 0.9816868  0.97638553
 0.96698797 0.9674699  0.96433735 0.97975904 0.9922892 ]
2022-01-11 07:13:59,762 - INFO - Epoch time: 397.5514008998871
2022-01-11 07:13:59,762 - INFO - 
Epoch: 52
2022-01-11 07:13:59,762 - INFO - 
Learning Rate: 0.0100
2022-01-11 07:14:38,820 - INFO - [Step=43500]	Loss=1.4026	254.4 examples/second
2022-01-11 07:16:34,704 - INFO - [Step=43750]	Loss=1.3679	276.1 examples/second
2022-01-11 07:18:30,904 - INFO - [Step=44000]	Loss=1.4120	275.4 examples/second
2022-01-11 07:20:27,214 - INFO - [Step=44250]	Loss=1.3625	275.1 examples/second
2022-01-11 07:20:37,372 - INFO - Test Loss=1.2843, Test top-1 acc=0.6959
2022-01-11 07:20:37,372 - INFO - Group Accuracy:

2022-01-11 07:20:37,372 - INFO - [0.9696385  0.9809638  0.9775904  0.98963857 0.9725301  0.99084336
 0.97204816 0.97566265 0.9850602  0.9725301  0.98240966 0.9775904
 0.9660241  0.96795183 0.9657831  0.9809638  0.9920482 ]
2022-01-11 07:20:37,373 - INFO - Saving...
2022-01-11 07:20:37,587 - INFO - Epoch time: 397.82552886009216
2022-01-11 07:20:37,588 - INFO - 
Epoch: 53
2022-01-11 07:20:37,588 - INFO - 
Learning Rate: 0.0100
2022-01-11 07:22:33,494 - INFO - [Step=44500]	Loss=1.3666	253.4 examples/second
2022-01-11 07:24:29,497 - INFO - [Step=44750]	Loss=1.3643	275.9 examples/second
2022-01-11 07:26:25,684 - INFO - [Step=45000]	Loss=1.3634	275.4 examples/second
2022-01-11 07:27:15,036 - INFO - Test Loss=1.2860, Test top-1 acc=0.6851
2022-01-11 07:27:15,036 - INFO - Group Accuracy:

2022-01-11 07:27:15,036 - INFO - [0.97204816 0.9809638  0.9773494  0.9891566  0.973253   0.99108434
 0.9713253  0.97518075 0.9845783  0.9708434  0.9814458  0.97638553
 0.9660241  0.9662651  0.96433735 0.97903615 0.99156624]
2022-01-11 07:27:15,037 - INFO - Epoch time: 397.4497084617615
2022-01-11 07:27:15,037 - INFO - 
Epoch: 54
2022-01-11 07:27:15,037 - INFO - 
Learning Rate: 0.0100
2022-01-11 07:28:31,457 - INFO - [Step=45250]	Loss=1.3591	254.4 examples/second
2022-01-11 07:30:27,480 - INFO - [Step=45500]	Loss=1.3562	275.8 examples/second
2022-01-11 07:32:23,583 - INFO - [Step=45750]	Loss=1.3731	275.6 examples/second
2022-01-11 07:33:52,836 - INFO - Test Loss=1.2688, Test top-1 acc=0.6993
2022-01-11 07:33:52,837 - INFO - Group Accuracy:

2022-01-11 07:33:52,837 - INFO - [0.97180724 0.9821687  0.97542167 0.98963857 0.973253   0.99084336
 0.97108436 0.9771084  0.98433733 0.9725301  0.9812048  0.9773494
 0.966747   0.9693976  0.966506   0.9804819  0.9920482 ]
2022-01-11 07:33:52,838 - INFO - Saving...
2022-01-11 07:33:53,086 - INFO - Epoch time: 398.04812502861023
2022-01-11 07:33:53,086 - INFO - 
Epoch: 55
2022-01-11 07:33:53,086 - INFO - 
Learning Rate: 0.0100
2022-01-11 07:34:30,312 - INFO - [Step=46000]	Loss=1.3616	252.5 examples/second
2022-01-11 07:36:26,636 - INFO - [Step=46250]	Loss=1.3344	275.1 examples/second
2022-01-11 07:38:22,870 - INFO - [Step=46500]	Loss=1.3583	275.3 examples/second
2022-01-11 07:40:19,222 - INFO - [Step=46750]	Loss=1.3597	275.0 examples/second
2022-01-11 07:40:31,855 - INFO - Test Loss=1.2565, Test top-1 acc=0.6940
2022-01-11 07:40:31,856 - INFO - Group Accuracy:

2022-01-11 07:40:31,856 - INFO - [0.9713253  0.9840964  0.9773494  0.9889157  0.9739759  0.99084336
 0.97180724 0.97590363 0.98433733 0.9727711  0.9826506  0.97638553
 0.966506   0.96795183 0.9655422  0.9807229  0.9922892 ]
2022-01-11 07:40:31,857 - INFO - Epoch time: 398.7711715698242
2022-01-11 07:40:31,857 - INFO - 
Epoch: 56
2022-01-11 07:40:31,857 - INFO - 
Learning Rate: 0.0100
2022-01-11 07:42:25,703 - INFO - [Step=47000]	Loss=1.3433	253.0 examples/second
2022-01-11 07:44:21,689 - INFO - [Step=47250]	Loss=1.3314	275.9 examples/second
2022-01-11 07:46:17,819 - INFO - [Step=47500]	Loss=1.3595	275.6 examples/second
2022-01-11 07:47:09,440 - INFO - Test Loss=1.2514, Test top-1 acc=0.6964
2022-01-11 07:47:09,441 - INFO - Group Accuracy:

2022-01-11 07:47:09,441 - INFO - [0.9703615  0.9809638  0.97590363 0.9893976  0.97228914 0.99060243
 0.973253   0.9766265  0.9850602  0.9737349  0.9833735  0.97638553
 0.9696385  0.9686747  0.96409637 0.9809638  0.9930121 ]
2022-01-11 07:47:09,442 - INFO - Epoch time: 397.5844626426697
2022-01-11 07:47:09,442 - INFO - 
Epoch: 57
2022-01-11 07:47:09,442 - INFO - 
Learning Rate: 0.0100
2022-01-11 07:48:23,440 - INFO - [Step=47750]	Loss=1.3326	254.7 examples/second
2022-01-11 07:50:19,458 - INFO - [Step=48000]	Loss=1.3378	275.8 examples/second
2022-01-11 07:52:15,589 - INFO - [Step=48250]	Loss=1.3428	275.6 examples/second
2022-01-11 07:53:46,876 - INFO - Test Loss=1.2494, Test top-1 acc=0.6964
2022-01-11 07:53:46,876 - INFO - Group Accuracy:

2022-01-11 07:53:46,876 - INFO - [0.9713253  0.9816868  0.97566265 0.9886747  0.9725301  0.99108434
 0.97180724 0.97445786 0.9848193  0.97228914 0.9838554  0.9773494
 0.9672289  0.9698795  0.9657831  0.9804819  0.99084336]
2022-01-11 07:53:46,877 - INFO - Epoch time: 397.4356327056885
2022-01-11 07:53:46,877 - INFO - 
Epoch: 58
2022-01-11 07:53:46,877 - INFO - 
Learning Rate: 0.0100
2022-01-11 07:54:21,391 - INFO - [Step=48500]	Loss=1.3434	254.4 examples/second
2022-01-11 07:56:17,945 - INFO - [Step=48750]	Loss=1.3512	274.6 examples/second
2022-01-11 07:58:14,194 - INFO - [Step=49000]	Loss=1.3286	275.3 examples/second
2022-01-11 08:00:10,393 - INFO - [Step=49250]	Loss=1.3335	275.4 examples/second
2022-01-11 08:00:24,592 - INFO - Test Loss=1.2374, Test top-1 acc=0.7014
2022-01-11 08:00:24,593 - INFO - Group Accuracy:

2022-01-11 08:00:24,593 - INFO - [0.9693976  0.9814458  0.97566265 0.9901205  0.9739759  0.99060243
 0.97204816 0.97614455 0.9853012  0.9737349  0.9840964  0.97951806
 0.96795183 0.96891564 0.96433735 0.9814458  0.9927711 ]
2022-01-11 08:00:24,594 - INFO - Saving...
2022-01-11 08:00:24,831 - INFO - Epoch time: 397.95342683792114
2022-01-11 08:00:24,831 - INFO - 
Epoch: 59
2022-01-11 08:00:24,831 - INFO - 
Learning Rate: 0.0010
2022-01-11 08:02:16,069 - INFO - [Step=49500]	Loss=1.2961	254.6 examples/second
2022-01-11 08:04:12,410 - INFO - [Step=49750]	Loss=1.3001	275.1 examples/second
2022-01-11 08:06:08,702 - INFO - [Step=50000]	Loss=1.2718	275.2 examples/second
2022-01-11 08:07:02,940 - INFO - Test Loss=1.2156, Test top-1 acc=0.7063
2022-01-11 08:07:02,940 - INFO - Group Accuracy:

2022-01-11 08:07:02,940 - INFO - [0.97156626 0.98240966 0.97638553 0.98963857 0.9737349  0.99036145
 0.97180724 0.97614455 0.98578316 0.973253   0.9853012  0.9780723
 0.9674699  0.9706024  0.966506   0.9826506  0.9925301 ]
2022-01-11 08:07:02,941 - INFO - Saving...
2022-01-11 08:07:03,185 - INFO - Epoch time: 398.35358357429504
2022-01-11 08:07:03,185 - INFO - 
Epoch: 60
2022-01-11 08:07:03,185 - INFO - 
Learning Rate: 0.0010
2022-01-11 08:08:15,079 - INFO - [Step=50250]	Loss=1.2763	253.2 examples/second
2022-01-11 08:10:11,236 - INFO - [Step=50500]	Loss=1.2822	275.5 examples/second
2022-01-11 08:12:07,427 - INFO - [Step=50750]	Loss=1.2680	275.4 examples/second
2022-01-11 08:13:40,985 - INFO - Test Loss=1.2149, Test top-1 acc=0.7070
2022-01-11 08:13:40,985 - INFO - Group Accuracy:

2022-01-11 08:13:40,986 - INFO - [0.97108436 0.98289156 0.9768675  0.9901205  0.9742169  0.99084336
 0.97180724 0.97614455 0.9855422  0.973253   0.9848193  0.97903615
 0.96819276 0.97156626 0.9653012  0.98240966 0.9927711 ]
2022-01-11 08:13:40,987 - INFO - Saving...
2022-01-11 08:13:41,229 - INFO - Epoch time: 398.04433488845825
2022-01-11 08:13:41,229 - INFO - 
Epoch: 61
2022-01-11 08:13:41,229 - INFO - 
Learning Rate: 0.0010
2022-01-11 08:14:13,356 - INFO - [Step=51000]	Loss=1.2764	254.1 examples/second
2022-01-11 08:16:09,645 - INFO - [Step=51250]	Loss=1.2896	275.2 examples/second
2022-01-11 08:18:06,135 - INFO - [Step=51500]	Loss=1.2773	274.7 examples/second
2022-01-11 08:20:02,533 - INFO - [Step=51750]	Loss=1.2863	274.9 examples/second
2022-01-11 08:20:19,115 - INFO - Test Loss=1.2085, Test top-1 acc=0.7060
2022-01-11 08:20:19,115 - INFO - Group Accuracy:

2022-01-11 08:20:19,115 - INFO - [0.9701205  0.9816868  0.9771084  0.9886747  0.973253   0.98963857
 0.97180724 0.97638553 0.9850602  0.97301203 0.9850602  0.97927713
 0.966747   0.9698795  0.9657831  0.9826506  0.9925301 ]
2022-01-11 08:20:19,116 - INFO - Epoch time: 397.88674688339233
2022-01-11 08:20:19,116 - INFO - 
Epoch: 62
2022-01-11 08:20:19,116 - INFO - 
Learning Rate: 0.0010
2022-01-11 08:22:07,795 - INFO - [Step=52000]	Loss=1.2742	255.5 examples/second
2022-01-11 08:24:03,925 - INFO - [Step=52250]	Loss=1.2634	275.6 examples/second
2022-01-11 08:26:00,321 - INFO - [Step=52500]	Loss=1.2859	274.9 examples/second
2022-01-11 08:26:56,694 - INFO - Test Loss=1.2183, Test top-1 acc=0.7029
2022-01-11 08:26:56,695 - INFO - Group Accuracy:

2022-01-11 08:26:56,695 - INFO - [0.9708434  0.98240966 0.9768675  0.9891566  0.9739759  0.99060243
 0.97108436 0.97542167 0.9853012  0.9727711  0.9848193  0.97927713
 0.96795183 0.9693976  0.9655422  0.9821687  0.9925301 ]
2022-01-11 08:26:56,696 - INFO - Epoch time: 397.5795590877533
2022-01-11 08:26:56,696 - INFO - 
Epoch: 63
2022-01-11 08:26:56,696 - INFO - 
Learning Rate: 0.0010
2022-01-11 08:28:05,861 - INFO - [Step=52750]	Loss=1.2602	254.9 examples/second
2022-01-11 08:30:02,114 - INFO - [Step=53000]	Loss=1.2833	275.3 examples/second
2022-01-11 08:31:58,288 - INFO - [Step=53250]	Loss=1.2767	275.5 examples/second
2022-01-11 08:33:34,213 - INFO - Test Loss=1.2106, Test top-1 acc=0.7092
2022-01-11 08:33:34,213 - INFO - Group Accuracy:

2022-01-11 08:33:34,213 - INFO - [0.9703615  0.9833735  0.97831327 0.9891566  0.9742169  0.99084336
 0.97301203 0.9768675  0.9855422  0.973253   0.9855422  0.9785542
 0.96771085 0.9703615  0.9660241  0.9819277  0.9922892 ]
2022-01-11 08:33:34,214 - INFO - Saving...
2022-01-11 08:33:34,455 - INFO - Epoch time: 397.75910782814026
2022-01-11 08:33:34,455 - INFO - 
Epoch: 64
2022-01-11 08:33:34,455 - INFO - 
Learning Rate: 0.0010
2022-01-11 08:34:04,199 - INFO - [Step=53500]	Loss=1.2839	254.1 examples/second
2022-01-11 08:36:00,120 - INFO - [Step=53750]	Loss=1.2712	276.1 examples/second
2022-01-11 08:37:56,018 - INFO - [Step=54000]	Loss=1.2640	276.1 examples/second
2022-01-11 08:39:51,993 - INFO - [Step=54250]	Loss=1.2828	275.9 examples/second
2022-01-11 08:40:11,059 - INFO - Test Loss=1.2105, Test top-1 acc=0.7092
2022-01-11 08:40:11,059 - INFO - Group Accuracy:

2022-01-11 08:40:11,059 - INFO - [0.97108436 0.9840964  0.9780723  0.9893976  0.9739759  0.99108434
 0.97204816 0.97493976 0.9855422  0.97301203 0.9845783  0.97927713
 0.96843374 0.9701205  0.9655422  0.9826506  0.9925301 ]
2022-01-11 08:40:11,060 - INFO - Epoch time: 396.6044931411743
2022-01-11 08:40:11,060 - INFO - 
Epoch: 65
2022-01-11 08:40:11,060 - INFO - 
Learning Rate: 0.0010
2022-01-11 08:41:57,260 - INFO - [Step=54500]	Loss=1.2942	255.5 examples/second
2022-01-11 08:43:53,410 - INFO - [Step=54750]	Loss=1.2576	275.5 examples/second
2022-01-11 08:45:49,654 - INFO - [Step=55000]	Loss=1.2617	275.3 examples/second
2022-01-11 08:46:47,972 - INFO - Test Loss=1.2098, Test top-1 acc=0.7092
2022-01-11 08:46:47,972 - INFO - Group Accuracy:

2022-01-11 08:46:47,972 - INFO - [0.97180724 0.9826506  0.9773494  0.9893976  0.97445786 0.99084336
 0.97228914 0.97638553 0.9853012  0.973494   0.98433733 0.9787952
 0.966506   0.9698795  0.9660241  0.98289156 0.9927711 ]
2022-01-11 08:46:47,973 - INFO - Epoch time: 396.91318702697754
2022-01-11 08:46:47,973 - INFO - 
Epoch: 66
2022-01-11 08:46:47,973 - INFO - 
Learning Rate: 0.0010
2022-01-11 08:47:54,945 - INFO - [Step=55250]	Loss=1.2816	255.4 examples/second
2022-01-11 08:49:50,895 - INFO - [Step=55500]	Loss=1.2595	276.0 examples/second
2022-01-11 08:51:46,871 - INFO - [Step=55750]	Loss=1.2758	275.9 examples/second
2022-01-11 08:53:24,819 - INFO - Test Loss=1.2133, Test top-1 acc=0.7070
2022-01-11 08:53:24,819 - INFO - Group Accuracy:

2022-01-11 08:53:24,819 - INFO - [0.9713253  0.9833735  0.9771084  0.9898795  0.9739759  0.99060243
 0.9727711  0.97542167 0.98578316 0.9727711  0.9838554  0.9778313
 0.9691566  0.9701205  0.966506   0.98240966 0.993253  ]
2022-01-11 08:53:24,820 - INFO - Epoch time: 396.8469498157501
2022-01-11 08:53:24,820 - INFO - 
Epoch: 67
2022-01-11 08:53:24,820 - INFO - 
Learning Rate: 0.0010
2022-01-11 08:53:52,113 - INFO - [Step=56000]	Loss=1.2557	255.5 examples/second
2022-01-11 08:55:48,257 - INFO - [Step=56250]	Loss=1.2688	275.5 examples/second
2022-01-11 08:57:44,516 - INFO - [Step=56500]	Loss=1.2663	275.2 examples/second
2022-01-11 08:59:40,781 - INFO - [Step=56750]	Loss=1.2691	275.2 examples/second
2022-01-11 09:00:02,188 - INFO - Test Loss=1.2121, Test top-1 acc=0.7111
2022-01-11 09:00:02,188 - INFO - Group Accuracy:

2022-01-11 09:00:02,188 - INFO - [0.97108436 0.98289156 0.9775904  0.9891566  0.97445786 0.99060243
 0.9713253  0.97590363 0.9850602  0.973253   0.9838554  0.9787952
 0.96819276 0.9706024  0.9674699  0.98240966 0.9927711 ]
2022-01-11 09:00:02,189 - INFO - Saving...
2022-01-11 09:00:02,415 - INFO - Epoch time: 397.5946545600891
2022-01-11 09:00:02,415 - INFO - 
Epoch: 68
2022-01-11 09:00:02,415 - INFO - 
Learning Rate: 0.0010
2022-01-11 09:01:46,285 - INFO - [Step=57000]	Loss=1.2641	255.0 examples/second
2022-01-11 09:03:42,732 - INFO - [Step=57250]	Loss=1.2673	274.8 examples/second
2022-01-11 09:05:39,011 - INFO - [Step=57500]	Loss=1.2597	275.2 examples/second
2022-01-11 09:06:39,675 - INFO - Test Loss=1.2032, Test top-1 acc=0.7096
2022-01-11 09:06:39,676 - INFO - Group Accuracy:

2022-01-11 09:06:39,676 - INFO - [0.9703615  0.98313254 0.9775904  0.9898795  0.9737349  0.9920482
 0.97204816 0.97493976 0.9848193  0.97301203 0.98433733 0.97951806
 0.96795183 0.9706024  0.9660241  0.9826506  0.9927711 ]
2022-01-11 09:06:39,677 - INFO - Epoch time: 397.261581659317
2022-01-11 09:06:39,677 - INFO - 
Epoch: 69
2022-01-11 09:06:39,677 - INFO - 
Learning Rate: 0.0010
2022-01-11 09:07:44,396 - INFO - [Step=57750]	Loss=1.2774	255.2 examples/second
2022-01-11 09:09:40,618 - INFO - [Step=58000]	Loss=1.2637	275.3 examples/second
2022-01-11 09:11:36,853 - INFO - [Step=58250]	Loss=1.2784	275.3 examples/second
2022-01-11 09:13:17,390 - INFO - Test Loss=1.2069, Test top-1 acc=0.7080
2022-01-11 09:13:17,390 - INFO - Group Accuracy:

2022-01-11 09:13:17,390 - INFO - [0.97108436 0.9821687  0.9771084  0.9893976  0.973494   0.99108434
 0.97204816 0.97493976 0.9853012  0.9739759  0.9845783  0.97831327
 0.96819276 0.9696385  0.966506   0.9826506  0.9925301 ]
2022-01-11 09:13:17,391 - INFO - Epoch time: 397.71453738212585
2022-01-11 09:13:17,391 - INFO - 
Epoch: 70
2022-01-11 09:13:17,391 - INFO - 
Learning Rate: 0.0010
2022-01-11 09:13:42,721 - INFO - [Step=58500]	Loss=1.2569	254.2 examples/second
2022-01-11 09:15:38,716 - INFO - [Step=58750]	Loss=1.2557	275.9 examples/second
2022-01-11 09:17:34,799 - INFO - [Step=59000]	Loss=1.2785	275.7 examples/second
2022-01-11 09:19:31,033 - INFO - [Step=59250]	Loss=1.2786	275.3 examples/second
2022-01-11 09:19:55,094 - INFO - Test Loss=1.2082, Test top-1 acc=0.7087
2022-01-11 09:19:55,094 - INFO - Group Accuracy:

2022-01-11 09:19:55,095 - INFO - [0.9713253  0.9833735  0.9778313  0.9893976  0.973253   0.99156624
 0.97180724 0.9768675  0.9850602  0.973494   0.9845783  0.97927713
 0.96819276 0.9693976  0.9660241  0.98240966 0.9925301 ]
2022-01-11 09:19:55,095 - INFO - Epoch time: 397.70409178733826
2022-01-11 09:19:55,095 - INFO - 
Epoch: 71
2022-01-11 09:19:55,095 - INFO - 
Learning Rate: 0.0010
2022-01-11 09:21:37,282 - INFO - [Step=59500]	Loss=1.2549	253.5 examples/second
2022-01-11 09:23:33,531 - INFO - [Step=59750]	Loss=1.2670	275.3 examples/second
2022-01-11 09:25:29,784 - INFO - [Step=60000]	Loss=1.2612	275.3 examples/second
2022-01-11 09:26:33,245 - INFO - Test Loss=1.2035, Test top-1 acc=0.7099
2022-01-11 09:26:33,245 - INFO - Group Accuracy:

2022-01-11 09:26:33,245 - INFO - [0.97156626 0.9826506  0.9787952  0.9889157  0.973253   0.99084336
 0.97228914 0.9780723  0.9855422  0.9739759  0.9845783  0.97951806
 0.96771085 0.9701205  0.966506   0.98289156 0.9925301 ]
2022-01-11 09:26:33,246 - INFO - Epoch time: 398.1508309841156
2022-01-11 09:26:33,246 - INFO - 
Epoch: 72
2022-01-11 09:26:33,246 - INFO - 
Learning Rate: 0.0010
2022-01-11 09:27:35,824 - INFO - [Step=60250]	Loss=1.2722	253.9 examples/second
2022-01-11 09:29:31,775 - INFO - [Step=60500]	Loss=1.2545	276.0 examples/second
2022-01-11 09:31:27,911 - INFO - [Step=60750]	Loss=1.2657	275.5 examples/second
2022-01-11 09:33:10,751 - INFO - Test Loss=1.2045, Test top-1 acc=0.7118
2022-01-11 09:33:10,751 - INFO - Group Accuracy:

2022-01-11 09:33:10,751 - INFO - [0.97228914 0.98361444 0.9785542  0.98963857 0.9737349  0.99060243
 0.9727711  0.97542167 0.9850602  0.973494   0.9850602  0.97951806
 0.9686747  0.9701205  0.9662651  0.98289156 0.9930121 ]
2022-01-11 09:33:10,752 - INFO - Saving...
2022-01-11 09:33:10,999 - INFO - Epoch time: 397.7528831958771
2022-01-11 09:33:10,999 - INFO - 
Epoch: 73
2022-01-11 09:33:11,000 - INFO - 
Learning Rate: 0.0010
2022-01-11 09:33:34,073 - INFO - [Step=61000]	Loss=1.2741	253.6 examples/second
2022-01-11 09:35:30,097 - INFO - [Step=61250]	Loss=1.2525	275.8 examples/second
2022-01-11 09:37:26,455 - INFO - [Step=61500]	Loss=1.2566	275.0 examples/second
2022-01-11 09:39:22,635 - INFO - [Step=61750]	Loss=1.2717	275.4 examples/second
2022-01-11 09:39:48,915 - INFO - Test Loss=1.2007, Test top-1 acc=0.7130
2022-01-11 09:39:48,915 - INFO - Group Accuracy:

2022-01-11 09:39:48,915 - INFO - [0.9708434  0.98240966 0.9780723  0.9893976  0.9739759  0.9913253
 0.9713253  0.9768675  0.9850602  0.9739759  0.9850602  0.9787952
 0.96771085 0.9708434  0.9657831  0.9833735  0.9925301 ]
2022-01-11 09:39:48,916 - INFO - Saving...
2022-01-11 09:39:49,161 - INFO - Epoch time: 398.1616668701172
2022-01-11 09:39:49,161 - INFO - 
Epoch: 74
2022-01-11 09:39:49,161 - INFO - 
Learning Rate: 0.0010
2022-01-11 09:41:29,157 - INFO - [Step=62000]	Loss=1.2537	252.9 examples/second
2022-01-11 09:43:25,607 - INFO - [Step=62250]	Loss=1.2613	274.8 examples/second
2022-01-11 09:45:22,216 - INFO - [Step=62500]	Loss=1.2490	274.4 examples/second
2022-01-11 09:46:27,980 - INFO - Test Loss=1.2036, Test top-1 acc=0.7101
2022-01-11 09:46:27,981 - INFO - Group Accuracy:

2022-01-11 09:46:27,981 - INFO - [0.9713253  0.98313254 0.97831327 0.9893976  0.973494   0.99108434
 0.97301203 0.97518075 0.9855422  0.973253   0.98433733 0.97927713
 0.96771085 0.9693976  0.9662651  0.9833735  0.9927711 ]
2022-01-11 09:46:27,982 - INFO - Epoch time: 398.82038974761963
2022-01-11 09:46:27,982 - INFO - 
Epoch: 75
2022-01-11 09:46:27,982 - INFO - 
Learning Rate: 0.0010
2022-01-11 09:47:28,383 - INFO - [Step=62750]	Loss=1.2605	253.6 examples/second
2022-01-11 09:49:24,814 - INFO - [Step=63000]	Loss=1.2462	274.8 examples/second
2022-01-11 09:51:21,131 - INFO - [Step=63250]	Loss=1.2594	275.1 examples/second
2022-01-11 09:53:06,676 - INFO - Test Loss=1.2060, Test top-1 acc=0.7094
2022-01-11 09:53:06,676 - INFO - Group Accuracy:

2022-01-11 09:53:06,677 - INFO - [0.97156626 0.9833735  0.97831327 0.9891566  0.97301203 0.99156624
 0.9727711  0.97614455 0.98578316 0.97301203 0.9845783  0.9787952
 0.96891564 0.9691566  0.9653012  0.9833735  0.9927711 ]
2022-01-11 09:53:06,678 - INFO - Epoch time: 398.6958587169647
2022-01-11 09:53:06,678 - INFO - 
Epoch: 76
2022-01-11 09:53:06,678 - INFO - 
Learning Rate: 0.0010
2022-01-11 09:53:27,470 - INFO - [Step=63500]	Loss=1.2765	253.3 examples/second
2022-01-11 09:55:23,700 - INFO - [Step=63750]	Loss=1.2542	275.3 examples/second
2022-01-11 09:57:19,758 - INFO - [Step=64000]	Loss=1.2495	275.7 examples/second
2022-01-11 09:59:16,162 - INFO - [Step=64250]	Loss=1.2613	274.9 examples/second
2022-01-11 09:59:44,512 - INFO - Test Loss=1.1984, Test top-1 acc=0.7099
2022-01-11 09:59:44,512 - INFO - Group Accuracy:

2022-01-11 09:59:44,512 - INFO - [0.9713253  0.98313254 0.9780723  0.9884337  0.9742169  0.9918072
 0.97228914 0.97614455 0.9853012  0.9742169  0.9848193  0.97903615
 0.9698795  0.9693976  0.9660241  0.98240966 0.993253  ]
2022-01-11 09:59:44,513 - INFO - Epoch time: 397.83571672439575
2022-01-11 09:59:44,514 - INFO - 
Epoch: 77
2022-01-11 09:59:44,514 - INFO - 
Learning Rate: 0.0010
2022-01-11 10:01:22,097 - INFO - [Step=64500]	Loss=1.2488	254.1 examples/second
2022-01-11 10:03:18,417 - INFO - [Step=64750]	Loss=1.2423	275.1 examples/second
2022-01-11 10:05:14,908 - INFO - [Step=65000]	Loss=1.2585	274.7 examples/second
2022-01-11 10:06:23,271 - INFO - Test Loss=1.1960, Test top-1 acc=0.7106
2022-01-11 10:06:23,272 - INFO - Group Accuracy:

2022-01-11 10:06:23,272 - INFO - [0.97180724 0.9833735  0.9780723  0.9893976  0.97445786 0.99156624
 0.9727711  0.97638553 0.9850602  0.9739759  0.98433733 0.98
 0.96819276 0.9693976  0.9657831  0.98313254 0.993253  ]
2022-01-11 10:06:23,274 - INFO - Epoch time: 398.75998640060425
2022-01-11 10:06:23,274 - INFO - 
Epoch: 78
2022-01-11 10:06:23,274 - INFO - 
Learning Rate: 0.0010
2022-01-11 10:07:21,272 - INFO - [Step=65250]	Loss=1.2624	253.2 examples/second
2022-01-11 10:09:17,370 - INFO - [Step=65500]	Loss=1.2630	275.6 examples/second
2022-01-11 10:11:13,998 - INFO - [Step=65750]	Loss=1.2474	274.4 examples/second
2022-01-11 10:13:01,959 - INFO - Test Loss=1.1980, Test top-1 acc=0.7123
2022-01-11 10:13:01,959 - INFO - Group Accuracy:

2022-01-11 10:13:01,959 - INFO - [0.97228914 0.98313254 0.9778313  0.9886747  0.9737349  0.99156624
 0.97180724 0.97590363 0.9848193  0.97445786 0.9845783  0.97903615
 0.96771085 0.9696385  0.9660241  0.98361444 0.9925301 ]
2022-01-11 10:13:01,960 - INFO - Epoch time: 398.68649101257324
2022-01-11 10:13:01,960 - INFO - 
Epoch: 79
2022-01-11 10:13:01,960 - INFO - 
Learning Rate: 0.0010
2022-01-11 10:13:20,374 - INFO - [Step=66000]	Loss=1.2692	253.2 examples/second
2022-01-11 10:15:16,471 - INFO - [Step=66250]	Loss=1.2642	275.6 examples/second
2022-01-11 10:17:12,836 - INFO - [Step=66500]	Loss=1.2519	275.0 examples/second
2022-01-11 10:19:09,693 - INFO - [Step=66750]	Loss=1.2485	273.8 examples/second
2022-01-11 10:19:40,803 - INFO - Test Loss=1.1966, Test top-1 acc=0.7116
2022-01-11 10:19:40,803 - INFO - Group Accuracy:

2022-01-11 10:19:40,803 - INFO - [0.9713253  0.98361444 0.9780723  0.9893976  0.9739759  0.99084336
 0.973494   0.9746988  0.9850602  0.9739759  0.9845783  0.97975904
 0.966747   0.9686747  0.9662651  0.9833735  0.9925301 ]
2022-01-11 10:19:40,804 - INFO - Epoch time: 398.8442370891571
2022-01-11 10:19:40,805 - INFO - 
Epoch: 80
2022-01-11 10:19:40,805 - INFO - 
Learning Rate: 0.0010
2022-01-11 10:21:15,693 - INFO - [Step=67000]	Loss=1.2489	254.0 examples/second
2022-01-11 10:23:11,899 - INFO - [Step=67250]	Loss=1.2497	275.4 examples/second
2022-01-11 10:25:08,333 - INFO - [Step=67500]	Loss=1.2477	274.8 examples/second
2022-01-11 10:26:19,047 - INFO - Test Loss=1.2046, Test top-1 acc=0.7120
2022-01-11 10:26:19,047 - INFO - Group Accuracy:

2022-01-11 10:26:19,047 - INFO - [0.97156626 0.9833735  0.97903615 0.9891566  0.973253   0.99084336
 0.97228914 0.97566265 0.9860241  0.9737349  0.9848193  0.97975904
 0.96698797 0.9696385  0.9657831  0.9826506  0.9927711 ]
2022-01-11 10:26:19,048 - INFO - Epoch time: 398.2434597015381
2022-01-11 10:26:19,048 - INFO - 
Epoch: 81
2022-01-11 10:26:19,048 - INFO - 
Learning Rate: 0.0010
2022-01-11 10:27:14,979 - INFO - [Step=67750]	Loss=1.2550	252.7 examples/second
2022-01-11 10:29:11,031 - INFO - [Step=68000]	Loss=1.2492	275.7 examples/second
2022-01-11 10:31:07,122 - INFO - [Step=68250]	Loss=1.2526	275.6 examples/second
2022-01-11 10:32:57,364 - INFO - Test Loss=1.1995, Test top-1 acc=0.7106
2022-01-11 10:32:57,365 - INFO - Group Accuracy:

2022-01-11 10:32:57,365 - INFO - [0.97108436 0.98361444 0.9785542  0.9893976  0.97445786 0.9918072
 0.97228914 0.97614455 0.9855422  0.9742169  0.9840964  0.97927713
 0.96891564 0.9693976  0.966747   0.9833735  0.9927711 ]
2022-01-11 10:32:57,366 - INFO - Epoch time: 398.31793761253357
2022-01-11 10:32:57,366 - INFO - 
Epoch: 82
2022-01-11 10:32:57,366 - INFO - 
Learning Rate: 0.0010
2022-01-11 10:33:13,637 - INFO - [Step=68500]	Loss=1.2555	252.9 examples/second
2022-01-11 10:35:09,771 - INFO - [Step=68750]	Loss=1.2473	275.5 examples/second
2022-01-11 10:37:05,901 - INFO - [Step=69000]	Loss=1.2385	275.6 examples/second
2022-01-11 10:39:02,493 - INFO - [Step=69250]	Loss=1.2569	274.5 examples/second
2022-01-11 10:39:35,785 - INFO - Test Loss=1.2021, Test top-1 acc=0.7123
2022-01-11 10:39:35,786 - INFO - Group Accuracy:

2022-01-11 10:39:35,786 - INFO - [0.97204816 0.98289156 0.9787952  0.98963857 0.973253   0.99156624
 0.97156626 0.9771084  0.9860241  0.97518075 0.9840964  0.97831327
 0.96819276 0.96891564 0.9662651  0.9826506  0.9925301 ]
2022-01-11 10:39:35,786 - INFO - Epoch time: 398.420291185379
2022-01-11 10:39:35,786 - INFO - 
Epoch: 83
2022-01-11 10:39:35,787 - INFO - 
Learning Rate: 0.0010
2022-01-11 10:41:08,918 - INFO - [Step=69500]	Loss=1.2444	253.1 examples/second
2022-01-11 10:43:05,028 - INFO - [Step=69750]	Loss=1.2385	275.6 examples/second
2022-01-11 10:45:01,440 - INFO - [Step=70000]	Loss=1.2632	274.9 examples/second
2022-01-11 10:46:14,302 - INFO - Test Loss=1.2041, Test top-1 acc=0.7096
2022-01-11 10:46:14,302 - INFO - Group Accuracy:

2022-01-11 10:46:14,302 - INFO - [0.97228914 0.98313254 0.9778313  0.9889157  0.97301203 0.99060243
 0.97301203 0.97493976 0.9855422  0.9739759  0.98433733 0.97975904
 0.96843374 0.9696385  0.9662651  0.9833735  0.9927711 ]
2022-01-11 10:46:14,303 - INFO - Epoch time: 398.51665902137756
2022-01-11 10:46:14,303 - INFO - 
Epoch: 84
2022-01-11 10:46:14,303 - INFO - 
Learning Rate: 0.0010
2022-01-11 10:47:07,547 - INFO - [Step=70250]	Loss=1.2589	253.8 examples/second
2022-01-11 10:49:03,863 - INFO - [Step=70500]	Loss=1.2502	275.1 examples/second
2022-01-11 10:51:00,050 - INFO - [Step=70750]	Loss=1.2312	275.4 examples/second
2022-01-11 10:52:52,625 - INFO - Test Loss=1.1951, Test top-1 acc=0.7120
2022-01-11 10:52:52,625 - INFO - Group Accuracy:

2022-01-11 10:52:52,625 - INFO - [0.9725301  0.9840964  0.9778313  0.9893976  0.9742169  0.99108434
 0.97156626 0.97590363 0.98578316 0.9746988  0.9848193  0.98024094
 0.96843374 0.9703615  0.9655422  0.98240966 0.993253  ]
2022-01-11 10:52:52,626 - INFO - Epoch time: 398.32290387153625
2022-01-11 10:52:52,626 - INFO - 
Epoch: 85
2022-01-11 10:52:52,626 - INFO - 
Learning Rate: 0.0010
2022-01-11 10:53:06,345 - INFO - [Step=71000]	Loss=1.2457	253.4 examples/second
2022-01-11 10:55:02,587 - INFO - [Step=71250]	Loss=1.2459	275.3 examples/second
2022-01-11 10:56:59,111 - INFO - [Step=71500]	Loss=1.2305	274.6 examples/second
2022-01-11 10:58:55,706 - INFO - [Step=71750]	Loss=1.2515	274.5 examples/second
2022-01-11 10:59:31,671 - INFO - Test Loss=1.1967, Test top-1 acc=0.7108
2022-01-11 10:59:31,672 - INFO - Group Accuracy:

2022-01-11 10:59:31,672 - INFO - [0.9727711  0.9840964  0.97831327 0.9901205  0.9737349  0.99084336
 0.9725301  0.97566265 0.9855422  0.9737349  0.98433733 0.97831327
 0.9686747  0.9693976  0.9655422  0.98361444 0.9930121 ]
2022-01-11 10:59:31,673 - INFO - Epoch time: 399.0464708805084
2022-01-11 10:59:31,673 - INFO - 
Epoch: 86
2022-01-11 10:59:31,673 - INFO - 
Learning Rate: 0.0010
2022-01-11 11:01:02,365 - INFO - [Step=72000]	Loss=1.2457	252.6 examples/second
2022-01-11 11:02:58,513 - INFO - [Step=72250]	Loss=1.2419	275.5 examples/second
2022-01-11 11:04:54,814 - INFO - [Step=72500]	Loss=1.2527	275.1 examples/second
2022-01-11 11:06:10,027 - INFO - Test Loss=1.2005, Test top-1 acc=0.7133
2022-01-11 11:06:10,028 - INFO - Group Accuracy:

2022-01-11 11:06:10,028 - INFO - [0.97156626 0.98433733 0.9785542  0.98963857 0.9737349  0.99156624
 0.97204816 0.97590363 0.98578316 0.97493976 0.9853012  0.97903615
 0.96843374 0.9701205  0.966506   0.98240966 0.993253  ]
2022-01-11 11:06:10,028 - INFO - Saving...
2022-01-11 11:06:10,294 - INFO - Epoch time: 398.62126541137695
2022-01-11 11:06:10,294 - INFO - 
Epoch: 87
2022-01-11 11:06:10,294 - INFO - 
Learning Rate: 0.0010
2022-01-11 11:07:01,212 - INFO - [Step=72750]	Loss=1.2485	253.2 examples/second
2022-01-11 11:08:57,279 - INFO - [Step=73000]	Loss=1.2682	275.7 examples/second
2022-01-11 11:10:53,476 - INFO - [Step=73250]	Loss=1.2441	275.4 examples/second
2022-01-11 11:12:48,995 - INFO - Test Loss=1.1957, Test top-1 acc=0.7113
2022-01-11 11:12:48,995 - INFO - Group Accuracy:

2022-01-11 11:12:48,995 - INFO - [0.9701205  0.9838554  0.9780723  0.9884337  0.973494   0.99108434
 0.9725301  0.97590363 0.9853012  0.97445786 0.98361444 0.9778313
 0.9693976  0.9701205  0.9657831  0.9826506  0.993253  ]
2022-01-11 11:12:48,996 - INFO - Epoch time: 398.701375246048
2022-01-11 11:12:48,996 - INFO - 
Epoch: 88
2022-01-11 11:12:48,996 - INFO - 
Learning Rate: 0.0010
2022-01-11 11:13:00,263 - INFO - [Step=73500]	Loss=1.2416	252.4 examples/second
2022-01-11 11:14:56,209 - INFO - [Step=73750]	Loss=1.2356	276.0 examples/second
2022-01-11 11:16:52,327 - INFO - [Step=74000]	Loss=1.2469	275.6 examples/second
2022-01-11 11:18:48,847 - INFO - [Step=74250]	Loss=1.2367	274.6 examples/second
2022-01-11 11:19:26,946 - INFO - Test Loss=1.1950, Test top-1 acc=0.7149
2022-01-11 11:19:26,946 - INFO - Group Accuracy:

2022-01-11 11:19:26,947 - INFO - [0.9727711  0.9833735  0.98       0.9891566  0.9739759  0.9913253
 0.97228914 0.97638553 0.98578316 0.9746988  0.9853012  0.97975904
 0.9686747  0.9698795  0.966506   0.9826506  0.993253  ]
2022-01-11 11:19:26,947 - INFO - Saving...
2022-01-11 11:19:27,171 - INFO - Epoch time: 398.1752734184265
2022-01-11 11:19:27,171 - INFO - 
Epoch: 89
2022-01-11 11:19:27,171 - INFO - 
Learning Rate: 0.0010
2022-01-11 11:20:55,274 - INFO - [Step=74500]	Loss=1.2324	253.1 examples/second
2022-01-11 11:22:51,457 - INFO - [Step=74750]	Loss=1.2349	275.4 examples/second
2022-01-11 11:24:47,737 - INFO - [Step=75000]	Loss=1.2456	275.2 examples/second
2022-01-11 11:26:05,389 - INFO - Test Loss=1.2014, Test top-1 acc=0.7154
2022-01-11 11:26:05,389 - INFO - Group Accuracy:

2022-01-11 11:26:05,389 - INFO - [0.97228914 0.98361444 0.9780723  0.98963857 0.973253   0.9918072
 0.97301203 0.97638553 0.98578316 0.97445786 0.9853012  0.97951806
 0.9686747  0.9701205  0.9662651  0.9821687  0.993253  ]
2022-01-11 11:26:05,390 - INFO - Saving...
2022-01-11 11:26:05,617 - INFO - Epoch time: 398.4459226131439
2022-01-11 11:26:16,498 - INFO - Computing OOD Statistics...
2022-01-11 11:26:16,513 - INFO - 	Baseline.          AUROC: 0.6089. TNR@95TPR: 0.1000. AUPR OUT: 0.2257
2022-01-11 11:26:16,522 - INFO - 	ODIN (T=1000).     AUROC: 0.8788. TNR@95TPR: 0.4212. AUPR OUT: 0.5652
2022-01-11 11:26:16,522 - INFO - Printing Final Accuracy + OOD Detection stats
2022-01-11 11:26:16,522 - INFO - Top 1 Accuracy:  Min: 0.7154; Max: 0.7154; Avg: 0.7154; Std: 0.0000; Len: 1
2022-01-11 11:26:16,522 - INFO - Top 5 Accuracy:  Min: 0.9790; Max: 0.9790; Avg: 0.9790; Std: 0.0000; Len: 1
2022-01-11 11:26:16,522 - INFO - **********************************************************************
2022-01-11 11:26:16,522 - INFO - 	MSP (auroc): [0.6089377746279235] Min: 0.6089; Max: 0.6089; Avg: 0.6089; Std: 0.0000; Len: 1
2022-01-11 11:26:16,523 - INFO - 	MSP (tnr): [0.09999999999999998] Min: 0.1000; Max: 0.1000; Avg: 0.1000; Std: 0.0000; Len: 1
2022-01-11 11:26:16,523 - INFO - 	MSP (aupr): [0.2256533780343626] Min: 0.2257; Max: 0.2257; Avg: 0.2257; Std: 0.0000; Len: 1
2022-01-11 11:26:16,523 - INFO - 	ODIN (auroc): [0.8788051027639971] Min: 0.8788; Max: 0.8788; Avg: 0.8788; Std: 0.0000; Len: 1
2022-01-11 11:26:16,523 - INFO - 	ODIN (tnr): [0.42117647058823526] Min: 0.4212; Max: 0.4212; Avg: 0.4212; Std: 0.0000; Len: 1
2022-01-11 11:26:16,523 - INFO - 	ODIN (aupr): [0.5651799596856899] Min: 0.5652; Max: 0.5652; Avg: 0.5652; Std: 0.0000; Len: 1
