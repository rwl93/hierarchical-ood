2022-01-11 01:27:34,566 - INFO - ==> Preparing data..
2022-01-11 01:27:34,970 - INFO - checkpoint filename: experiments/coarse/mos/adam_LRp1_R2/checkpoint.pt
2022-01-11 01:27:34,970 - INFO - log filename: experiments/coarse/mos/adam_LRp1_R2/train.log
2022-01-11 01:27:34,970 - INFO - ********************************************************
2022-01-11 01:27:34,970 - INFO - Starting Iter: 0 / 1
2022-01-11 01:27:34,970 - INFO - ********************************************************
2022-01-11 01:27:38,445 - INFO - 
Epoch: 0
2022-01-11 01:27:38,446 - INFO - 
Learning Rate: 0.0100
2022-01-11 01:29:37,659 - INFO - [Step=250]	Loss=6.3325	268.4 examples/second
2022-01-11 01:31:34,889 - INFO - [Step=500]	Loss=5.2535	273.0 examples/second
2022-01-11 01:33:31,930 - INFO - [Step=750]	Loss=4.9744	273.4 examples/second
2022-01-11 01:34:19,385 - INFO - Test Loss=4.7649, Test top-1 acc=0.0627
2022-01-11 01:34:19,385 - INFO - Group Accuracy:

2022-01-11 01:34:19,385 - INFO - [0.939759  0.939759  0.939759  0.9404819 0.939759  0.9392771 0.939759
 0.939759  0.939759  0.939759  0.9518072 0.939759  0.9387952 0.939759
 0.939759  0.939759  0.9518072]
2022-01-11 01:34:19,387 - INFO - Saving...
2022-01-11 01:34:19,630 - INFO - Epoch time: 401.18508672714233
2022-01-11 01:34:19,631 - INFO - 
Epoch: 1
2022-01-11 01:34:19,631 - INFO - 
Learning Rate: 0.0280
2022-01-11 01:35:38,971 - INFO - [Step=1000]	Loss=4.8632	251.9 examples/second
2022-01-11 01:37:35,738 - INFO - [Step=1250]	Loss=4.6326	274.1 examples/second
2022-01-11 01:39:32,986 - INFO - [Step=1500]	Loss=4.3943	272.9 examples/second
2022-01-11 01:41:00,275 - INFO - Test Loss=4.2716, Test top-1 acc=0.1800
2022-01-11 01:41:00,276 - INFO - Group Accuracy:

2022-01-11 01:41:00,276 - INFO - [0.9407229  0.9433735  0.93783134 0.946506   0.94       0.9281928
 0.9426506  0.93783134 0.93373495 0.939759   0.9515663  0.939759
 0.939759   0.939759   0.939759   0.94096386 0.9518072 ]
2022-01-11 01:41:00,277 - INFO - Saving...
2022-01-11 01:41:00,575 - INFO - Epoch time: 400.9439034461975
2022-01-11 01:41:00,575 - INFO - 
Epoch: 2
2022-01-11 01:41:00,575 - INFO - 
Learning Rate: 0.0460
2022-01-11 01:41:40,013 - INFO - [Step=1750]	Loss=4.2699	251.9 examples/second
2022-01-11 01:43:37,043 - INFO - [Step=2000]	Loss=4.2265	273.4 examples/second
2022-01-11 01:45:34,316 - INFO - [Step=2250]	Loss=4.0140	272.9 examples/second
2022-01-11 01:47:31,501 - INFO - [Step=2500]	Loss=3.8904	273.1 examples/second
2022-01-11 01:47:41,796 - INFO - Test Loss=3.9079, Test top-1 acc=0.2311
2022-01-11 01:47:41,796 - INFO - Group Accuracy:

2022-01-11 01:47:41,796 - INFO - [0.9440964  0.9436145  0.94120485 0.9515663  0.9404819  0.95325303
 0.94578314 0.9356626  0.95228916 0.9385542  0.95036143 0.9233735
 0.9387952  0.93783134 0.94144577 0.94144577 0.9498795 ]
2022-01-11 01:47:41,797 - INFO - Saving...
2022-01-11 01:47:42,076 - INFO - Epoch time: 401.50061655044556
2022-01-11 01:47:42,076 - INFO - 
Epoch: 3
2022-01-11 01:47:42,076 - INFO - 
Learning Rate: 0.0640
2022-01-11 01:49:38,576 - INFO - [Step=2750]	Loss=3.8594	251.8 examples/second
2022-01-11 01:51:35,435 - INFO - [Step=3000]	Loss=3.7592	273.8 examples/second
2022-01-11 01:53:32,307 - INFO - [Step=3250]	Loss=3.6634	273.8 examples/second
2022-01-11 01:54:22,089 - INFO - Test Loss=3.4425, Test top-1 acc=0.2819
2022-01-11 01:54:22,090 - INFO - Group Accuracy:

2022-01-11 01:54:22,090 - INFO - [0.9426506  0.9486747  0.9440964  0.9554217  0.9440964  0.96361446
 0.9436145  0.9445783  0.9506024  0.94240963 0.95036143 0.94144577
 0.9404819  0.94       0.9431325  0.94578314 0.9626506 ]
2022-01-11 01:54:22,090 - INFO - Saving...
2022-01-11 01:54:22,329 - INFO - Epoch time: 400.2530527114868
2022-01-11 01:54:22,329 - INFO - 
Epoch: 4
2022-01-11 01:54:22,329 - INFO - 
Learning Rate: 0.1000
2022-01-11 01:55:39,333 - INFO - [Step=3500]	Loss=3.7305	251.9 examples/second
2022-01-11 01:57:36,285 - INFO - [Step=3750]	Loss=3.7252	273.6 examples/second
2022-01-11 01:59:32,953 - INFO - [Step=4000]	Loss=3.6948	274.3 examples/second
2022-01-11 02:01:03,060 - INFO - Test Loss=3.5786, Test top-1 acc=0.2692
2022-01-11 02:01:03,061 - INFO - Group Accuracy:

2022-01-11 02:01:03,061 - INFO - [0.94216865 0.94       0.9440964  0.95421684 0.939759   0.9549398
 0.9431325  0.94891566 0.9515663  0.94120485 0.9469879  0.9407229
 0.9395181  0.94216865 0.94578314 0.9448193  0.9631325 ]
2022-01-11 02:01:03,062 - INFO - Epoch time: 400.7326683998108
2022-01-11 02:01:03,062 - INFO - 
Epoch: 5
2022-01-11 02:01:03,062 - INFO - 
Learning Rate: 0.1000
2022-01-11 02:01:40,953 - INFO - [Step=4250]	Loss=3.5977	250.0 examples/second
2022-01-11 02:03:38,242 - INFO - [Step=4500]	Loss=3.5261	272.8 examples/second
2022-01-11 02:05:35,263 - INFO - [Step=4750]	Loss=3.4740	273.5 examples/second
2022-01-11 02:07:32,357 - INFO - [Step=5000]	Loss=3.4592	273.3 examples/second
2022-01-11 02:07:44,855 - INFO - Test Loss=3.7871, Test top-1 acc=0.2212
2022-01-11 02:07:44,855 - INFO - Group Accuracy:

2022-01-11 02:07:44,856 - INFO - [0.9385542  0.9460241  0.939759   0.95349395 0.94168675 0.9578313
 0.94       0.94626504 0.9472289  0.939759   0.9520482  0.94144577
 0.940241   0.9392771  0.94192773 0.946506   0.96457833]
2022-01-11 02:07:44,856 - INFO - Epoch time: 401.7945008277893
2022-01-11 02:07:44,856 - INFO - 
Epoch: 6
2022-01-11 02:07:44,856 - INFO - 
Learning Rate: 0.1000
2022-01-11 02:09:39,012 - INFO - [Step=5250]	Loss=3.3846	252.7 examples/second
2022-01-11 02:11:35,540 - INFO - [Step=5500]	Loss=3.3663	274.6 examples/second
2022-01-11 02:13:32,345 - INFO - [Step=5750]	Loss=3.2913	274.0 examples/second
2022-01-11 02:14:24,144 - INFO - Test Loss=3.1917, Test top-1 acc=0.3267
2022-01-11 02:14:24,144 - INFO - Group Accuracy:

2022-01-11 02:14:24,144 - INFO - [0.9448193  0.9506024  0.9426506  0.9544578  0.9431325  0.9727711
 0.9404819  0.9479518  0.96072286 0.9440964  0.9520482  0.9481928
 0.9395181  0.9404819  0.94433737 0.94144577 0.97180724]
2022-01-11 02:14:24,146 - INFO - Saving...
2022-01-11 02:14:24,393 - INFO - Epoch time: 399.5365962982178
2022-01-11 02:14:24,393 - INFO - 
Epoch: 7
2022-01-11 02:14:24,393 - INFO - 
Learning Rate: 0.1000
2022-01-11 02:15:38,984 - INFO - [Step=6000]	Loss=3.2759	252.7 examples/second
2022-01-11 02:17:35,739 - INFO - [Step=6250]	Loss=3.2481	274.1 examples/second
2022-01-11 02:19:32,562 - INFO - [Step=6500]	Loss=3.2185	273.9 examples/second
2022-01-11 02:21:04,214 - INFO - Test Loss=3.2376, Test top-1 acc=0.3243
2022-01-11 02:21:04,215 - INFO - Group Accuracy:

2022-01-11 02:21:04,215 - INFO - [0.9460241  0.95325303 0.9440964  0.9544578  0.9445783  0.95638555
 0.9477109  0.9510843  0.95975906 0.9445783  0.95759034 0.9474699
 0.9395181  0.939759   0.94433737 0.9513253  0.9766265 ]
2022-01-11 02:21:04,216 - INFO - Epoch time: 399.8223955631256
2022-01-11 02:21:04,216 - INFO - 
Epoch: 8
2022-01-11 02:21:04,216 - INFO - 
Learning Rate: 0.1000
2022-01-11 02:21:39,193 - INFO - [Step=6750]	Loss=3.1294	252.7 examples/second
2022-01-11 02:23:36,090 - INFO - [Step=7000]	Loss=3.1383	273.7 examples/second
2022-01-11 02:25:33,076 - INFO - [Step=7250]	Loss=3.0905	273.5 examples/second
2022-01-11 02:27:29,538 - INFO - [Step=7500]	Loss=3.0722	274.8 examples/second
2022-01-11 02:27:44,585 - INFO - Test Loss=3.2795, Test top-1 acc=0.3398
2022-01-11 02:27:44,586 - INFO - Group Accuracy:

2022-01-11 02:27:44,586 - INFO - [0.9508434  0.95325303 0.94289154 0.9612048  0.9438554  0.97204816
 0.9448193  0.9515663  0.9293976  0.9433735  0.9561446  0.9460241
 0.940241   0.94144577 0.94192773 0.9501205  0.9771084 ]
2022-01-11 02:27:44,586 - INFO - Saving...
2022-01-11 02:27:44,826 - INFO - Epoch time: 400.6102602481842
2022-01-11 02:27:44,826 - INFO - 
Epoch: 9
2022-01-11 02:27:44,826 - INFO - 
Learning Rate: 0.1000
2022-01-11 02:29:37,493 - INFO - [Step=7750]	Loss=3.0046	250.1 examples/second
2022-01-11 02:31:34,276 - INFO - [Step=8000]	Loss=3.0079	274.0 examples/second
2022-01-11 02:33:30,955 - INFO - [Step=8250]	Loss=3.1017	274.3 examples/second
2022-01-11 02:34:25,730 - INFO - Test Loss=2.9608, Test top-1 acc=0.3875
2022-01-11 02:34:25,730 - INFO - Group Accuracy:

2022-01-11 02:34:25,730 - INFO - [0.9513253  0.95301205 0.946506   0.96385545 0.94963855 0.9693976
 0.93084335 0.94963855 0.96698797 0.94578314 0.96096385 0.946747
 0.94240963 0.94216865 0.9460241  0.95325303 0.9768675 ]
2022-01-11 02:34:25,731 - INFO - Saving...
2022-01-11 02:34:25,972 - INFO - Epoch time: 401.1459090709686
2022-01-11 02:34:25,972 - INFO - 
Epoch: 10
2022-01-11 02:34:25,973 - INFO - 
Learning Rate: 0.1000
2022-01-11 02:35:38,382 - INFO - [Step=8500]	Loss=2.9837	251.1 examples/second
2022-01-11 02:37:34,709 - INFO - [Step=8750]	Loss=2.9668	275.1 examples/second
2022-01-11 02:39:31,190 - INFO - [Step=9000]	Loss=2.9566	274.7 examples/second
2022-01-11 02:41:05,562 - INFO - Test Loss=2.8793, Test top-1 acc=0.3940
2022-01-11 02:41:05,563 - INFO - Group Accuracy:

2022-01-11 02:41:05,563 - INFO - [0.9513253  0.9559036  0.94939756 0.9612048  0.9433735  0.973253
 0.95036143 0.95325303 0.9674699  0.95277107 0.9559036  0.95277107
 0.94144577 0.9431325  0.94578314 0.9518072  0.9633735 ]
2022-01-11 02:41:05,564 - INFO - Saving...
2022-01-11 02:41:05,808 - INFO - Epoch time: 399.835391998291
2022-01-11 02:41:05,808 - INFO - 
Epoch: 11
2022-01-11 02:41:05,808 - INFO - 
Learning Rate: 0.1000
2022-01-11 02:41:38,283 - INFO - [Step=9250]	Loss=2.8967	251.8 examples/second
2022-01-11 02:43:34,664 - INFO - [Step=9500]	Loss=2.8748	275.0 examples/second
2022-01-11 02:45:30,963 - INFO - [Step=9750]	Loss=2.8347	275.2 examples/second
2022-01-11 02:47:27,580 - INFO - [Step=10000]	Loss=2.8575	274.4 examples/second
2022-01-11 02:47:45,378 - INFO - Test Loss=3.0414, Test top-1 acc=0.3778
2022-01-11 02:47:45,378 - INFO - Group Accuracy:

2022-01-11 02:47:45,378 - INFO - [0.9477109  0.9546988  0.9373494  0.9587952  0.94578314 0.97156626
 0.94843376 0.95301205 0.9662651  0.9392771  0.9559036  0.95421684
 0.94216865 0.9433735  0.9501205  0.95421684 0.9773494 ]
2022-01-11 02:47:45,379 - INFO - Epoch time: 399.57088232040405
2022-01-11 02:47:45,379 - INFO - 
Epoch: 12
2022-01-11 02:47:45,379 - INFO - 
Learning Rate: 0.1000
2022-01-11 02:49:34,478 - INFO - [Step=10250]	Loss=2.8086	252.2 examples/second
2022-01-11 02:51:30,964 - INFO - [Step=10500]	Loss=2.7469	274.7 examples/second
2022-01-11 02:53:27,538 - INFO - [Step=10750]	Loss=2.7454	274.5 examples/second
2022-01-11 02:54:24,383 - INFO - Test Loss=2.9939, Test top-1 acc=0.3839
2022-01-11 02:54:24,383 - INFO - Group Accuracy:

2022-01-11 02:54:24,383 - INFO - [0.94240963 0.94915664 0.9460241  0.966747   0.9506024  0.97493976
 0.9479518  0.9549398  0.96795183 0.9510843  0.95759034 0.93759036
 0.94144577 0.94506025 0.94843376 0.96024096 0.9701205 ]
2022-01-11 02:54:24,384 - INFO - Epoch time: 399.0053777694702
2022-01-11 02:54:24,385 - INFO - 
Epoch: 13
2022-01-11 02:54:24,385 - INFO - 
Learning Rate: 0.1000
2022-01-11 02:55:34,375 - INFO - [Step=11000]	Loss=2.7272	252.3 examples/second
2022-01-11 02:57:30,601 - INFO - [Step=11250]	Loss=2.6933	275.3 examples/second
2022-01-11 02:59:26,720 - INFO - [Step=11500]	Loss=2.7160	275.6 examples/second
2022-01-11 03:01:02,959 - INFO - Test Loss=2.6693, Test top-1 acc=0.4063
2022-01-11 03:01:02,959 - INFO - Group Accuracy:

2022-01-11 03:01:02,959 - INFO - [0.9510843  0.9621687  0.95036143 0.9624096  0.95253015 0.966506
 0.94843376 0.9592771  0.9631325  0.95373493 0.96481925 0.95566267
 0.94168675 0.9433735  0.94843376 0.95421684 0.9807229 ]
2022-01-11 03:01:02,960 - INFO - Saving...
2022-01-11 03:01:03,188 - INFO - Epoch time: 398.8030939102173
2022-01-11 03:01:03,188 - INFO - 
Epoch: 14
2022-01-11 03:01:03,188 - INFO - 
Learning Rate: 0.1000
2022-01-11 03:01:33,255 - INFO - [Step=11750]	Loss=2.6921	252.9 examples/second
2022-01-11 03:03:29,844 - INFO - [Step=12000]	Loss=2.6597	274.5 examples/second
2022-01-11 03:05:25,807 - INFO - [Step=12250]	Loss=2.6274	275.9 examples/second
2022-01-11 03:07:21,911 - INFO - [Step=12500]	Loss=2.6102	275.6 examples/second
2022-01-11 03:07:41,269 - INFO - Test Loss=2.4422, Test top-1 acc=0.4508
2022-01-11 03:07:41,269 - INFO - Group Accuracy:

2022-01-11 03:07:41,269 - INFO - [0.95373493 0.9551807  0.9510843  0.97180724 0.95277107 0.9780723
 0.95228916 0.96096385 0.966747   0.9549398  0.966747   0.95638555
 0.94289154 0.94216865 0.9515663  0.9587952  0.98289156]
2022-01-11 03:07:41,270 - INFO - Saving...
2022-01-11 03:07:41,533 - INFO - Epoch time: 398.3454215526581
2022-01-11 03:07:41,534 - INFO - 
Epoch: 15
2022-01-11 03:07:41,534 - INFO - 
Learning Rate: 0.1000
2022-01-11 03:09:29,110 - INFO - [Step=12750]	Loss=2.6521	251.6 examples/second
2022-01-11 03:11:25,126 - INFO - [Step=13000]	Loss=2.5609	275.8 examples/second
2022-01-11 03:13:21,188 - INFO - [Step=13250]	Loss=2.5801	275.7 examples/second
2022-01-11 03:14:19,961 - INFO - Test Loss=2.4424, Test top-1 acc=0.4672
2022-01-11 03:14:19,961 - INFO - Group Accuracy:

2022-01-11 03:14:19,961 - INFO - [0.95349395 0.96096385 0.9510843  0.97542167 0.9554217  0.9785542
 0.9515663  0.9621687  0.97590363 0.95301205 0.9674699  0.9578313
 0.94433737 0.94843376 0.9498795  0.9631325  0.98024094]
2022-01-11 03:14:19,962 - INFO - Saving...
2022-01-11 03:14:20,234 - INFO - Epoch time: 398.70027780532837
2022-01-11 03:14:20,234 - INFO - 
Epoch: 16
2022-01-11 03:14:20,234 - INFO - 
Learning Rate: 0.1000
2022-01-11 03:15:27,305 - INFO - [Step=13500]	Loss=2.6083	253.7 examples/second
2022-01-11 03:17:23,386 - INFO - [Step=13750]	Loss=2.5256	275.7 examples/second
2022-01-11 03:19:19,405 - INFO - [Step=14000]	Loss=2.4939	275.8 examples/second
2022-01-11 03:20:57,936 - INFO - Test Loss=2.4278, Test top-1 acc=0.4827
2022-01-11 03:20:57,936 - INFO - Group Accuracy:

2022-01-11 03:20:57,936 - INFO - [0.9546988  0.9590362  0.95421684 0.9727711  0.95710844 0.97927713
 0.95253015 0.9585542  0.97156626 0.9592771  0.96409637 0.95710844
 0.9291566  0.94915664 0.9501205  0.96433735 0.9814458 ]
2022-01-11 03:20:57,937 - INFO - Saving...
2022-01-11 03:20:58,179 - INFO - Epoch time: 397.9447057247162
2022-01-11 03:20:58,179 - INFO - 
Epoch: 17
2022-01-11 03:20:58,179 - INFO - 
Learning Rate: 0.1000
2022-01-11 03:21:25,732 - INFO - [Step=14250]	Loss=2.4913	253.3 examples/second
2022-01-11 03:23:21,894 - INFO - [Step=14500]	Loss=2.4391	275.5 examples/second
2022-01-11 03:25:18,085 - INFO - [Step=14750]	Loss=2.4483	275.4 examples/second
2022-01-11 03:27:14,038 - INFO - [Step=15000]	Loss=2.4262	276.0 examples/second
2022-01-11 03:27:35,802 - INFO - Test Loss=2.3688, Test top-1 acc=0.4827
2022-01-11 03:27:35,802 - INFO - Group Accuracy:

2022-01-11 03:27:35,802 - INFO - [0.95228916 0.96457833 0.9510843  0.97493976 0.9554217  0.9787952
 0.95228916 0.95710844 0.96843374 0.9580723  0.96771085 0.9595181
 0.9426506  0.94       0.9510843  0.96457833 0.9804819 ]
2022-01-11 03:27:35,803 - INFO - Epoch time: 397.6236979961395
2022-01-11 03:27:35,803 - INFO - 
Epoch: 18
2022-01-11 03:27:35,803 - INFO - 
Learning Rate: 0.1000
2022-01-11 03:29:20,280 - INFO - [Step=15250]	Loss=2.3595	253.5 examples/second
2022-01-11 03:31:16,329 - INFO - [Step=15500]	Loss=2.3741	275.7 examples/second
2022-01-11 03:33:12,545 - INFO - [Step=15750]	Loss=2.3809	275.3 examples/second
2022-01-11 03:34:13,396 - INFO - Test Loss=2.2654, Test top-1 acc=0.4757
2022-01-11 03:34:13,396 - INFO - Group Accuracy:

2022-01-11 03:34:13,397 - INFO - [0.9590362  0.9633735  0.9508434  0.973494   0.9546988  0.98
 0.95349395 0.9626506  0.9746988  0.9549398  0.9693976  0.96072286
 0.9469879  0.95036143 0.9501205  0.96361446 0.98024094]
2022-01-11 03:34:13,397 - INFO - Epoch time: 397.59440875053406
2022-01-11 03:34:13,397 - INFO - 
Epoch: 19
2022-01-11 03:34:13,397 - INFO - 
Learning Rate: 0.1000
2022-01-11 03:35:18,253 - INFO - [Step=16000]	Loss=2.3794	254.6 examples/second
2022-01-11 03:37:14,532 - INFO - [Step=16250]	Loss=2.3573	275.2 examples/second
2022-01-11 03:39:10,473 - INFO - [Step=16500]	Loss=2.3463	276.0 examples/second
2022-01-11 03:40:51,277 - INFO - Test Loss=2.1025, Test top-1 acc=0.5176
2022-01-11 03:40:51,278 - INFO - Group Accuracy:

2022-01-11 03:40:51,278 - INFO - [0.9614458  0.96819276 0.96096385 0.97542167 0.9595181  0.9812048
 0.9551807  0.9624096  0.97566265 0.96048194 0.9698795  0.96409637
 0.9426506  0.9486747  0.9513253  0.96385545 0.98240966]
2022-01-11 03:40:51,278 - INFO - Saving...
2022-01-11 03:40:51,521 - INFO - Epoch time: 398.1241023540497
2022-01-11 03:40:51,522 - INFO - 
Epoch: 20
2022-01-11 03:40:51,522 - INFO - 
Learning Rate: 0.1000
2022-01-11 03:41:17,008 - INFO - [Step=16750]	Loss=2.3394	252.9 examples/second
2022-01-11 03:43:13,432 - INFO - [Step=17000]	Loss=2.3056	274.9 examples/second
2022-01-11 03:45:09,278 - INFO - [Step=17250]	Loss=2.2800	276.2 examples/second
2022-01-11 03:47:05,676 - INFO - [Step=17500]	Loss=2.2834	274.9 examples/second
2022-01-11 03:47:29,617 - INFO - Test Loss=2.0527, Test top-1 acc=0.5441
2022-01-11 03:47:29,618 - INFO - Group Accuracy:

2022-01-11 03:47:29,618 - INFO - [0.96096385 0.96433735 0.9561446  0.9780723  0.9612048  0.9860241
 0.9549398  0.96698797 0.9778313  0.96433735 0.9742169  0.9653012
 0.9546988  0.95228916 0.95036143 0.966747   0.9855422 ]
2022-01-11 03:47:29,618 - INFO - Saving...
2022-01-11 03:47:29,859 - INFO - Epoch time: 398.33688831329346
2022-01-11 03:47:29,859 - INFO - 
Epoch: 21
2022-01-11 03:47:29,859 - INFO - 
Learning Rate: 0.1000
2022-01-11 03:49:11,701 - INFO - [Step=17750]	Loss=2.2385	253.9 examples/second
2022-01-11 03:51:07,865 - INFO - [Step=18000]	Loss=2.2478	275.5 examples/second
2022-01-11 03:53:03,877 - INFO - [Step=18250]	Loss=2.2395	275.8 examples/second
2022-01-11 03:54:07,333 - INFO - Test Loss=2.0656, Test top-1 acc=0.5393
2022-01-11 03:54:07,334 - INFO - Group Accuracy:

2022-01-11 03:54:07,334 - INFO - [0.9587952  0.9660241  0.9595181  0.9807229  0.96024096 0.98626506
 0.9559036  0.9621687  0.97903615 0.96072286 0.96891564 0.9612048
 0.94939756 0.9513253  0.95662653 0.96698797 0.9850602 ]
2022-01-11 03:54:07,334 - INFO - Epoch time: 397.4754846096039
2022-01-11 03:54:07,334 - INFO - 
Epoch: 22
2022-01-11 03:54:07,334 - INFO - 
Learning Rate: 0.1000
2022-01-11 03:55:10,069 - INFO - [Step=18500]	Loss=2.2277	253.6 examples/second
2022-01-11 03:57:06,266 - INFO - [Step=18750]	Loss=2.2230	275.4 examples/second
2022-01-11 03:59:02,294 - INFO - [Step=19000]	Loss=2.2475	275.8 examples/second
2022-01-11 04:00:45,014 - INFO - Test Loss=2.1593, Test top-1 acc=0.5219
2022-01-11 04:00:45,014 - INFO - Group Accuracy:

2022-01-11 04:00:45,014 - INFO - [0.96048194 0.96168673 0.9626506  0.9727711  0.9592771  0.9853012
 0.94144577 0.9631325  0.97445786 0.96096385 0.9660241  0.9662651
 0.9539759  0.9539759  0.95686746 0.966747   0.9853012 ]
2022-01-11 04:00:45,015 - INFO - Epoch time: 397.6805245876312
2022-01-11 04:00:45,015 - INFO - 
Epoch: 23
2022-01-11 04:00:45,015 - INFO - 
Learning Rate: 0.1000
2022-01-11 04:01:08,165 - INFO - [Step=19250]	Loss=2.1701	254.2 examples/second
2022-01-11 04:03:04,563 - INFO - [Step=19500]	Loss=2.1661	274.9 examples/second
2022-01-11 04:05:00,639 - INFO - [Step=19750]	Loss=2.1806	275.7 examples/second
2022-01-11 04:06:56,576 - INFO - [Step=20000]	Loss=2.1622	276.0 examples/second
2022-01-11 04:07:22,990 - INFO - Test Loss=2.0646, Test top-1 acc=0.5443
2022-01-11 04:07:22,991 - INFO - Group Accuracy:

2022-01-11 04:07:22,991 - INFO - [0.96096385 0.9696385  0.9433735  0.97831327 0.9619277  0.9853012
 0.9587952  0.9655422  0.9701205  0.9657831  0.9657831  0.9619277
 0.95325303 0.9477109  0.95566267 0.9703615  0.98674697]
2022-01-11 04:07:22,992 - INFO - Saving...
2022-01-11 04:07:23,227 - INFO - Epoch time: 398.21165585517883
2022-01-11 04:07:23,227 - INFO - 
Epoch: 24
2022-01-11 04:07:23,227 - INFO - 
Learning Rate: 0.1000
2022-01-11 04:09:03,115 - INFO - [Step=20250]	Loss=2.1169	252.9 examples/second
2022-01-11 04:10:59,494 - INFO - [Step=20500]	Loss=2.1194	275.0 examples/second
2022-01-11 04:12:55,424 - INFO - [Step=20750]	Loss=2.1252	276.0 examples/second
2022-01-11 04:14:01,457 - INFO - Test Loss=1.8920, Test top-1 acc=0.5733
2022-01-11 04:14:01,457 - INFO - Group Accuracy:

2022-01-11 04:14:01,457 - INFO - [0.96481925 0.966506   0.9619277  0.9742169  0.9624096  0.98626506
 0.95349395 0.9708434  0.9778313  0.9657831  0.9737349  0.9655422
 0.9549398  0.9590362  0.9573494  0.9653012  0.98771083]
2022-01-11 04:14:01,458 - INFO - Saving...
2022-01-11 04:14:01,694 - INFO - Epoch time: 398.46669936180115
2022-01-11 04:14:01,694 - INFO - 
Epoch: 25
2022-01-11 04:14:01,694 - INFO - 
Learning Rate: 0.1000
2022-01-11 04:15:01,967 - INFO - [Step=21000]	Loss=2.0887	252.9 examples/second
2022-01-11 04:16:58,114 - INFO - [Step=21250]	Loss=2.0977	275.5 examples/second
2022-01-11 04:18:54,334 - INFO - [Step=21500]	Loss=2.1013	275.3 examples/second
2022-01-11 04:20:39,944 - INFO - Test Loss=1.9729, Test top-1 acc=0.5646
2022-01-11 04:20:39,945 - INFO - Group Accuracy:

2022-01-11 04:20:39,945 - INFO - [0.9631325  0.97180724 0.9628916  0.97831327 0.9626506  0.98650604
 0.95686746 0.9674699  0.9696385  0.96481925 0.9737349  0.9612048
 0.95421684 0.9554217  0.95253015 0.97156626 0.9893976 ]
2022-01-11 04:20:39,946 - INFO - Epoch time: 398.25189661979675
2022-01-11 04:20:39,946 - INFO - 
Epoch: 26
2022-01-11 04:20:39,946 - INFO - 
Learning Rate: 0.1000
2022-01-11 04:21:00,756 - INFO - [Step=21750]	Loss=2.0880	253.1 examples/second
2022-01-11 04:22:56,939 - INFO - [Step=22000]	Loss=2.0554	275.4 examples/second
2022-01-11 04:24:53,193 - INFO - [Step=22250]	Loss=2.0790	275.3 examples/second
2022-01-11 04:26:49,165 - INFO - [Step=22500]	Loss=2.0602	275.9 examples/second
2022-01-11 04:27:18,080 - INFO - Test Loss=2.0418, Test top-1 acc=0.5513
2022-01-11 04:27:18,080 - INFO - Group Accuracy:

2022-01-11 04:27:18,091 - INFO - [0.96168673 0.96819276 0.9626506  0.9814458  0.96       0.9819277
 0.9561446  0.9691566  0.97638553 0.94891566 0.9725301  0.9612048
 0.9508434  0.9549398  0.9585542  0.96891564 0.9853012 ]
2022-01-11 04:27:18,091 - INFO - Epoch time: 398.1455180644989
2022-01-11 04:27:18,091 - INFO - 
Epoch: 27
2022-01-11 04:27:18,091 - INFO - 
Learning Rate: 0.1000
2022-01-11 04:28:55,492 - INFO - [Step=22750]	Loss=2.0170	253.3 examples/second
2022-01-11 04:30:51,572 - INFO - [Step=23000]	Loss=2.0232	275.7 examples/second
2022-01-11 04:32:47,936 - INFO - [Step=23250]	Loss=2.0433	275.0 examples/second
2022-01-11 04:33:56,049 - INFO - Test Loss=1.9367, Test top-1 acc=0.5660
2022-01-11 04:33:56,050 - INFO - Group Accuracy:

2022-01-11 04:33:56,050 - INFO - [0.96       0.97204816 0.9631325  0.9807229  0.96506023 0.98361444
 0.9580723  0.96433735 0.97638553 0.9628916  0.973494   0.96361446
 0.9518072  0.9590362  0.95277107 0.9698795  0.9879518 ]
2022-01-11 04:33:56,051 - INFO - Epoch time: 397.9591510295868
2022-01-11 04:33:56,051 - INFO - 
Epoch: 28
2022-01-11 04:33:56,051 - INFO - 
Learning Rate: 0.1000
2022-01-11 04:34:54,689 - INFO - [Step=23500]	Loss=2.0165	252.5 examples/second
2022-01-11 04:36:50,482 - INFO - [Step=23750]	Loss=1.9912	276.4 examples/second
2022-01-11 04:38:46,791 - INFO - [Step=24000]	Loss=2.0067	275.1 examples/second
2022-01-11 04:40:34,285 - INFO - Test Loss=1.8174, Test top-1 acc=0.5908
2022-01-11 04:40:34,286 - INFO - Group Accuracy:

2022-01-11 04:40:34,286 - INFO - [0.9614458  0.96433735 0.9621687  0.9814458  0.96481925 0.9850602
 0.9614458  0.96819276 0.97903615 0.9674699  0.97493976 0.9662651
 0.95686746 0.9585542  0.9561446  0.9742169  0.98746985]
2022-01-11 04:40:34,287 - INFO - Saving...
2022-01-11 04:40:34,553 - INFO - Epoch time: 398.5027377605438
2022-01-11 04:40:34,554 - INFO - 
Epoch: 29
2022-01-11 04:40:34,554 - INFO - 
Learning Rate: 0.0100
2022-01-11 04:40:52,990 - INFO - [Step=24250]	Loss=1.9789	253.6 examples/second
2022-01-11 04:42:49,388 - INFO - [Step=24500]	Loss=1.7097	274.9 examples/second
2022-01-11 04:44:45,815 - INFO - [Step=24750]	Loss=1.6619	274.9 examples/second
2022-01-11 04:46:42,202 - INFO - [Step=25000]	Loss=1.6401	274.9 examples/second
2022-01-11 04:47:13,463 - INFO - Test Loss=1.3918, Test top-1 acc=0.6704
2022-01-11 04:47:13,464 - INFO - Group Accuracy:

2022-01-11 04:47:13,464 - INFO - [0.96771085 0.9766265  0.973253   0.98674697 0.97301203 0.99036145
 0.96819276 0.97614455 0.98433733 0.9703615  0.9821687  0.97590363
 0.9660241  0.96481925 0.96433735 0.97638553 0.99084336]
2022-01-11 04:47:13,464 - INFO - Saving...
2022-01-11 04:47:13,708 - INFO - Epoch time: 399.1545202732086
2022-01-11 04:47:13,708 - INFO - 
Epoch: 30
2022-01-11 04:47:13,708 - INFO - 
Learning Rate: 0.0100
2022-01-11 04:48:48,792 - INFO - [Step=25250]	Loss=1.5811	252.8 examples/second
2022-01-11 04:50:44,935 - INFO - [Step=25500]	Loss=1.5867	275.5 examples/second
2022-01-11 04:52:40,786 - INFO - [Step=25750]	Loss=1.5665	276.2 examples/second
2022-01-11 04:53:51,177 - INFO - Test Loss=1.3752, Test top-1 acc=0.6781
2022-01-11 04:53:51,177 - INFO - Group Accuracy:

2022-01-11 04:53:51,177 - INFO - [0.9691566  0.9775904  0.9737349  0.98650604 0.973494   0.99060243
 0.9686747  0.97614455 0.9840964  0.9713253  0.9819277  0.97566265
 0.96771085 0.96409637 0.9657831  0.9780723  0.99156624]
2022-01-11 04:53:51,178 - INFO - Saving...
2022-01-11 04:53:51,423 - INFO - Epoch time: 397.71476697921753
2022-01-11 04:53:51,423 - INFO - 
Epoch: 31
2022-01-11 04:53:51,423 - INFO - 
Learning Rate: 0.0100
2022-01-11 04:54:46,790 - INFO - [Step=26000]	Loss=1.5850	254.0 examples/second
2022-01-11 04:56:42,817 - INFO - [Step=26250]	Loss=1.5498	275.8 examples/second
2022-01-11 04:58:38,837 - INFO - [Step=26500]	Loss=1.5266	275.8 examples/second
2022-01-11 05:00:28,981 - INFO - Test Loss=1.3387, Test top-1 acc=0.6776
2022-01-11 05:00:28,981 - INFO - Group Accuracy:

2022-01-11 05:00:28,981 - INFO - [0.9698795  0.9780723  0.97518075 0.98650604 0.973253   0.99156624
 0.9701205  0.97445786 0.98433733 0.9713253  0.9804819  0.9775904
 0.9662651  0.9660241  0.96457833 0.9778313  0.9913253 ]
2022-01-11 05:00:28,982 - INFO - Epoch time: 397.55865693092346
2022-01-11 05:00:28,982 - INFO - 
Epoch: 32
2022-01-11 05:00:28,982 - INFO - 
Learning Rate: 0.0100
2022-01-11 05:00:44,883 - INFO - [Step=26750]	Loss=1.5385	253.9 examples/second
2022-01-11 05:02:41,089 - INFO - [Step=27000]	Loss=1.5295	275.4 examples/second
2022-01-11 05:04:37,247 - INFO - [Step=27250]	Loss=1.5142	275.5 examples/second
2022-01-11 05:06:33,298 - INFO - [Step=27500]	Loss=1.5160	275.7 examples/second
2022-01-11 05:07:06,488 - INFO - Test Loss=1.3129, Test top-1 acc=0.6805
2022-01-11 05:07:06,488 - INFO - Group Accuracy:

2022-01-11 05:07:06,488 - INFO - [0.96891564 0.9785542  0.97542167 0.9881928  0.9742169  0.99084336
 0.9696385  0.9773494  0.98313254 0.97180724 0.9814458  0.97831327
 0.9660241  0.96361446 0.9653012  0.9766265  0.9913253 ]
2022-01-11 05:07:06,490 - INFO - Saving...
2022-01-11 05:07:06,731 - INFO - Epoch time: 397.7485535144806
2022-01-11 05:07:06,731 - INFO - 
Epoch: 33
2022-01-11 05:07:06,731 - INFO - 
Learning Rate: 0.0100
2022-01-11 05:08:39,590 - INFO - [Step=27750]	Loss=1.4979	253.4 examples/second
2022-01-11 05:10:35,600 - INFO - [Step=28000]	Loss=1.5029	275.8 examples/second
2022-01-11 05:12:31,869 - INFO - [Step=28250]	Loss=1.4897	275.2 examples/second
2022-01-11 05:13:44,652 - INFO - Test Loss=1.2888, Test top-1 acc=0.6892
2022-01-11 05:13:44,652 - INFO - Group Accuracy:

2022-01-11 05:13:44,653 - INFO - [0.9686747  0.9785542  0.9766265  0.98746985 0.9739759  0.99108434
 0.9698795  0.97590363 0.98313254 0.9708434  0.9821687  0.9775904
 0.9672289  0.96795183 0.9657831  0.97903615 0.9920482 ]
2022-01-11 05:13:44,653 - INFO - Saving...
2022-01-11 05:13:44,913 - INFO - Epoch time: 398.18219685554504
2022-01-11 05:13:44,913 - INFO - 
Epoch: 34
2022-01-11 05:13:44,913 - INFO - 
Learning Rate: 0.0100
2022-01-11 05:14:38,148 - INFO - [Step=28500]	Loss=1.4939	253.4 examples/second
2022-01-11 05:16:34,344 - INFO - [Step=28750]	Loss=1.4968	275.4 examples/second
2022-01-11 05:18:30,644 - INFO - [Step=29000]	Loss=1.4852	275.2 examples/second
2022-01-11 05:20:23,283 - INFO - Test Loss=1.2947, Test top-1 acc=0.6882
2022-01-11 05:20:23,283 - INFO - Group Accuracy:

2022-01-11 05:20:23,283 - INFO - [0.97108436 0.9775904  0.97566265 0.9881928  0.97518075 0.99108434
 0.97156626 0.9771084  0.9838554  0.97156626 0.9812048  0.9785542
 0.9660241  0.9674699  0.9653012  0.9780723  0.9920482 ]
2022-01-11 05:20:23,284 - INFO - Epoch time: 398.3708875179291
2022-01-11 05:20:23,284 - INFO - 
Epoch: 35
2022-01-11 05:20:23,284 - INFO - 
Learning Rate: 0.0100
2022-01-11 05:20:37,460 - INFO - [Step=29250]	Loss=1.4844	252.3 examples/second
2022-01-11 05:22:33,753 - INFO - [Step=29500]	Loss=1.4803	275.2 examples/second
2022-01-11 05:24:29,894 - INFO - [Step=29750]	Loss=1.4869	275.5 examples/second
2022-01-11 05:26:25,922 - INFO - [Step=30000]	Loss=1.4712	275.8 examples/second
2022-01-11 05:27:01,889 - INFO - Test Loss=1.2767, Test top-1 acc=0.6940
2022-01-11 05:27:01,889 - INFO - Group Accuracy:

2022-01-11 05:27:01,889 - INFO - [0.9706024  0.9785542  0.97638553 0.9881928  0.97493976 0.9913253
 0.9727711  0.9766265  0.98361444 0.9727711  0.9821687  0.9773494
 0.96843374 0.9703615  0.966506   0.97903615 0.9930121 ]
2022-01-11 05:27:01,890 - INFO - Saving...
2022-01-11 05:27:02,132 - INFO - Epoch time: 398.84799003601074
2022-01-11 05:27:02,133 - INFO - 
Epoch: 36
2022-01-11 05:27:02,133 - INFO - 
Learning Rate: 0.0100
2022-01-11 05:28:32,867 - INFO - [Step=30250]	Loss=1.4442	252.1 examples/second
2022-01-11 05:30:28,702 - INFO - [Step=30500]	Loss=1.4639	276.3 examples/second
2022-01-11 05:32:24,778 - INFO - [Step=30750]	Loss=1.4567	275.7 examples/second
2022-01-11 05:33:40,128 - INFO - Test Loss=1.2688, Test top-1 acc=0.6935
2022-01-11 05:33:40,129 - INFO - Group Accuracy:

2022-01-11 05:33:40,129 - INFO - [0.9703615  0.9778313  0.97638553 0.98722893 0.97518075 0.99084336
 0.97204816 0.97614455 0.9845783  0.97204816 0.9833735  0.9785542
 0.9686747  0.96771085 0.96433735 0.9787952  0.9913253 ]
2022-01-11 05:33:40,130 - INFO - Epoch time: 397.9971549510956
2022-01-11 05:33:40,130 - INFO - 
Epoch: 37
2022-01-11 05:33:40,130 - INFO - 
Learning Rate: 0.0100
2022-01-11 05:34:31,319 - INFO - [Step=31000]	Loss=1.4437	252.9 examples/second
2022-01-11 05:36:27,135 - INFO - [Step=31250]	Loss=1.4451	276.3 examples/second
2022-01-11 05:38:23,175 - INFO - [Step=31500]	Loss=1.4470	275.8 examples/second
2022-01-11 05:40:17,938 - INFO - Test Loss=1.2762, Test top-1 acc=0.6945
2022-01-11 05:40:17,939 - INFO - Group Accuracy:

2022-01-11 05:40:17,939 - INFO - [0.9725301  0.97951806 0.97493976 0.98771083 0.9737349  0.99060243
 0.97108436 0.97518075 0.9850602  0.97204816 0.9845783  0.9780723
 0.9691566  0.9693976  0.96506023 0.9780723  0.9918072 ]
2022-01-11 05:40:17,940 - INFO - Saving...
2022-01-11 05:40:18,185 - INFO - Epoch time: 398.05471634864807
2022-01-11 05:40:18,185 - INFO - 
Epoch: 38
2022-01-11 05:40:18,185 - INFO - 
Learning Rate: 0.0100
2022-01-11 05:40:29,689 - INFO - [Step=31750]	Loss=1.4228	252.9 examples/second
2022-01-11 05:42:25,448 - INFO - [Step=32000]	Loss=1.4227	276.4 examples/second
2022-01-11 05:44:21,236 - INFO - [Step=32250]	Loss=1.4256	276.4 examples/second
2022-01-11 05:46:17,224 - INFO - [Step=32500]	Loss=1.4244	275.9 examples/second
2022-01-11 05:46:54,893 - INFO - Test Loss=1.2582, Test top-1 acc=0.6990
2022-01-11 05:46:54,893 - INFO - Group Accuracy:

2022-01-11 05:46:54,893 - INFO - [0.9727711  0.97975904 0.9768675  0.9884337  0.9739759  0.99108434
 0.97204816 0.97590363 0.9840964  0.9739759  0.9833735  0.97831327
 0.9672289  0.9696385  0.96481925 0.97951806 0.9930121 ]
2022-01-11 05:46:54,894 - INFO - Saving...
2022-01-11 05:46:55,125 - INFO - Epoch time: 396.9402663707733
2022-01-11 05:46:55,125 - INFO - 
Epoch: 39
2022-01-11 05:46:55,125 - INFO - 
Learning Rate: 0.0100
2022-01-11 05:48:23,338 - INFO - [Step=32750]	Loss=1.4232	253.7 examples/second
2022-01-11 05:50:19,267 - INFO - [Step=33000]	Loss=1.4082	276.0 examples/second
2022-01-11 05:52:15,041 - INFO - [Step=33250]	Loss=1.4105	276.4 examples/second
2022-01-11 05:53:31,951 - INFO - Test Loss=1.2360, Test top-1 acc=0.7007
2022-01-11 05:53:31,952 - INFO - Group Accuracy:

2022-01-11 05:53:31,952 - INFO - [0.9708434  0.97975904 0.9771084  0.98771083 0.9739759  0.99108434
 0.9696385  0.9771084  0.9845783  0.973494   0.9833735  0.9787952
 0.9693976  0.96843374 0.9662651  0.97831327 0.9922892 ]
2022-01-11 05:53:31,954 - INFO - Saving...
2022-01-11 05:53:32,190 - INFO - Epoch time: 397.06435322761536
2022-01-11 05:53:32,190 - INFO - 
Epoch: 40
2022-01-11 05:53:32,190 - INFO - 
Learning Rate: 0.0100
2022-01-11 05:54:20,375 - INFO - [Step=33500]	Loss=1.4290	255.3 examples/second
2022-01-11 05:56:16,826 - INFO - [Step=33750]	Loss=1.3860	274.8 examples/second
2022-01-11 05:58:12,677 - INFO - [Step=34000]	Loss=1.4127	276.2 examples/second
2022-01-11 06:00:09,234 - INFO - Test Loss=1.2529, Test top-1 acc=0.6940
2022-01-11 06:00:09,234 - INFO - Group Accuracy:

2022-01-11 06:00:09,234 - INFO - [0.9703615  0.97951806 0.97638553 0.9881928  0.97518075 0.99108434
 0.97180724 0.9771084  0.9840964  0.973494   0.98289156 0.9773494
 0.9686747  0.9674699  0.9655422  0.9780723  0.9925301 ]
2022-01-11 06:00:09,236 - INFO - Epoch time: 397.04578018188477
2022-01-11 06:00:09,236 - INFO - 
Epoch: 41
2022-01-11 06:00:09,236 - INFO - 
Learning Rate: 0.0100
2022-01-11 06:00:18,107 - INFO - [Step=34250]	Loss=1.4085	255.1 examples/second
2022-01-11 06:02:14,065 - INFO - [Step=34500]	Loss=1.3971	276.0 examples/second
2022-01-11 06:04:10,118 - INFO - [Step=34750]	Loss=1.3883	275.7 examples/second
2022-01-11 06:06:06,146 - INFO - [Step=35000]	Loss=1.3925	275.8 examples/second
2022-01-11 06:06:46,329 - INFO - Test Loss=1.2497, Test top-1 acc=0.7027
2022-01-11 06:06:46,330 - INFO - Group Accuracy:

2022-01-11 06:06:46,330 - INFO - [0.9708434  0.9807229  0.9771084  0.98722893 0.973253   0.99108434
 0.97180724 0.9785542  0.9850602  0.9725301  0.9833735  0.9787952
 0.966506   0.97108436 0.9653012  0.98       0.9930121 ]
2022-01-11 06:06:46,331 - INFO - Saving...
2022-01-11 06:06:46,563 - INFO - Epoch time: 397.3266866207123
2022-01-11 06:06:46,563 - INFO - 
Epoch: 42
2022-01-11 06:06:46,563 - INFO - 
Learning Rate: 0.0100
2022-01-11 06:08:12,219 - INFO - [Step=35250]	Loss=1.3863	253.8 examples/second
2022-01-11 06:10:07,928 - INFO - [Step=35500]	Loss=1.3857	276.6 examples/second
2022-01-11 06:12:04,148 - INFO - [Step=35750]	Loss=1.3800	275.3 examples/second
2022-01-11 06:13:23,911 - INFO - Test Loss=1.2325, Test top-1 acc=0.6990
2022-01-11 06:13:23,912 - INFO - Group Accuracy:

2022-01-11 06:13:23,912 - INFO - [0.97204816 0.97951806 0.9771084  0.98771083 0.97493976 0.99036145
 0.9713253  0.9775904  0.9848193  0.9737349  0.9821687  0.97903615
 0.9672289  0.9693976  0.96433735 0.97927713 0.993253  ]
2022-01-11 06:13:23,913 - INFO - Epoch time: 397.3503723144531
2022-01-11 06:13:23,913 - INFO - 
Epoch: 43
2022-01-11 06:13:23,913 - INFO - 
Learning Rate: 0.0100
2022-01-11 06:14:09,889 - INFO - [Step=36000]	Loss=1.3882	254.5 examples/second
2022-01-11 06:16:06,002 - INFO - [Step=36250]	Loss=1.3667	275.6 examples/second
2022-01-11 06:18:02,265 - INFO - [Step=36500]	Loss=1.3760	275.2 examples/second
2022-01-11 06:20:01,455 - INFO - Test Loss=1.2382, Test top-1 acc=0.7022
2022-01-11 06:20:01,455 - INFO - Group Accuracy:

2022-01-11 06:20:01,455 - INFO - [0.9706024  0.9807229  0.9768675  0.9884337  0.9746988  0.99108434
 0.9706024  0.9768675  0.9848193  0.97180724 0.9838554  0.9778313
 0.96843374 0.9701205  0.96457833 0.9785542  0.993494  ]
2022-01-11 06:20:01,456 - INFO - Epoch time: 397.5425066947937
2022-01-11 06:20:01,456 - INFO - 
Epoch: 44
2022-01-11 06:20:01,456 - INFO - 
Learning Rate: 0.0100
2022-01-11 06:20:08,052 - INFO - [Step=36750]	Loss=1.3723	254.4 examples/second
2022-01-11 06:22:03,958 - INFO - [Step=37000]	Loss=1.3449	276.1 examples/second
2022-01-11 06:23:59,910 - INFO - [Step=37250]	Loss=1.3810	276.0 examples/second
2022-01-11 06:25:55,948 - INFO - [Step=37500]	Loss=1.3711	275.8 examples/second
2022-01-11 06:26:38,246 - INFO - Test Loss=1.2206, Test top-1 acc=0.7060
2022-01-11 06:26:38,247 - INFO - Group Accuracy:

2022-01-11 06:26:38,247 - INFO - [0.97228914 0.97903615 0.9771084  0.9884337  0.97542167 0.9901205
 0.9739759  0.97638553 0.9853012  0.9727711  0.98240966 0.9809638
 0.9672289  0.9701205  0.96481925 0.9804819  0.993253  ]
2022-01-11 06:26:38,247 - INFO - Saving...
2022-01-11 06:26:38,503 - INFO - Epoch time: 397.04754066467285
2022-01-11 06:26:38,504 - INFO - 
Epoch: 45
2022-01-11 06:26:38,504 - INFO - 
Learning Rate: 0.0100
2022-01-11 06:28:01,851 - INFO - [Step=37750]	Loss=1.3585	254.2 examples/second
2022-01-11 06:29:57,886 - INFO - [Step=38000]	Loss=1.3620	275.8 examples/second
2022-01-11 06:31:53,677 - INFO - [Step=38250]	Loss=1.3627	276.4 examples/second
2022-01-11 06:33:15,593 - INFO - Test Loss=1.2446, Test top-1 acc=0.7027
2022-01-11 06:33:15,593 - INFO - Group Accuracy:

2022-01-11 06:33:15,593 - INFO - [0.97156626 0.9804819  0.9768675  0.9889157  0.9737349  0.9920482
 0.9713253  0.97614455 0.9845783  0.97228914 0.9819277  0.97903615
 0.9686747  0.9686747  0.9655422  0.97975904 0.9930121 ]
2022-01-11 06:33:15,594 - INFO - Epoch time: 397.0904338359833
2022-01-11 06:33:15,594 - INFO - 
Epoch: 46
2022-01-11 06:33:15,594 - INFO - 
Learning Rate: 0.0100
2022-01-11 06:33:59,426 - INFO - [Step=38500]	Loss=1.3633	254.5 examples/second
2022-01-11 06:35:55,550 - INFO - [Step=38750]	Loss=1.3502	275.6 examples/second
2022-01-11 06:37:51,305 - INFO - [Step=39000]	Loss=1.3668	276.4 examples/second
2022-01-11 06:39:52,648 - INFO - Test Loss=1.2195, Test top-1 acc=0.7041
2022-01-11 06:39:52,648 - INFO - Group Accuracy:

2022-01-11 06:39:52,648 - INFO - [0.9725301  0.9812048  0.97614455 0.9886747  0.97301203 0.99156624
 0.9713253  0.9785542  0.98433733 0.97180724 0.98361444 0.97831327
 0.96795183 0.9686747  0.9653012  0.9812048  0.993253  ]
2022-01-11 06:39:52,649 - INFO - Epoch time: 397.0550112724304
2022-01-11 06:39:52,649 - INFO - 
Epoch: 47
2022-01-11 06:39:52,649 - INFO - 
Learning Rate: 0.0100
2022-01-11 06:39:57,133 - INFO - [Step=39250]	Loss=1.3623	254.3 examples/second
2022-01-11 06:41:53,192 - INFO - [Step=39500]	Loss=1.3362	275.7 examples/second
2022-01-11 06:43:49,003 - INFO - [Step=39750]	Loss=1.3314	276.3 examples/second
2022-01-11 06:45:44,962 - INFO - [Step=40000]	Loss=1.3476	276.0 examples/second
2022-01-11 06:46:29,661 - INFO - Test Loss=1.2155, Test top-1 acc=0.7005
2022-01-11 06:46:29,661 - INFO - Group Accuracy:

2022-01-11 06:46:29,661 - INFO - [0.97204816 0.9807229  0.97831327 0.9891566  0.97301203 0.99036145
 0.9737349  0.9771084  0.9848193  0.9703615  0.9821687  0.9778313
 0.96795183 0.9706024  0.96481925 0.98024094 0.9927711 ]
2022-01-11 06:46:29,663 - INFO - Epoch time: 397.0135908126831
2022-01-11 06:46:29,663 - INFO - 
Epoch: 48
2022-01-11 06:46:29,663 - INFO - 
Learning Rate: 0.0100
2022-01-11 06:47:50,983 - INFO - [Step=40250]	Loss=1.3318	253.9 examples/second
2022-01-11 06:49:46,734 - INFO - [Step=40500]	Loss=1.3404	276.5 examples/second
2022-01-11 06:51:42,549 - INFO - [Step=40750]	Loss=1.3350	276.3 examples/second
2022-01-11 06:53:06,546 - INFO - Test Loss=1.2113, Test top-1 acc=0.7010
2022-01-11 06:53:06,547 - INFO - Group Accuracy:

2022-01-11 06:53:06,547 - INFO - [0.97108436 0.98289156 0.9773494  0.98771083 0.9742169  0.99060243
 0.9737349  0.9771084  0.9848193  0.9727711  0.9821687  0.9785542
 0.96843374 0.96843374 0.9653012  0.97975904 0.993494  ]
2022-01-11 06:53:06,548 - INFO - Epoch time: 396.88462710380554
2022-01-11 06:53:06,548 - INFO - 
Epoch: 49
2022-01-11 06:53:06,548 - INFO - 
Learning Rate: 0.0100
2022-01-11 06:53:48,410 - INFO - [Step=41000]	Loss=1.3388	254.2 examples/second
2022-01-11 06:55:44,849 - INFO - [Step=41250]	Loss=1.3214	274.8 examples/second
2022-01-11 06:57:40,986 - INFO - [Step=41500]	Loss=1.3199	275.5 examples/second
2022-01-11 06:59:37,003 - INFO - [Step=41750]	Loss=1.3422	275.8 examples/second
2022-01-11 06:59:44,848 - INFO - Test Loss=1.1928, Test top-1 acc=0.7108
2022-01-11 06:59:44,849 - INFO - Group Accuracy:

2022-01-11 06:59:44,849 - INFO - [0.9725301  0.9816868  0.9771084  0.9886747  0.9742169  0.9918072
 0.9739759  0.9780723  0.9840964  0.9725301  0.98433733 0.9780723
 0.96819276 0.9703615  0.9662651  0.98024094 0.9925301 ]
2022-01-11 06:59:44,850 - INFO - Saving...
2022-01-11 06:59:45,093 - INFO - Epoch time: 398.5455083847046
2022-01-11 06:59:45,093 - INFO - 
Epoch: 50
2022-01-11 06:59:45,093 - INFO - 
Learning Rate: 0.0100
2022-01-11 07:01:43,527 - INFO - [Step=42000]	Loss=1.3240	252.9 examples/second
2022-01-11 07:03:39,646 - INFO - [Step=42250]	Loss=1.3184	275.6 examples/second
2022-01-11 07:05:35,563 - INFO - [Step=42500]	Loss=1.3295	276.1 examples/second
2022-01-11 07:06:22,821 - INFO - Test Loss=1.2045, Test top-1 acc=0.7087
2022-01-11 07:06:22,822 - INFO - Group Accuracy:

2022-01-11 07:06:22,822 - INFO - [0.97301203 0.98289156 0.9771084  0.9886747  0.9742169  0.99108434
 0.973494   0.97614455 0.9848193  0.97301203 0.9826506  0.97903615
 0.9691566  0.9703615  0.96481925 0.9809638  0.993253  ]
2022-01-11 07:06:22,823 - INFO - Epoch time: 397.7291955947876
2022-01-11 07:06:22,823 - INFO - 
Epoch: 51
2022-01-11 07:06:22,823 - INFO - 
Learning Rate: 0.0100
2022-01-11 07:07:41,608 - INFO - [Step=42750]	Loss=1.3305	253.9 examples/second
2022-01-11 07:09:37,589 - INFO - [Step=43000]	Loss=1.3117	275.9 examples/second
2022-01-11 07:11:33,560 - INFO - [Step=43250]	Loss=1.3329	275.9 examples/second
2022-01-11 07:13:00,221 - INFO - Test Loss=1.2080, Test top-1 acc=0.7070
2022-01-11 07:13:00,221 - INFO - Group Accuracy:

2022-01-11 07:13:00,221 - INFO - [0.97228914 0.98313254 0.97566265 0.9891566  0.97566265 0.99156624
 0.973494   0.9766265  0.9840964  0.9725301  0.98289156 0.97831327
 0.96795183 0.9691566  0.9657831  0.9809638  0.9927711 ]
2022-01-11 07:13:00,222 - INFO - Epoch time: 397.3993308544159
2022-01-11 07:13:00,222 - INFO - 
Epoch: 52
2022-01-11 07:13:00,222 - INFO - 
Learning Rate: 0.0100
2022-01-11 07:13:39,309 - INFO - [Step=43500]	Loss=1.3080	254.5 examples/second
2022-01-11 07:15:35,257 - INFO - [Step=43750]	Loss=1.3006	276.0 examples/second
2022-01-11 07:17:30,979 - INFO - [Step=44000]	Loss=1.2970	276.5 examples/second
2022-01-11 07:19:26,980 - INFO - [Step=44250]	Loss=1.3080	275.9 examples/second
2022-01-11 07:19:37,222 - INFO - Test Loss=1.1885, Test top-1 acc=0.7113
2022-01-11 07:19:37,222 - INFO - Group Accuracy:

2022-01-11 07:19:37,257 - INFO - [0.97204816 0.9821687  0.9771084  0.9881928  0.97614455 0.9922892
 0.9742169  0.9771084  0.98578316 0.97301203 0.98240966 0.97951806
 0.9701205  0.9691566  0.9686747  0.97975904 0.993494  ]
2022-01-11 07:19:37,259 - INFO - Saving...
2022-01-11 07:19:37,490 - INFO - Epoch time: 397.26840233802795
2022-01-11 07:19:37,491 - INFO - 
Epoch: 53
2022-01-11 07:19:37,491 - INFO - 
Learning Rate: 0.0100
2022-01-11 07:21:33,139 - INFO - [Step=44500]	Loss=1.2902	253.6 examples/second
2022-01-11 07:23:29,022 - INFO - [Step=44750]	Loss=1.2954	276.1 examples/second
2022-01-11 07:25:25,189 - INFO - [Step=45000]	Loss=1.3092	275.5 examples/second
2022-01-11 07:26:14,581 - INFO - Test Loss=1.1995, Test top-1 acc=0.7125
2022-01-11 07:26:14,582 - INFO - Group Accuracy:

2022-01-11 07:26:14,582 - INFO - [0.97228914 0.98240966 0.9768675  0.9879518  0.9739759  0.99156624
 0.9725301  0.97590363 0.9845783  0.973253   0.9845783  0.97927713
 0.96891564 0.9708434  0.96506023 0.9807229  0.99373496]
2022-01-11 07:26:14,583 - INFO - Saving...
2022-01-11 07:26:14,814 - INFO - Epoch time: 397.32283425331116
2022-01-11 07:26:14,814 - INFO - 
Epoch: 54
2022-01-11 07:26:14,814 - INFO - 
Learning Rate: 0.0100
2022-01-11 07:27:31,250 - INFO - [Step=45250]	Loss=1.2887	253.8 examples/second
2022-01-11 07:29:27,129 - INFO - [Step=45500]	Loss=1.3030	276.2 examples/second
2022-01-11 07:31:22,961 - INFO - [Step=45750]	Loss=1.2903	276.3 examples/second
2022-01-11 07:32:51,955 - INFO - Test Loss=1.1783, Test top-1 acc=0.7142
2022-01-11 07:32:51,955 - INFO - Group Accuracy:

2022-01-11 07:32:51,956 - INFO - [0.973494   0.9814458  0.9780723  0.9884337  0.9742169  0.9920482
 0.97445786 0.9773494  0.9855422  0.97204816 0.98433733 0.97975904
 0.9708434  0.9698795  0.96457833 0.9807229  0.99373496]
2022-01-11 07:32:51,957 - INFO - Saving...
2022-01-11 07:32:52,230 - INFO - Epoch time: 397.4167170524597
2022-01-11 07:32:52,231 - INFO - 
Epoch: 55
2022-01-11 07:32:52,231 - INFO - 
Learning Rate: 0.0100
2022-01-11 07:33:29,074 - INFO - [Step=46000]	Loss=1.2746	253.7 examples/second
2022-01-11 07:35:25,276 - INFO - [Step=46250]	Loss=1.2924	275.4 examples/second
2022-01-11 07:37:21,186 - INFO - [Step=46500]	Loss=1.2928	276.1 examples/second
2022-01-11 07:39:16,998 - INFO - [Step=46750]	Loss=1.2879	276.3 examples/second
2022-01-11 07:39:29,323 - INFO - Test Loss=1.1672, Test top-1 acc=0.7171
2022-01-11 07:39:29,324 - INFO - Group Accuracy:

2022-01-11 07:39:29,324 - INFO - [0.973253   0.98313254 0.9768675  0.9889157  0.97566265 0.99156624
 0.97493976 0.9775904  0.9853012  0.9739759  0.9814458  0.97975904
 0.9698795  0.973253   0.96698797 0.9812048  0.9939759 ]
2022-01-11 07:39:29,325 - INFO - Saving...
2022-01-11 07:39:29,559 - INFO - Epoch time: 397.32874178886414
2022-01-11 07:39:29,560 - INFO - 
Epoch: 56
2022-01-11 07:39:29,560 - INFO - 
Learning Rate: 0.0100
2022-01-11 07:41:23,589 - INFO - [Step=47000]	Loss=1.2803	252.8 examples/second
2022-01-11 07:43:19,402 - INFO - [Step=47250]	Loss=1.3006	276.3 examples/second
2022-01-11 07:45:15,215 - INFO - [Step=47500]	Loss=1.2856	276.3 examples/second
2022-01-11 07:46:06,748 - INFO - Test Loss=1.1667, Test top-1 acc=0.7212
2022-01-11 07:46:06,748 - INFO - Group Accuracy:

2022-01-11 07:46:06,748 - INFO - [0.9746988  0.9826506  0.9778313  0.9884337  0.97590363 0.9913253
 0.97518075 0.97903615 0.9853012  0.97204816 0.9840964  0.9787952
 0.9698795  0.9713253  0.9686747  0.98240966 0.993494  ]
2022-01-11 07:46:06,753 - INFO - Saving...
2022-01-11 07:46:06,985 - INFO - Epoch time: 397.4257745742798
2022-01-11 07:46:06,986 - INFO - 
Epoch: 57
2022-01-11 07:46:06,986 - INFO - 
Learning Rate: 0.0100
2022-01-11 07:47:20,847 - INFO - [Step=47750]	Loss=1.2789	254.7 examples/second
2022-01-11 07:49:16,804 - INFO - [Step=48000]	Loss=1.2701	276.0 examples/second
2022-01-11 07:51:12,762 - INFO - [Step=48250]	Loss=1.2936	276.0 examples/second
2022-01-11 07:52:43,537 - INFO - Test Loss=1.1649, Test top-1 acc=0.7200
2022-01-11 07:52:43,537 - INFO - Group Accuracy:

2022-01-11 07:52:43,537 - INFO - [0.9746988  0.9814458  0.9778313  0.9889157  0.9771084  0.99156624
 0.97493976 0.97614455 0.9855422  0.9708434  0.98361444 0.97951806
 0.9693976  0.9725301  0.96385545 0.98361444 0.99421686]
2022-01-11 07:52:43,538 - INFO - Epoch time: 396.55242109298706
2022-01-11 07:52:43,538 - INFO - 
Epoch: 58
2022-01-11 07:52:43,538 - INFO - 
Learning Rate: 0.0100
2022-01-11 07:53:18,177 - INFO - [Step=48500]	Loss=1.2801	255.2 examples/second
2022-01-11 07:55:14,636 - INFO - [Step=48750]	Loss=1.2654	274.8 examples/second
2022-01-11 07:57:10,839 - INFO - [Step=49000]	Loss=1.2621	275.4 examples/second
2022-01-11 07:59:07,048 - INFO - [Step=49250]	Loss=1.2620	275.4 examples/second
2022-01-11 07:59:21,477 - INFO - Test Loss=1.1827, Test top-1 acc=0.7169
2022-01-11 07:59:21,478 - INFO - Group Accuracy:

2022-01-11 07:59:21,478 - INFO - [0.973253   0.98024094 0.9785542  0.9891566  0.97590363 0.99108434
 0.9742169  0.9778313  0.9855422  0.9713253  0.9821687  0.97903615
 0.9703615  0.9727711  0.966506   0.9816868  0.9927711 ]
2022-01-11 07:59:21,479 - INFO - Epoch time: 397.9404809474945
2022-01-11 07:59:21,479 - INFO - 
Epoch: 59
2022-01-11 07:59:21,479 - INFO - 
Learning Rate: 0.0010
2022-01-11 08:01:12,366 - INFO - [Step=49500]	Loss=1.2629	255.4 examples/second
2022-01-11 08:03:08,585 - INFO - [Step=49750]	Loss=1.2259	275.3 examples/second
2022-01-11 08:05:04,335 - INFO - [Step=50000]	Loss=1.2159	276.5 examples/second
2022-01-11 08:05:58,301 - INFO - Test Loss=1.1403, Test top-1 acc=0.7205
2022-01-11 08:05:58,301 - INFO - Group Accuracy:

2022-01-11 08:05:58,301 - INFO - [0.9746988  0.98361444 0.9785542  0.9891566  0.9766265  0.9927711
 0.97566265 0.97831327 0.98626506 0.97204816 0.9838554  0.98024094
 0.97108436 0.9725301  0.9662651  0.9821687  0.9930121 ]
2022-01-11 08:05:58,302 - INFO - Epoch time: 396.8232173919678
2022-01-11 08:05:58,302 - INFO - 
Epoch: 60
2022-01-11 08:05:58,302 - INFO - 
Learning Rate: 0.0010
2022-01-11 08:07:09,722 - INFO - [Step=50250]	Loss=1.2204	255.2 examples/second
2022-01-11 08:09:05,640 - INFO - [Step=50500]	Loss=1.2372	276.1 examples/second
2022-01-11 08:11:01,632 - INFO - [Step=50750]	Loss=1.2221	275.9 examples/second
2022-01-11 08:12:34,797 - INFO - Test Loss=1.1320, Test top-1 acc=0.7229
2022-01-11 08:12:34,797 - INFO - Group Accuracy:

2022-01-11 08:12:34,805 - INFO - [0.97445786 0.98313254 0.97975904 0.9891566  0.9766265  0.9922892
 0.97614455 0.9778313  0.98626506 0.9725301  0.9845783  0.97927713
 0.97108436 0.9727711  0.96698797 0.9821687  0.9925301 ]
2022-01-11 08:12:34,805 - INFO - Saving...
2022-01-11 08:12:34,978 - INFO - Epoch time: 396.67573523521423
2022-01-11 08:12:34,978 - INFO - 
Epoch: 61
2022-01-11 08:12:34,978 - INFO - 
Learning Rate: 0.0010
2022-01-11 08:13:07,368 - INFO - [Step=51000]	Loss=1.2234	254.5 examples/second
2022-01-11 08:15:03,280 - INFO - [Step=51250]	Loss=1.2137	276.1 examples/second
2022-01-11 08:16:59,180 - INFO - [Step=51500]	Loss=1.2207	276.1 examples/second
2022-01-11 08:18:54,982 - INFO - [Step=51750]	Loss=1.2113	276.3 examples/second
2022-01-11 08:19:11,887 - INFO - Test Loss=1.1314, Test top-1 acc=0.7214
2022-01-11 08:19:11,888 - INFO - Group Accuracy:

2022-01-11 08:19:11,888 - INFO - [0.97493976 0.98289156 0.97831327 0.9881928  0.9771084  0.9922892
 0.97566265 0.9778313  0.98626506 0.9725301  0.9838554  0.97951806
 0.97228914 0.9727711  0.9653012  0.9819277  0.9930121 ]
2022-01-11 08:19:11,888 - INFO - Epoch time: 396.91035628318787
2022-01-11 08:19:11,888 - INFO - 
Epoch: 62
2022-01-11 08:19:11,888 - INFO - 
Learning Rate: 0.0010
2022-01-11 08:21:00,351 - INFO - [Step=52000]	Loss=1.2180	255.2 examples/second
2022-01-11 08:22:56,195 - INFO - [Step=52250]	Loss=1.2106	276.2 examples/second
2022-01-11 08:24:51,968 - INFO - [Step=52500]	Loss=1.2249	276.4 examples/second
2022-01-11 08:25:48,233 - INFO - Test Loss=1.1322, Test top-1 acc=0.7272
2022-01-11 08:25:48,233 - INFO - Group Accuracy:

2022-01-11 08:25:48,233 - INFO - [0.97493976 0.9838554  0.97927713 0.9889157  0.9775904  0.9925301
 0.97566265 0.9773494  0.9860241  0.9727711  0.98433733 0.97951806
 0.9696385  0.97204816 0.96795183 0.9814458  0.993494  ]
2022-01-11 08:25:48,234 - INFO - Saving...
2022-01-11 08:25:48,468 - INFO - Epoch time: 396.5796175003052
2022-01-11 08:25:48,468 - INFO - 
Epoch: 63
2022-01-11 08:25:48,468 - INFO - 
Learning Rate: 0.0010
2022-01-11 08:26:57,984 - INFO - [Step=52750]	Loss=1.2156	253.9 examples/second
2022-01-11 08:28:53,868 - INFO - [Step=53000]	Loss=1.2251	276.1 examples/second
2022-01-11 08:30:49,592 - INFO - [Step=53250]	Loss=1.2134	276.5 examples/second
2022-01-11 08:32:25,227 - INFO - Test Loss=1.1316, Test top-1 acc=0.7272
2022-01-11 08:32:25,227 - INFO - Group Accuracy:

2022-01-11 08:32:25,227 - INFO - [0.9746988  0.9838554  0.9785542  0.9898795  0.9773494  0.9922892
 0.97590363 0.9773494  0.9860241  0.973253   0.98433733 0.97951806
 0.9701205  0.97301203 0.96698797 0.9821687  0.993253  ]
2022-01-11 08:32:25,228 - INFO - Epoch time: 396.7594759464264
2022-01-11 08:32:25,228 - INFO - 
Epoch: 64
2022-01-11 08:32:25,228 - INFO - 
Learning Rate: 0.0010
2022-01-11 08:32:55,047 - INFO - [Step=53500]	Loss=1.2280	255.1 examples/second
2022-01-11 08:34:50,912 - INFO - [Step=53750]	Loss=1.2135	276.2 examples/second
2022-01-11 08:36:46,808 - INFO - [Step=54000]	Loss=1.2158	276.1 examples/second
2022-01-11 08:38:42,542 - INFO - [Step=54250]	Loss=1.2017	276.5 examples/second
2022-01-11 08:39:01,724 - INFO - Test Loss=1.1301, Test top-1 acc=0.7258
2022-01-11 08:39:01,724 - INFO - Group Accuracy:

2022-01-11 08:39:01,724 - INFO - [0.9746988  0.98289156 0.97903615 0.9893976  0.9768675  0.9922892
 0.97566265 0.9773494  0.9855422  0.973253   0.9850602  0.97975904
 0.9713253  0.973253   0.96795183 0.9826506  0.9930121 ]
2022-01-11 08:39:01,725 - INFO - Epoch time: 396.4972229003906
2022-01-11 08:39:01,725 - INFO - 
Epoch: 65
2022-01-11 08:39:01,725 - INFO - 
Learning Rate: 0.0010
2022-01-11 08:40:48,111 - INFO - [Step=54500]	Loss=1.1966	254.8 examples/second
2022-01-11 08:42:43,968 - INFO - [Step=54750]	Loss=1.2056	276.2 examples/second
2022-01-11 08:44:39,547 - INFO - [Step=55000]	Loss=1.2439	276.9 examples/second
2022-01-11 08:45:38,042 - INFO - Test Loss=1.1261, Test top-1 acc=0.7277
2022-01-11 08:45:38,042 - INFO - Group Accuracy:

2022-01-11 08:45:38,042 - INFO - [0.97566265 0.9840964  0.97831327 0.9893976  0.9768675  0.9922892
 0.97542167 0.9780723  0.9860241  0.9727711  0.98433733 0.97975904
 0.9706024  0.9737349  0.9662651  0.9819277  0.993494  ]
2022-01-11 08:45:38,043 - INFO - Saving...
2022-01-11 08:45:38,273 - INFO - Epoch time: 396.5479395389557
2022-01-11 08:45:38,273 - INFO - 
Epoch: 66
2022-01-11 08:45:38,273 - INFO - 
Learning Rate: 0.0010
2022-01-11 08:46:45,013 - INFO - [Step=55250]	Loss=1.1920	255.1 examples/second
2022-01-11 08:48:40,858 - INFO - [Step=55500]	Loss=1.2088	276.2 examples/second
2022-01-11 08:50:36,869 - INFO - [Step=55750]	Loss=1.2017	275.8 examples/second
2022-01-11 08:52:14,879 - INFO - Test Loss=1.1263, Test top-1 acc=0.7267
2022-01-11 08:52:14,879 - INFO - Group Accuracy:

2022-01-11 08:52:14,879 - INFO - [0.97518075 0.9838554  0.97903615 0.9889157  0.9771084  0.9920482
 0.97566265 0.9778313  0.9855422  0.97301203 0.98361444 0.97927713
 0.97180724 0.9725301  0.9660241  0.98289156 0.99373496]
2022-01-11 08:52:14,880 - INFO - Epoch time: 396.6065881252289
2022-01-11 08:52:14,880 - INFO - 
Epoch: 67
2022-01-11 08:52:14,880 - INFO - 
Learning Rate: 0.0010
2022-01-11 08:52:42,213 - INFO - [Step=56000]	Loss=1.2199	255.3 examples/second
2022-01-11 08:54:38,445 - INFO - [Step=56250]	Loss=1.2132	275.3 examples/second
2022-01-11 08:56:34,830 - INFO - [Step=56500]	Loss=1.2173	275.0 examples/second
2022-01-11 08:58:30,934 - INFO - [Step=56750]	Loss=1.2114	275.6 examples/second
2022-01-11 08:58:52,408 - INFO - Test Loss=1.1320, Test top-1 acc=0.7263
2022-01-11 08:58:52,409 - INFO - Group Accuracy:

2022-01-11 08:58:52,419 - INFO - [0.97445786 0.9838554  0.9785542  0.9889157  0.9768675  0.9922892
 0.97590363 0.9768675  0.98650604 0.97301203 0.98289156 0.9807229
 0.9706024  0.97180724 0.9672289  0.98313254 0.993253  ]
2022-01-11 08:58:52,420 - INFO - Epoch time: 397.5398187637329
2022-01-11 08:58:52,420 - INFO - 
Epoch: 68
2022-01-11 08:58:52,420 - INFO - 
Learning Rate: 0.0010
2022-01-11 09:00:36,234 - INFO - [Step=57000]	Loss=1.2027	255.4 examples/second
2022-01-11 09:02:32,104 - INFO - [Step=57250]	Loss=1.1940	276.2 examples/second
2022-01-11 09:04:28,047 - INFO - [Step=57500]	Loss=1.2134	276.0 examples/second
2022-01-11 09:05:29,004 - INFO - Test Loss=1.1273, Test top-1 acc=0.7275
2022-01-11 09:05:29,004 - INFO - Group Accuracy:

2022-01-11 09:05:29,004 - INFO - [0.97566265 0.9840964  0.97903615 0.9898795  0.97614455 0.9918072
 0.97518075 0.9773494  0.98650604 0.973253   0.98433733 0.9804819
 0.9708434  0.9727711  0.9662651  0.98361444 0.993494  ]
2022-01-11 09:05:29,006 - INFO - Epoch time: 396.5856673717499
2022-01-11 09:05:29,006 - INFO - 
Epoch: 69
2022-01-11 09:05:29,006 - INFO - 
Learning Rate: 0.0010
2022-01-11 09:06:33,610 - INFO - [Step=57750]	Loss=1.2229	254.9 examples/second
2022-01-11 09:08:30,039 - INFO - [Step=58000]	Loss=1.1959	274.8 examples/second
2022-01-11 09:10:26,138 - INFO - [Step=58250]	Loss=1.2049	275.6 examples/second
2022-01-11 09:12:07,058 - INFO - Test Loss=1.1255, Test top-1 acc=0.7294
2022-01-11 09:12:07,059 - INFO - Group Accuracy:

2022-01-11 09:12:07,059 - INFO - [0.97493976 0.9840964  0.98       0.98963857 0.9768675  0.9922892
 0.97542167 0.9773494  0.98650604 0.97301203 0.98361444 0.9804819
 0.9703615  0.97301203 0.96698797 0.98289156 0.993494  ]
2022-01-11 09:12:07,059 - INFO - Saving...
2022-01-11 09:12:07,313 - INFO - Epoch time: 398.30717277526855
2022-01-11 09:12:07,313 - INFO - 
Epoch: 70
2022-01-11 09:12:07,313 - INFO - 
Learning Rate: 0.0010
2022-01-11 09:12:32,603 - INFO - [Step=58500]	Loss=1.2103	253.0 examples/second
2022-01-11 09:14:28,508 - INFO - [Step=58750]	Loss=1.1998	276.1 examples/second
2022-01-11 09:16:24,377 - INFO - [Step=59000]	Loss=1.1974	276.2 examples/second
2022-01-11 09:18:20,220 - INFO - [Step=59250]	Loss=1.2159	276.2 examples/second
2022-01-11 09:18:44,109 - INFO - Test Loss=1.1288, Test top-1 acc=0.7275
2022-01-11 09:18:44,109 - INFO - Group Accuracy:

2022-01-11 09:18:44,109 - INFO - [0.97518075 0.98313254 0.97975904 0.9893976  0.97590363 0.9925301
 0.9746988  0.9780723  0.98650604 0.973494   0.9845783  0.9807229
 0.9708434  0.97204816 0.9655422  0.98313254 0.99421686]
2022-01-11 09:18:44,110 - INFO - Epoch time: 396.7970314025879
2022-01-11 09:18:44,110 - INFO - 
Epoch: 71
2022-01-11 09:18:44,110 - INFO - 
Learning Rate: 0.0010
2022-01-11 09:20:26,111 - INFO - [Step=59500]	Loss=1.1989	254.2 examples/second
2022-01-11 09:22:21,904 - INFO - [Step=59750]	Loss=1.1995	276.4 examples/second
2022-01-11 09:24:17,792 - INFO - [Step=60000]	Loss=1.2082	276.1 examples/second
2022-01-11 09:25:21,335 - INFO - Test Loss=1.1320, Test top-1 acc=0.7280
2022-01-11 09:25:21,335 - INFO - Group Accuracy:

2022-01-11 09:25:21,335 - INFO - [0.97493976 0.9833735  0.9785542  0.9893976  0.97614455 0.9920482
 0.97493976 0.9766265  0.9860241  0.9727711  0.9840964  0.9804819
 0.9703615  0.973494   0.9662651  0.9814458  0.99373496]
2022-01-11 09:25:21,336 - INFO - Epoch time: 397.2256736755371
2022-01-11 09:25:21,336 - INFO - 
Epoch: 72
2022-01-11 09:25:21,336 - INFO - 
Learning Rate: 0.0010
2022-01-11 09:26:23,574 - INFO - [Step=60250]	Loss=1.1825	254.4 examples/second
2022-01-11 09:28:19,493 - INFO - [Step=60500]	Loss=1.2005	276.1 examples/second
2022-01-11 09:30:15,325 - INFO - [Step=60750]	Loss=1.2006	276.3 examples/second
2022-01-11 09:31:58,328 - INFO - Test Loss=1.1246, Test top-1 acc=0.7299
2022-01-11 09:31:58,328 - INFO - Group Accuracy:

2022-01-11 09:31:58,328 - INFO - [0.9737349  0.9833735  0.9787952  0.9898795  0.9771084  0.9918072
 0.97518075 0.97831327 0.98626506 0.973253   0.98433733 0.98024094
 0.97156626 0.9727711  0.966747   0.9826506  0.9939759 ]
2022-01-11 09:31:58,329 - INFO - Saving...
2022-01-11 09:31:58,608 - INFO - Epoch time: 397.2719602584839
2022-01-11 09:31:58,608 - INFO - 
Epoch: 73
2022-01-11 09:31:58,608 - INFO - 
Learning Rate: 0.0010
2022-01-11 09:32:21,499 - INFO - [Step=61000]	Loss=1.2184	253.6 examples/second
2022-01-11 09:34:17,653 - INFO - [Step=61250]	Loss=1.2102	275.5 examples/second
2022-01-11 09:36:13,542 - INFO - [Step=61500]	Loss=1.2067	276.1 examples/second
2022-01-11 09:38:09,617 - INFO - [Step=61750]	Loss=1.1873	275.7 examples/second
2022-01-11 09:38:35,948 - INFO - Test Loss=1.1256, Test top-1 acc=0.7275
2022-01-11 09:38:35,949 - INFO - Group Accuracy:

2022-01-11 09:38:35,949 - INFO - [0.9742169  0.98361444 0.97951806 0.9889157  0.97542167 0.9922892
 0.97566265 0.9773494  0.9860241  0.97301203 0.98433733 0.9807229
 0.9708434  0.9742169  0.9662651  0.9819277  0.99421686]
2022-01-11 09:38:35,950 - INFO - Epoch time: 397.34166049957275
2022-01-11 09:38:35,950 - INFO - 
Epoch: 74
2022-01-11 09:38:35,950 - INFO - 
Learning Rate: 0.0010
2022-01-11 09:40:15,484 - INFO - [Step=62000]	Loss=1.2195	254.2 examples/second
2022-01-11 09:42:11,522 - INFO - [Step=62250]	Loss=1.1894	275.8 examples/second
2022-01-11 09:44:07,687 - INFO - [Step=62500]	Loss=1.1799	275.5 examples/second
2022-01-11 09:45:13,781 - INFO - Test Loss=1.1291, Test top-1 acc=0.7284
2022-01-11 09:45:13,781 - INFO - Group Accuracy:

2022-01-11 09:45:13,781 - INFO - [0.9746988  0.98433733 0.97927713 0.98963857 0.9773494  0.9920482
 0.9746988  0.9771084  0.98578316 0.97301203 0.9845783  0.98
 0.9708434  0.9725301  0.9672289  0.98313254 0.99421686]
2022-01-11 09:45:13,782 - INFO - Epoch time: 397.83214950561523
2022-01-11 09:45:13,782 - INFO - 
Epoch: 75
2022-01-11 09:45:13,782 - INFO - 
Learning Rate: 0.0010
2022-01-11 09:46:14,260 - INFO - [Step=62750]	Loss=1.1924	252.8 examples/second
2022-01-11 09:48:10,770 - INFO - [Step=63000]	Loss=1.1862	274.7 examples/second
2022-01-11 09:50:06,896 - INFO - [Step=63250]	Loss=1.2019	275.6 examples/second
2022-01-11 09:51:52,200 - INFO - Test Loss=1.1291, Test top-1 acc=0.7272
2022-01-11 09:51:52,200 - INFO - Group Accuracy:

2022-01-11 09:51:52,200 - INFO - [0.97493976 0.9838554  0.97831327 0.98963857 0.97590363 0.9925301
 0.9746988  0.9778313  0.98578316 0.973253   0.9840964  0.9804819
 0.9713253  0.973253   0.966506   0.9816868  0.9939759 ]
2022-01-11 09:51:52,201 - INFO - Epoch time: 398.41877818107605
2022-01-11 09:51:52,201 - INFO - 
Epoch: 76
2022-01-11 09:51:52,201 - INFO - 
Learning Rate: 0.0010
2022-01-11 09:52:12,803 - INFO - [Step=63500]	Loss=1.2019	254.2 examples/second
2022-01-11 09:54:08,979 - INFO - [Step=63750]	Loss=1.1947	275.4 examples/second
2022-01-11 09:56:05,096 - INFO - [Step=64000]	Loss=1.1840	275.6 examples/second
2022-01-11 09:58:01,049 - INFO - [Step=64250]	Loss=1.1732	276.0 examples/second
2022-01-11 09:58:29,693 - INFO - Test Loss=1.1284, Test top-1 acc=0.7280
2022-01-11 09:58:29,693 - INFO - Group Accuracy:

2022-01-11 09:58:29,693 - INFO - [0.97493976 0.9840964  0.97831327 0.9898795  0.9768675  0.9918072
 0.97590363 0.9775904  0.98578316 0.97228914 0.9845783  0.98024094
 0.97156626 0.9727711  0.9662651  0.9821687  0.9939759 ]
2022-01-11 09:58:29,694 - INFO - Epoch time: 397.4931080341339
2022-01-11 09:58:29,694 - INFO - 
Epoch: 77
2022-01-11 09:58:29,694 - INFO - 
Learning Rate: 0.0010
2022-01-11 10:00:07,119 - INFO - [Step=64500]	Loss=1.1967	253.8 examples/second
2022-01-11 10:02:02,927 - INFO - [Step=64750]	Loss=1.1904	276.3 examples/second
2022-01-11 10:03:58,819 - INFO - [Step=65000]	Loss=1.1964	276.1 examples/second
2022-01-11 10:05:07,113 - INFO - Test Loss=1.1333, Test top-1 acc=0.7294
2022-01-11 10:05:07,114 - INFO - Group Accuracy:

2022-01-11 10:05:07,114 - INFO - [0.97493976 0.98361444 0.9785542  0.9886747  0.9768675  0.99156624
 0.97518075 0.9775904  0.98578316 0.973253   0.9840964  0.98
 0.97204816 0.97301203 0.966506   0.9826506  0.99373496]
2022-01-11 10:05:07,114 - INFO - Epoch time: 397.4201738834381
2022-01-11 10:05:07,114 - INFO - 
Epoch: 78
2022-01-11 10:05:07,114 - INFO - 
Learning Rate: 0.0010
2022-01-11 10:06:05,001 - INFO - [Step=65250]	Loss=1.1895	253.6 examples/second
2022-01-11 10:08:01,052 - INFO - [Step=65500]	Loss=1.2051	275.7 examples/second
2022-01-11 10:09:57,017 - INFO - [Step=65750]	Loss=1.1904	275.9 examples/second
2022-01-11 10:11:45,263 - INFO - Test Loss=1.1185, Test top-1 acc=0.7284
2022-01-11 10:11:45,263 - INFO - Group Accuracy:

2022-01-11 10:11:45,263 - INFO - [0.97518075 0.9845783  0.97951806 0.9891566  0.97590363 0.9922892
 0.97493976 0.9773494  0.9855422  0.97301203 0.9840964  0.9804819
 0.9713253  0.9742169  0.9662651  0.9819277  0.99421686]
2022-01-11 10:11:45,264 - INFO - Epoch time: 398.14946818351746
2022-01-11 10:11:45,264 - INFO - 
Epoch: 79
2022-01-11 10:11:45,264 - INFO - 
Learning Rate: 0.0010
2022-01-11 10:12:03,351 - INFO - [Step=66000]	Loss=1.1978	253.3 examples/second
2022-01-11 10:13:59,431 - INFO - [Step=66250]	Loss=1.2051	275.7 examples/second
2022-01-11 10:15:55,363 - INFO - [Step=66500]	Loss=1.1882	276.0 examples/second
2022-01-11 10:17:51,381 - INFO - [Step=66750]	Loss=1.2078	275.8 examples/second
2022-01-11 10:18:22,755 - INFO - Test Loss=1.1215, Test top-1 acc=0.7296
2022-01-11 10:18:22,755 - INFO - Group Accuracy:

2022-01-11 10:18:22,755 - INFO - [0.97445786 0.98361444 0.97903615 0.9893976  0.9766265  0.9920482
 0.97566265 0.9787952  0.98578316 0.973253   0.9840964  0.9809638
 0.9713253  0.97445786 0.9672289  0.9821687  0.9939759 ]
2022-01-11 10:18:22,756 - INFO - Epoch time: 397.4921917915344
2022-01-11 10:18:22,756 - INFO - 
Epoch: 80
2022-01-11 10:18:22,756 - INFO - 
Learning Rate: 0.0010
2022-01-11 10:19:57,832 - INFO - [Step=67000]	Loss=1.1882	253.1 examples/second
2022-01-11 10:21:53,816 - INFO - [Step=67250]	Loss=1.1887	275.9 examples/second
2022-01-11 10:23:49,790 - INFO - [Step=67500]	Loss=1.1989	275.9 examples/second
2022-01-11 10:25:00,361 - INFO - Test Loss=1.1225, Test top-1 acc=0.7284
2022-01-11 10:25:00,361 - INFO - Group Accuracy:

2022-01-11 10:25:00,361 - INFO - [0.97566265 0.9833735  0.97927713 0.9893976  0.9766265  0.9920482
 0.97518075 0.9775904  0.98578316 0.9727711  0.9845783  0.97975904
 0.9713253  0.97301203 0.9672289  0.9826506  0.99373496]
2022-01-11 10:25:00,362 - INFO - Epoch time: 397.6060290336609
2022-01-11 10:25:00,362 - INFO - 
Epoch: 81
2022-01-11 10:25:00,362 - INFO - 
Learning Rate: 0.0010
2022-01-11 10:25:56,128 - INFO - [Step=67750]	Loss=1.2096	253.3 examples/second
2022-01-11 10:27:52,892 - INFO - [Step=68000]	Loss=1.1923	274.1 examples/second
2022-01-11 10:29:49,335 - INFO - [Step=68250]	Loss=1.1989	274.8 examples/second
2022-01-11 10:31:40,182 - INFO - Test Loss=1.1316, Test top-1 acc=0.7272
2022-01-11 10:31:40,182 - INFO - Group Accuracy:

2022-01-11 10:31:40,182 - INFO - [0.9742169  0.98361444 0.97831327 0.9891566  0.9766265  0.9925301
 0.97566265 0.9775904  0.9853012  0.973494   0.9848193  0.9807229
 0.9706024  0.97180724 0.9660241  0.98313254 0.993494  ]
2022-01-11 10:31:40,183 - INFO - Epoch time: 399.82083654403687
2022-01-11 10:31:40,183 - INFO - 
Epoch: 82
2022-01-11 10:31:40,183 - INFO - 
Learning Rate: 0.0010
2022-01-11 10:31:56,178 - INFO - [Step=68500]	Loss=1.1863	252.3 examples/second
2022-01-11 10:33:52,550 - INFO - [Step=68750]	Loss=1.1942	275.0 examples/second
2022-01-11 10:35:48,463 - INFO - [Step=69000]	Loss=1.1917	276.1 examples/second
2022-01-11 10:37:44,419 - INFO - [Step=69250]	Loss=1.1812	276.0 examples/second
2022-01-11 10:38:18,077 - INFO - Test Loss=1.1222, Test top-1 acc=0.7287
2022-01-11 10:38:18,077 - INFO - Group Accuracy:

2022-01-11 10:38:18,077 - INFO - [0.97493976 0.9838554  0.9785542  0.9889157  0.97542167 0.9918072
 0.97518075 0.9775904  0.9853012  0.9727711  0.98433733 0.98
 0.9706024  0.9742169  0.966506   0.9821687  0.99373496]
2022-01-11 10:38:18,078 - INFO - Epoch time: 397.89470529556274
2022-01-11 10:38:18,078 - INFO - 
Epoch: 83
2022-01-11 10:38:18,078 - INFO - 
Learning Rate: 0.0010
2022-01-11 10:39:50,932 - INFO - [Step=69500]	Loss=1.1660	252.9 examples/second
2022-01-11 10:41:46,872 - INFO - [Step=69750]	Loss=1.1986	276.0 examples/second
2022-01-11 10:43:42,695 - INFO - [Step=70000]	Loss=1.2115	276.3 examples/second
2022-01-11 10:44:55,582 - INFO - Test Loss=1.1240, Test top-1 acc=0.7275
2022-01-11 10:44:55,582 - INFO - Group Accuracy:

2022-01-11 10:44:55,582 - INFO - [0.9746988  0.9840964  0.9780723  0.98963857 0.97590363 0.9925301
 0.97542167 0.9771084  0.9853012  0.9739759  0.9838554  0.98024094
 0.97180724 0.97445786 0.9653012  0.9819277  0.99373496]
2022-01-11 10:44:55,583 - INFO - Epoch time: 397.50453424453735
2022-01-11 10:44:55,583 - INFO - 
Epoch: 84
2022-01-11 10:44:55,583 - INFO - 
Learning Rate: 0.0010
2022-01-11 10:45:48,834 - INFO - [Step=70250]	Loss=1.1804	253.7 examples/second
2022-01-11 10:47:44,915 - INFO - [Step=70500]	Loss=1.1726	275.7 examples/second
2022-01-11 10:49:41,086 - INFO - [Step=70750]	Loss=1.1842	275.5 examples/second
2022-01-11 10:51:33,291 - INFO - Test Loss=1.1203, Test top-1 acc=0.7258
2022-01-11 10:51:33,291 - INFO - Group Accuracy:

2022-01-11 10:51:33,291 - INFO - [0.97445786 0.9840964  0.9778313  0.9893976  0.97638553 0.9922892
 0.973494   0.9775904  0.98626506 0.9742169  0.9853012  0.97975904
 0.97108436 0.97301203 0.9660241  0.98313254 0.99373496]
2022-01-11 10:51:33,293 - INFO - Epoch time: 397.7102656364441
2022-01-11 10:51:33,293 - INFO - 
Epoch: 85
2022-01-11 10:51:33,293 - INFO - 
Learning Rate: 0.0010
2022-01-11 10:51:47,169 - INFO - [Step=71000]	Loss=1.1937	253.8 examples/second
2022-01-11 10:53:43,451 - INFO - [Step=71250]	Loss=1.2082	275.2 examples/second
2022-01-11 10:55:39,561 - INFO - [Step=71500]	Loss=1.1925	275.6 examples/second
2022-01-11 10:57:35,880 - INFO - [Step=71750]	Loss=1.1904	275.1 examples/second
2022-01-11 10:58:11,498 - INFO - Test Loss=1.1182, Test top-1 acc=0.7292
2022-01-11 10:58:11,498 - INFO - Group Accuracy:

2022-01-11 10:58:11,498 - INFO - [0.97518075 0.9838554  0.97831327 0.9884337  0.9766265  0.9918072
 0.97566265 0.9785542  0.9855422  0.97301203 0.9850602  0.98024094
 0.9708434  0.9727711  0.9672289  0.9826506  0.99421686]
2022-01-11 10:58:11,499 - INFO - Epoch time: 398.2058336734772
2022-01-11 10:58:11,499 - INFO - 
Epoch: 86
2022-01-11 10:58:11,499 - INFO - 
Learning Rate: 0.0010
2022-01-11 10:59:41,907 - INFO - [Step=72000]	Loss=1.1964	253.9 examples/second
2022-01-11 11:01:38,041 - INFO - [Step=72250]	Loss=1.1992	275.5 examples/second
2022-01-11 11:03:33,993 - INFO - [Step=72500]	Loss=1.1983	276.0 examples/second
2022-01-11 11:04:49,133 - INFO - Test Loss=1.1198, Test top-1 acc=0.7275
2022-01-11 11:04:49,134 - INFO - Group Accuracy:

2022-01-11 11:04:49,134 - INFO - [0.97445786 0.98361444 0.9780723  0.9891566  0.9768675  0.9920482
 0.97493976 0.9780723  0.9855422  0.973253   0.9840964  0.9804819
 0.9713253  0.97204816 0.9657831  0.98240966 0.99421686]
2022-01-11 11:04:49,134 - INFO - Epoch time: 397.63528656959534
2022-01-11 11:04:49,134 - INFO - 
Epoch: 87
2022-01-11 11:04:49,134 - INFO - 
Learning Rate: 0.0010
2022-01-11 11:05:40,411 - INFO - [Step=72750]	Loss=1.1806	253.1 examples/second
2022-01-11 11:07:36,567 - INFO - [Step=73000]	Loss=1.1824	275.5 examples/second
2022-01-11 11:09:32,384 - INFO - [Step=73250]	Loss=1.1856	276.3 examples/second
2022-01-11 11:11:27,099 - INFO - Test Loss=1.1144, Test top-1 acc=0.7318
2022-01-11 11:11:27,099 - INFO - Group Accuracy:

2022-01-11 11:11:27,099 - INFO - [0.97518075 0.9845783  0.97903615 0.9898795  0.9775904  0.9920482
 0.97518075 0.97831327 0.9855422  0.9739759  0.98433733 0.98024094
 0.9701205  0.97445786 0.966506   0.9833735  0.99421686]
2022-01-11 11:11:27,101 - INFO - Saving...
2022-01-11 11:11:27,348 - INFO - Epoch time: 398.2138681411743
2022-01-11 11:11:27,348 - INFO - 
Epoch: 88
2022-01-11 11:11:27,349 - INFO - 
Learning Rate: 0.0010
2022-01-11 11:11:38,815 - INFO - [Step=73500]	Loss=1.1887	253.1 examples/second
2022-01-11 11:13:35,049 - INFO - [Step=73750]	Loss=1.1813	275.3 examples/second
2022-01-11 11:15:30,811 - INFO - [Step=74000]	Loss=1.1835	276.4 examples/second
2022-01-11 11:17:26,602 - INFO - [Step=74250]	Loss=1.1839	276.4 examples/second
2022-01-11 11:18:04,723 - INFO - Test Loss=1.1191, Test top-1 acc=0.7296
2022-01-11 11:18:04,723 - INFO - Group Accuracy:

2022-01-11 11:18:04,723 - INFO - [0.9746988  0.9850602  0.9787952  0.9891566  0.9766265  0.9918072
 0.97518075 0.97831327 0.98578316 0.973494   0.9838554  0.97975904
 0.97108436 0.97301203 0.966506   0.9826506  0.9939759 ]
2022-01-11 11:18:04,724 - INFO - Epoch time: 397.3759503364563
2022-01-11 11:18:04,725 - INFO - 
Epoch: 89
2022-01-11 11:18:04,725 - INFO - 
Learning Rate: 0.0010
2022-01-11 11:19:33,055 - INFO - [Step=74500]	Loss=1.1759	253.1 examples/second
2022-01-11 11:21:29,160 - INFO - [Step=74750]	Loss=1.1679	275.6 examples/second
2022-01-11 11:23:25,214 - INFO - [Step=75000]	Loss=1.2048	275.7 examples/second
2022-01-11 11:24:42,593 - INFO - Test Loss=1.1126, Test top-1 acc=0.7306
2022-01-11 11:24:42,593 - INFO - Group Accuracy:

2022-01-11 11:24:42,593 - INFO - [0.97493976 0.9845783  0.9780723  0.9898795  0.9771084  0.9922892
 0.97518075 0.9787952  0.9855422  0.97301203 0.9838554  0.9812048
 0.97180724 0.9739759  0.9657831  0.98240966 0.99421686]
2022-01-11 11:24:42,594 - INFO - Epoch time: 397.86974239349365
2022-01-11 11:24:53,317 - INFO - Computing OOD Statistics...
2022-01-11 11:24:53,328 - INFO - 	Baseline.          AUROC: 0.3434. TNR@95TPR: 0.0353. AUPR OUT: 0.1264
2022-01-11 11:24:53,333 - INFO - 	ODIN (T=1000).     AUROC: 0.8338. TNR@95TPR: 0.1871. AUPR OUT: 0.4082
2022-01-11 11:24:53,333 - INFO - Printing Final Accuracy + OOD Detection stats
2022-01-11 11:24:53,333 - INFO - Top 1 Accuracy:  Min: 0.7318; Max: 0.7318; Avg: 0.7318; Std: 0.0000; Len: 1
2022-01-11 11:24:53,333 - INFO - Top 5 Accuracy:  Min: 0.9803; Max: 0.9803; Avg: 0.9803; Std: 0.0000; Len: 1
2022-01-11 11:24:53,333 - INFO - **********************************************************************
2022-01-11 11:24:53,333 - INFO - 	MSP (auroc): [0.3434069454287739] Min: 0.3434; Max: 0.3434; Avg: 0.3434; Std: 0.0000; Len: 1
2022-01-11 11:24:53,333 - INFO - 	MSP (tnr): [0.03529411764705881] Min: 0.0353; Max: 0.0353; Avg: 0.0353; Std: 0.0000; Len: 1
2022-01-11 11:24:53,334 - INFO - 	MSP (aupr): [0.12641746518778246] Min: 0.1264; Max: 0.1264; Avg: 0.1264; Std: 0.0000; Len: 1
2022-01-11 11:24:53,334 - INFO - 	ODIN (auroc): [0.8338423812898653] Min: 0.8338; Max: 0.8338; Avg: 0.8338; Std: 0.0000; Len: 1
2022-01-11 11:24:53,334 - INFO - 	ODIN (tnr): [0.18705882352941172] Min: 0.1871; Max: 0.1871; Avg: 0.1871; Std: 0.0000; Len: 1
2022-01-11 11:24:53,334 - INFO - 	ODIN (aupr): [0.40824139955915056] Min: 0.4082; Max: 0.4082; Avg: 0.4082; Std: 0.0000; Len: 1
